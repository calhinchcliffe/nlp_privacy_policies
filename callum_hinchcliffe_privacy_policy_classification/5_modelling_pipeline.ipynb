{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb32d037",
   "metadata": {},
   "source": [
    "<h1> Modelling <span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "Having ran a baseline model, and having conducted further preprocessing, I would now like to see whether I can replicate similar steps to Story et al. and see how different preprocessing steps affect the performance of models.\n",
    "\n",
    "I will begin by replicating their steps, but just training a single classifier.  Then I will evaluate the results. This will be section 1 of this notebook.\n",
    "\n",
    "Then, in section 2, having demonstrated the steps, I will build functions for training models to use to train multiple classifiers.\n",
    "\n",
    "This will give results for all classifiers that I can then discuss.\n",
    "\n",
    "Finally I can run my most promising models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475bb91",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Crafted-Features-and-Sentence-Filtering\" data-toc-modified-id=\"Crafted-Features-and-Sentence-Filtering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Crafted Features and Sentence Filtering</a></span></li><li><span><a href=\"#Section-1\" data-toc-modified-id=\"Section-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Section 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-and-splitting-into-train/validate/test\" data-toc-modified-id=\"Loading-data-and-splitting-into-train/validate/test-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Loading data and splitting into train/validate/test</a></span></li></ul></li><li><span><a href=\"#Step-1:-select-classifier:-&quot;1st_party&quot;\" data-toc-modified-id=\"Step-1:-select-classifier:-&quot;1st_party&quot;-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 1: select classifier: \"1st_party\"</a></span></li><li><span><a href=\"#Step-2:-apply-Sentence-Filtering\" data-toc-modified-id=\"Step-2:-apply-Sentence-Filtering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 2: apply Sentence Filtering</a></span></li><li><span><a href=\"#Step-3:-Separate-into-X-and-y\" data-toc-modified-id=\"Step-3:-Separate-into-X-and-y-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 3: Separate into X and y</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-X-(Crafted-features-+-TF-IDF-matrix)\" data-toc-modified-id=\"Create-X-(Crafted-features-+-TF-IDF-matrix)-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create X (Crafted features + TF-IDF matrix)</a></span></li><li><span><a href=\"#Create-y\" data-toc-modified-id=\"Create-y-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Create y</a></span></li></ul></li><li><span><a href=\"#Step-4:-5-fold-CV-Grid-Search-over-hyperparameters\" data-toc-modified-id=\"Step-4:-5-fold-CV-Grid-Search-over-hyperparameters-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 4: 5-fold CV Grid Search over hyperparameters</a></span></li><li><span><a href=\"#Evaluation-on-Train-set\" data-toc-modified-id=\"Evaluation-on-Train-set-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluation on Train set</a></span></li><li><span><a href=\"#Evaluation-on-Validate-set\" data-toc-modified-id=\"Evaluation-on-Validate-set-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation on Validate set</a></span></li><li><span><a href=\"#Section-2:-modelling-pipeline\" data-toc-modified-id=\"Section-2:-modelling-pipeline-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Section 2: modelling pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements-for-modelling-pipeline\" data-toc-modified-id=\"Requirements-for-modelling-pipeline-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Requirements for modelling pipeline</a></span></li></ul></li><li><span><a href=\"#Run-all-classifiers-through-the-pipeline\" data-toc-modified-id=\"Run-all-classifiers-through-the-pipeline-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Run all classifiers through the pipeline</a></span></li><li><span><a href=\"#Creating-model-evaluation-table\" data-toc-modified-id=\"Creating-model-evaluation-table-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Creating model evaluation table</a></span></li><li><span><a href=\"#Discussion-of-low-F1-scores\" data-toc-modified-id=\"Discussion-of-low-F1-scores-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Discussion of low F1 scores</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-wide-range-of-scores\" data-toc-modified-id=\"A-wide-range-of-scores-12.1\"><span class=\"toc-item-num\">12.1&nbsp;&nbsp;</span>A wide range of scores</a></span></li><li><span><a href=\"#Neg-F1-tends-to-be-higher-than-Pos-F1\" data-toc-modified-id=\"Neg-F1-tends-to-be-higher-than-Pos-F1-12.2\"><span class=\"toc-item-num\">12.2&nbsp;&nbsp;</span>Neg F1 tends to be higher than Pos F1</a></span></li><li><span><a href=\"#Compare-with-results-from-the-paper\" data-toc-modified-id=\"Compare-with-results-from-the-paper-12.3\"><span class=\"toc-item-num\">12.3&nbsp;&nbsp;</span>Compare with results from the paper</a></span></li><li><span><a href=\"#Matching-scores-for-SSO-and-FacebookSSO\" data-toc-modified-id=\"Matching-scores-for-SSO-and-FacebookSSO-12.4\"><span class=\"toc-item-num\">12.4&nbsp;&nbsp;</span>Matching scores for SSO and FacebookSSO</a></span></li></ul></li><li><span><a href=\"#Final-model-performance-on-test-set\" data-toc-modified-id=\"Final-model-performance-on-test-set-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Final model performance on test set</a></span></li><li><span><a href=\"#Further-work\" data-toc-modified-id=\"Further-work-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Further work</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719e140",
   "metadata": {},
   "source": [
    "The steps in this notebook are:\n",
    "\n",
    "**Section 1**\n",
    "\n",
    "- Load data and split into Train/Validate/Test\n",
    "\n",
    "For demonstrative purposes I will begin by running through the modelling steps for just a single classifier: 1st Party\n",
    "\n",
    "Apply sentence filtering\n",
    "- This means I will only train the 1st Party classifier on a relevant subset of the data\n",
    "\n",
    "Separating into X and y\n",
    "- X, the input data to the model, is a union of the crafted features columns and a tf-idf matrix\n",
    "\n",
    "5-fold cross-validation grid search\n",
    "- Over the hyperparameters for SVM model as performed by Story et al.\n",
    "\n",
    "Evaluation on train set\n",
    "\n",
    "Evaluation on test set and review.\n",
    "\n",
    "**Section 2**\n",
    "\n",
    "Having demonstrated the steps for model construction for one classifier, I move to create a pipeline to train multiple classifiers.\n",
    "\n",
    "The full pipeline consists of a series of functions that allows me to train a classifier for each of the 18 targets of interest. The steps are:\n",
    "\n",
    "Listing the requirements for the pipeline\n",
    "\n",
    "Functions for the pipeline. \n",
    "- These correspond to the steps executed above for ‘1st party’.\n",
    "\n",
    "Display of all model scores\n",
    "\n",
    "Discussion of some zero scores\n",
    "\n",
    "Further discussion\n",
    "\n",
    "Final model performance\n",
    "\n",
    "# Crafted Features and Sentence Filtering\n",
    "\n",
    "To try to improve classifier performance, Story et al. apply a preprocessing technique that they call Sentence Filtering. This involves filtering the data to only train a classifier on segments that contain a relevant feature for the target.  For example a short segment about location data wouldn't be used to train an email classifier because it wouldn't contain keywords such as \"Email\" or \"Contact\".\n",
    "\n",
    "They do not explicitly specify that these features are equivalent to the crafted features but I am inferring that they are.\n",
    "\n",
    "The effect of applying sentence filtering is that instead of a model being trained on all the data and there being class imbalance in favour of the negative class, a much greater proportion of the training data features the target being classified. The effect is similar to downsampling.\n",
    "\n",
    "This will be a moderate approximation for a replication of most of their work. The main missing element will be better text pre-processing to get better results from the crafted features and sentence filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a733d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# modelling\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# modelling pipeline\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# modelling evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eae328",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "## Loading data and splitting into train/validate/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5d3a",
   "metadata": {},
   "source": [
    "Train, Validate and Test dataframes to use. Story et al. already randomly split them into training, validation and test. They do not explain that this split was stratified by any classifier.  The proportions of the splits that they use seem standard for common data science practice – above a 25% test split.  I am suspicious that there is not enough data in this domain to train some classifiers so I would bring this proportion to lower, closer to 25% or perhaps slightly below.  (But I think this would have been difficult for Story et al. to know in advance and they may have had an idea already since they were building on earlier work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1d24f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "# Create separate dataframes for each group\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a7e54",
   "metadata": {},
   "source": [
    "Seeing the shapes of the dataframe – number of observations and confirming the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34e9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8068, 511)\n",
      "(2651, 511)\n",
      "(4824, 511)\n"
     ]
    }
   ],
   "source": [
    "print(df_for_pipelining_train.shape)\n",
    "print(df_for_pipelining_val.shape)\n",
    "print(df_for_pipelining_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414ab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation features to use for sentence filtering later\n",
    "clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2db48",
   "metadata": {},
   "source": [
    "# Step 1: select classifier: \"1st_party\"\n",
    "\n",
    "Let's start with 1st Party as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565514c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \"1st_party\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb83b",
   "metadata": {},
   "source": [
    "# Step 2: apply Sentence Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc013b2d",
   "metadata": {},
   "source": [
    "Retrieving the crafted features for 1st Party to use for sentence filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d283eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'you', 'us', 'our', 'the app', 'the software']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering the table to get the list object from the same row that lists the classifier\n",
    "classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "\n",
    "classifier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f69f62",
   "metadata": {},
   "source": [
    "Filter the dataframe for rows where any of those features is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7903b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataframe's shape was (8068, 511).\n",
      "The dataframe after sentence filtering is (7297, 511)\n"
     ]
    }
   ],
   "source": [
    "df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"The original dataframe's shape was {df_for_pipelining_train.shape}\")\n",
    "print(f\"The dataframe after sentence filtering is {df_for_pipelining_train_SF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d125a",
   "metadata": {},
   "source": [
    "We have lost a big proportion of data but still have enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e3f6d",
   "metadata": {},
   "source": [
    "# Step 3: Separate into X and y\n",
    "\n",
    "## Create X (Crafted features + TF-IDF matrix)\n",
    "The input data to the model, X, requires a union of the Crafted Features columns and the TF-IDF matrix.\n",
    "\n",
    "Create TF-IDF matrix, fit it to the text data, and use it to transform it to create the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate transformer object\n",
    "tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "# fit the transformer and transform the data\n",
    "train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a23f3f",
   "metadata": {},
   "source": [
    "This tf-idf matrix is stored in a sparse matrix format.\n",
    "\n",
    "Extract crafted features columns from X and convert it to sparse format so that it can be combined with TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be left with the 476 different crafted features (CFs). CF shape is: (7297, 476)\n"
     ]
    }
   ],
   "source": [
    "# Extract CF columns:\n",
    "classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy() \n",
    "# the CF columns happen to be all those after and including 'contact info', so I \n",
    "# use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "print(f\"Should be left with the 476 different crafted features (CFs). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "\n",
    "#convert to sparse\n",
    "classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ]) # hstack combines the matricies horizontally (more columns, same number of rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1026263",
   "metadata": {},
   "source": [
    "## Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "709d38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5b020",
   "metadata": {},
   "source": [
    "An extra stage of data verification never hurts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6cef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Y_train only has binary values\n",
    "if classifier_y_train.max() > 1:\n",
    "    for i in range(len(classifier_y_train)):\n",
    "        if classifier_y_train[i] > 1:\n",
    "            classifier_y_train[i] = 1\n",
    "    print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbcde5",
   "metadata": {},
   "source": [
    "Now we have our input data and target ready to pass into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927efab",
   "metadata": {},
   "source": [
    "# Step 4: 5-fold CV Grid Search over hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161595e9",
   "metadata": {},
   "source": [
    "There are multiple things going on here that are worth explaining:\n",
    "- 5-fold CV (cross-validation)\n",
    "- Model of choice: SVM\n",
    "- Hyperparameter Grid Search\n",
    "- A note on scoring\n",
    "\n",
    "**5-fold Cross Validation**\n",
    "\n",
    "The technique involves partitioning the dataset into five folds, each of which is used in turn as a test set while the remaining folds are used as training data for a model. This process is repeated a number of times, with each fold being used as a test set once. The average performance across all test sets is then used to judge the model's performance.\n",
    "\n",
    "This means that each different model being tested in the grid search is trained and tested 5 times. This approach can be computationally intensive, but it often results in a more accurate estimate of the model's true performance.\n",
    "\n",
    "**Model of Choice: SVM (Support Vector Machines)**\n",
    "\n",
    "Of all possible machine learning models, Story et al. exclusively mention SVM. I presume that other models were not discussed because:\n",
    "- they were building on earlier work informing them of how to find success with SVM in this domain, or\n",
    "- neglected to explain all the other model searching they conducted\n",
    "\n",
    "SVM is a supervised machine learning algorithm that can be used for both classification and regression tasks. In the n-dimensional space of training datapoints, the SVM algorithm finds the hyperplane that maximizes the margin between the data of the two classes (i.e. the decision boundary). The margin is defined as the distance between the hyperplane and the closest data points.  The closest data points to the margin are used to construct the hyperplane and are thus called the 'support vectors'. \n",
    "\n",
    "The SVM algorithm has a number of advantages, including its ability to handle non-linear decision boundaries, its robustness to overfitting, and its efficiency in high-dimensional spaces such as with text data.\n",
    "\n",
    "In general the time complexity for support vector machines is between O(m * n^2) and O(m * n^3), where m is the number of features and n is the number of training examples, but it is more complex than this and can be specific to different kernels.  I find greatly increasing training time with more observations. But with this small dataset, given the width of the data, the training times are very reasonable.\n",
    "\n",
    "I recommend [Sci-kit Learn's page](https://scikit-learn.org/stable/modules/svm.html) to learn more.\n",
    "\n",
    "**SVM: Which hyperparameters can be tuned?**\n",
    "\n",
    "Story et al. state: \"we use scikit-learn’s SVC implementation (scikit-learn developers 2016b). We train those with a linear kernel (kernel=’linear’), balanced class weights (class weight=’balanced’), and a grid search with five-fold cross-validation over the penalty (C=[0.1, 1, 10]) and gamma (gamma=[0.001, 0.01, 0.1]) parameters.\n",
    "\n",
    "- The Kernel function for transformation, e.g. rbf, polynomial, linear. Having decided that Linear is suitable for this problem, Story et al. only consider a linear kernel, which is known to perform well on text data.\n",
    "- C. Decreasing C corresponds to more regularization.\n",
    "- Gamma. This is not relevant for linear kernels so I'm unsure why they have included it. (See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) ). It is possible that it was left there as an error after the researchers tried different kernels. The code runs without error and I want to include it in case there is something that I have missed in my research but to save computation time I will not include it in my grid search.\n",
    "\n",
    "**Hyperparameter Grid Search**\n",
    "To find the best hyperparameters for each classifier, we want to find which combination of hyperparameters scores best. So we train and evaluate a model for each different combination of hyperparameters. Effectively, this 'grid' of hyperparameters is searched over to find the best model.\n",
    "\n",
    "**Scoring**\n",
    "\n",
    "I am electing to assess each model's performance by the F1 score (harmonic mean of precision and recall). The F1 metric is an important metric for Story et al.'s work so this will help me compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34da8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "pipeline_sequences = [\n",
    "        ('SVC', SVC(kernel='linear', class_weight='balanced', random_state=1)) ]\n",
    "pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "svc_params = {'SVC__C': [0.1, 1, 10]}\n",
    "\n",
    "# Create grid search object\n",
    "grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=1, n_jobs=-1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1f9b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "CPU times: user 9.35 s, sys: 68.5 ms, total: 9.42 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dc002",
   "metadata": {},
   "source": [
    "# Evaluation on Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bd8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      5504\n",
      "           1       0.94      1.00      0.97      1793\n",
      "\n",
      "    accuracy                           0.98      7297\n",
      "   macro avg       0.97      0.99      0.98      7297\n",
      "weighted avg       0.98      0.98      0.98      7297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_prediction = fitted_search.predict(classifier_X_train)\n",
    "print(classification_report(classifier_y_train, classifier_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0f22",
   "metadata": {},
   "source": [
    "These seem like healthy scores across the board but they are all quite high so there is a risk of overfitting.  I will discuss the results once evaluating the score on the validation set, which allows me to compare to Story et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b0b32",
   "metadata": {},
   "source": [
    "# Evaluation on Validate set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936e811",
   "metadata": {},
   "source": [
    "I conduct similar preprocessing steps to prepare the validate set for prediction. I can ommit sentence filtering because it is only important for training the model, and the model is already trained.\n",
    "\n",
    "Load the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa31bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d14fe0",
   "metadata": {},
   "source": [
    "Create the tfid matrix by transforming the text data (using the transformer fit on the training data), combining this with the created features, and also extracting the target y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "787f50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value should be one. Highest value is: 1\n"
     ]
    }
   ],
   "source": [
    "val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "\n",
    "# Extract CF columns:\n",
    "classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "#convert to sparse\n",
    "classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "# Ensure Y_val only has binary values\n",
    "for i in range(len(classifier_y_val)):\n",
    "    if classifier_y_val[i] > 1:\n",
    "        classifier_y_val[i] = 1\n",
    "print(f\"Highest value should be one. Highest value is: {classifier_y_val.max()}\") # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbd87f",
   "metadata": {},
   "source": [
    "Running the model.\n",
    "\n",
    "**Storing the model results** \n",
    "\n",
    "I create a series to store the results for each classifier I could make.\n",
    "\n",
    "For each model, I will create a list with 3 values: the fitted search, the y values and the y predictions. Then each of those lists is stored in a series where the index is the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76cc44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper\n",
    "\n",
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcf0c7",
   "metadata": {},
   "source": [
    "Running the model and showing confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d913c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1865</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>85</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1865                 109\n",
       "True Positive                  85                 592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(classifier_y_val, classifier_val_prediction):\n",
    "    cf_matrix = confusion_matrix(classifier_y_val, classifier_val_prediction)\n",
    "    cf_df = pd.DataFrame(\n",
    "        cf_matrix, columns=[\"Predicted Negative\", \"Predicted Positive\"], index=[\"True Negative\", \"True Positive\"])\n",
    "    display(cf_df)\n",
    "show_confusion_matrix(classifier_y_val, classifier_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1c7fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1974\n",
      "           1       0.84      0.87      0.86       677\n",
      "\n",
      "    accuracy                           0.93      2651\n",
      "   macro avg       0.90      0.91      0.90      2651\n",
      "weighted avg       0.93      0.93      0.93      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "# saving the model results for future use\n",
    "model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "model_results.to_pickle(\"objects/model_results.pkl\")\n",
    "\n",
    "print(classification_report(classifier_y_val, classifier_val_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772055a5",
   "metadata": {},
   "source": [
    "**Scoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab55e38",
   "metadata": {},
   "source": [
    "Only the F1 scores of each classifier is given in the paper (Table 1 pg 4). I assume that this is F1 for class 1, and so we can use that for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf6285",
   "metadata": {},
   "source": [
    "Looks like this scored okay.  \n",
    "\n",
    "Reviewing whether to focus on Positive F1 score or the negative F1 score – \"Negative F-1 score\" is important because it relates to when a policy fails to mention an important practice (relevant for detecting violations, important to Story et al.).  We want to be sure that if a policy fails to mention it, the classifier correctly states that it is not mentioned.\n",
    "\n",
    "Relatedly Negative Recall is the proportion of *When it was not in, did it say that it was not in?*<br>\n",
    "Negative Precision is *When it predicted that it wasn't in, how often was that the case?*\n",
    "\n",
    "Positive recall is *When it was in, what was the chance it was identified?*  <br>\n",
    "Positive precision is *When it was predicted to be in, what was the chance that it was in?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1770a1",
   "metadata": {},
   "source": [
    "# Section 2: modelling pipeline\n",
    "\n",
    "Firstly, what are the required inputs I need to be ready before running the pipeline?\n",
    "\n",
    "## Requirements for modelling pipeline\n",
    "\n",
    "1. List of all the different classifiers to train\n",
    "2. Empty table of classifier results to populate\n",
    "3. df_for_pipelining_train/val/test. This is the dataframe with all the X and y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1cd1fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating this list here so that the below cells can all be ran without running any above cells \n",
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper\n",
    "\n",
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77316778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_val.reset_index(inplace=True, drop=True)\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "df_for_pipelining_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76c1f4",
   "metadata": {},
   "source": [
    "Another next step is to move the pipeline functions I have created to a separate file to tidy up the notebook. For now, please accept the following sequence of custom functions. They mimic the steps already performed above for the 1st Party classifier.\n",
    "\n",
    "The first function calls the different sub-functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fcb51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_modelling_pipeline(classifier, model_results_series, sentence_filtering=True, inspect_flow=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "        classifier: name (exact string) of the target column to predict\n",
    "        \n",
    "        model_results_series: the empty series to which to save the results to. A pickle file of the same name will be saved with the results. \n",
    "        \n",
    "        sentence_filtering: sentence_filtering takes a boolean, default True. \n",
    "             Work in progress: If False, the flow will ommit the sentence filtering step.\n",
    "        \n",
    "        inspect_flow: Passing inspect_flow=True will print out the shape of dataframes moving through the flow \n",
    "    \"\"\"\n",
    "    if sentence_filtering == False:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # step 1 – declare the classifier\n",
    "    \n",
    "    print(f\"Running for classifier: {classifier}\")\n",
    "    start_code_time = time.time()\n",
    "    \n",
    "    # step 2 – execute sentence filtering on the data to use\n",
    "    \n",
    "    clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")\n",
    "    \n",
    "    if sentence_filtering == True:\n",
    "        df_for_pipelining_train_SF = model_pipeline_step_2(classifier, clean_annotation_features)\n",
    "    elif sentence_filtering == False:\n",
    "        df_for_pipelining_train_SF = df_for_pipelining_train.copy()\n",
    "        df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    if inspect_flow == True: print(f\"df_for_pipelining_train_SF: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    # step 3 – create the X and y data\n",
    "    \n",
    "    classifier_X_train, tfidfTransformer = model_pipeline_step_3_1(df_for_pipelining_train_SF)\n",
    "    \n",
    "    classifier_y_train = model_pipeline_step_3_2(df_for_pipelining_train_SF)\n",
    "    \n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_train (made of CFs plus tf-idf matrix): {classifier_X_train.shape}\")\n",
    "        print(f\"classifier_y_train: {classifier_y_train.shape}\")\n",
    "    \n",
    "    # step 4 – run CV Grid Search\n",
    "    \n",
    "    fitted_search = model_pipeline_step_4(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    # step 5 – run the best model on the validation set and save the results\n",
    "    \n",
    "    classifier_X_val, classifier_y_val = model_pipeline_step_5_1(df_for_pipelining_val, tfidfTransformer)\n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_val: {classifier_X_val.shape}\")\n",
    "        print(f\"classifier_y_val: {classifier_y_val.shape}\")\n",
    "    \n",
    "    model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val, model_results_series)\n",
    "    \n",
    "    # Tests\n",
    "    if type(model_results_series[classifier]) == int:\n",
    "        print(\"Model results not saved.\")\n",
    "        raise NotSavedError(\"Check model results\")\n",
    "    \n",
    "    print(f\"The runtime for {classifier} was {round(time.time() - start_code_time, 5)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6da5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_2(classifier, clean_annotation_features):\n",
    "    \n",
    "    # step 2 – Get crafted features for classifier to use for sentence filtering\n",
    "    \n",
    "    # filtering the annotations & features table to get the list object from the same row that lists the classifier:\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "    \n",
    "    # Filter the DF for rows where any of those features is 1:\n",
    "    df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "    df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    print(f\"Shape of {classifier} train df after sentence filtering is: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    return df_for_pipelining_train_SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d36c429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_1(df_for_pipelining_train_SF):\n",
    "    \n",
    "    # separate into X\n",
    "    \n",
    "    tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "    train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])\n",
    "    \n",
    "    # Extract crafted features columns:\n",
    "    \n",
    "    classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy() # Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "    \n",
    "    if classifier_X_train_cfs.shape[1] != 476:\n",
    "        print(f\"Should be left with the 476 crafted features (CF). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "        raise Step_3_CF_error(\"Crafted features not being applied correctly\")\n",
    "\n",
    "    #convert to sparse\n",
    "    classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X\n",
    "    classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])\n",
    "    \n",
    "    return classifier_X_train, tfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83018c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_2(df_for_pipelining_train_SF):\n",
    "    \n",
    "    # separate into y\n",
    "    \n",
    "    classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()\n",
    "    \n",
    "    # Ensure Y_train only has binary values\n",
    "    for i in range(len(classifier_y_train)):\n",
    "        if classifier_y_train[i] > 1:\n",
    "            classifier_y_train[i] = 1\n",
    "    \n",
    "    if classifier_y_train.max() != 1:\n",
    "        print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\")\n",
    "        raise Step_3_y_error(\"train target colum not binary\")\n",
    "    \n",
    "    return classifier_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a200e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_4(classifier_X_train, classifier_y_train):\n",
    "    \n",
    "    # Grid Search\n",
    "    \n",
    "    cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "    pipeline_sequences = [\n",
    "            ('SVC', SVC(kernel='linear', class_weight='balanced', random_state=1)) ]\n",
    "    pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "    svc_params = {'SVC__C': [0.1, 1, 10]}\n",
    "\n",
    "    # Create grid search object\n",
    "    grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=0, n_jobs=-1, scoring='f1')\n",
    "    \n",
    "    fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    return fitted_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b2b8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_1(df_for_pipelining_val, tfidfTransformer):\n",
    "    \n",
    "    # create validate X and y\n",
    "\n",
    "    val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "    \n",
    "    # Extract CF columns:\n",
    "    classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "    \n",
    "    # convert to sparse:\n",
    "    classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X:\n",
    "    classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "    classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "    # Ensure Y_val only has binary values\n",
    "    for i in range(len(classifier_y_val)):\n",
    "        if classifier_y_val[i] > 1:\n",
    "            classifier_y_val[i] = 1\n",
    "    \n",
    "    if classifier_y_val.max() != 1:\n",
    "        print(f\"Highest value should be one. Highest value is: {classifier_y_val.max()}\")\n",
    "        raise Step_5_val_error(\"Validation target column not binary\")\n",
    "    \n",
    "    return classifier_X_val, classifier_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b187f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val, model_results_series):\n",
    "    \n",
    "    # scoring\n",
    "    classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "    # save the model to a series\n",
    "    model_results_series[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "    \n",
    "    # save the series to disk to prevent data loss in case of crash\n",
    "    model_results_series.to_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4388b0",
   "metadata": {},
   "source": [
    "# Run all classifiers through the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df049c4",
   "metadata": {},
   "source": [
    "Creating a series to store the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0ba8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_results_sf = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9ce82",
   "metadata": {},
   "source": [
    "Pass every different classifier into the modelling pipeline:\n",
    "\n",
    "**! This takes a few minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4e25149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Shape of Contact train df after sentence filtering is: (366, 511)\n",
      "df_for_pipelining_train_SF: (366, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (366, 13708)\n",
      "classifier_y_train: (366,)\n",
      "classifier_X_val: (2651, 13708)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact was 4.33888\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Shape of Contact_E_Mail_Address train df after sentence filtering is: (557, 511)\n",
      "df_for_pipelining_train_SF: (557, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (557, 16513)\n",
      "classifier_y_train: (557,)\n",
      "classifier_X_val: (2651, 16513)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_E_Mail_Address was 0.94382\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Shape of Contact_Phone_Number train df after sentence filtering is: (487, 511)\n",
      "df_for_pipelining_train_SF: (487, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (487, 16970)\n",
      "classifier_y_train: (487,)\n",
      "classifier_X_val: (2651, 16970)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_Phone_Number was 0.97505\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Shape of Identifier_Cookie_or_similar_Tech train df after sentence filtering is: (679, 511)\n",
      "df_for_pipelining_train_SF: (679, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (679, 18904)\n",
      "classifier_y_train: (679,)\n",
      "classifier_X_val: (2651, 18904)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 1.55186\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Shape of Identifier_Device_ID train df after sentence filtering is: (271, 511)\n",
      "df_for_pipelining_train_SF: (271, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (271, 9529)\n",
      "classifier_y_train: (271,)\n",
      "classifier_X_val: (2651, 9529)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Device_ID was 0.50774\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Shape of Identifier_IMEI train df after sentence filtering is: (43, 511)\n",
      "df_for_pipelining_train_SF: (43, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (43, 1872)\n",
      "classifier_y_train: (43,)\n",
      "classifier_X_val: (2651, 1872)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_IMEI was 0.25879\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Shape of Identifier_MAC train df after sentence filtering is: (140, 511)\n",
      "df_for_pipelining_train_SF: (140, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (140, 7996)\n",
      "classifier_y_train: (140,)\n",
      "classifier_X_val: (2651, 7996)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_MAC was 0.40749\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Shape of Identifier_Mobile_Carrier train df after sentence filtering is: (69, 511)\n",
      "df_for_pipelining_train_SF: (69, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (69, 4623)\n",
      "classifier_y_train: (69,)\n",
      "classifier_X_val: (2651, 4623)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Mobile_Carrier was 0.29201\n",
      "\n",
      "Running for classifier: Location\n",
      "Shape of Location train df after sentence filtering is: (678, 511)\n",
      "df_for_pipelining_train_SF: (678, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (678, 20937)\n",
      "classifier_y_train: (678,)\n",
      "classifier_X_val: (2651, 20937)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location was 1.23163\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Shape of Location_Cell_Tower train df after sentence filtering is: (74, 511)\n",
      "df_for_pipelining_train_SF: (74, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (74, 3509)\n",
      "classifier_y_train: (74,)\n",
      "classifier_X_val: (2651, 3509)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_Cell_Tower was 0.3005\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Shape of Location_GPS train df after sentence filtering is: (133, 511)\n",
      "df_for_pipelining_train_SF: (133, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (133, 6103)\n",
      "classifier_y_train: (133,)\n",
      "classifier_X_val: (2651, 6103)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_GPS was 0.36753\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Shape of Location_WiFi train df after sentence filtering is: (101, 511)\n",
      "df_for_pipelining_train_SF: (101, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (101, 5253)\n",
      "classifier_y_train: (101,)\n",
      "classifier_X_val: (2651, 5253)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_WiFi was 0.32383\n",
      "\n",
      "Running for classifier: SSO\n",
      "Shape of SSO train df after sentence filtering is: (23, 511)\n",
      "df_for_pipelining_train_SF: (23, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (23, 1480)\n",
      "classifier_y_train: (23,)\n",
      "classifier_X_val: (2651, 1480)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for SSO was 0.23272\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Shape of Facebook_SSO train df after sentence filtering is: (23, 511)\n",
      "df_for_pipelining_train_SF: (23, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (23, 1480)\n",
      "classifier_y_train: (23,)\n",
      "classifier_X_val: (2651, 1480)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Facebook_SSO was 0.21235\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Shape of 1st_party train df after sentence filtering is: (7297, 511)\n",
      "df_for_pipelining_train_SF: (7297, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (7297, 108769)\n",
      "classifier_y_train: (7297,)\n",
      "classifier_X_val: (2651, 108769)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 1st_party was 33.67789\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Shape of 3rd_party train df after sentence filtering is: (4163, 511)\n",
      "df_for_pipelining_train_SF: (4163, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (4163, 76734)\n",
      "classifier_y_train: (4163,)\n",
      "classifier_X_val: (2651, 76734)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 3rd_party was 16.02854\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Shape of PERFORMED train df after sentence filtering is: (6742, 511)\n",
      "df_for_pipelining_train_SF: (6742, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (6742, 103405)\n",
      "classifier_y_train: (6742,)\n",
      "classifier_X_val: (2651, 103405)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for PERFORMED was 30.36679\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Shape of NOT_PERFORMED train df after sentence filtering is: (3413, 511)\n",
      "df_for_pipelining_train_SF: (3413, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (3413, 70104)\n",
      "classifier_y_train: (3413,)\n",
      "classifier_X_val: (2651, 70104)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for NOT_PERFORMED was 10.92314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    full_modelling_pipeline(each_classifier, model_results_series=initial_results_sf, \n",
    "                            sentence_filtering=True, inspect_flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475edc9",
   "metadata": {},
   "source": [
    "The results for any classifier can then be retrieved from the results series. The series stores the actual and predicted y-values in the form `[model_object, true_y, predicted_y]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d712ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.55      0.67      1974\n",
      "           1       0.36      0.75      0.49       677\n",
      "\n",
      "    accuracy                           0.60      2651\n",
      "   macro avg       0.61      0.65      0.58      2651\n",
      "weighted avg       0.74      0.60      0.62      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(initial_results_sf['Contact_E_Mail_Address'][1] , initial_results_sf['Contact_E_Mail_Address'][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd79937",
   "metadata": {},
   "source": [
    "# Creating model evaluation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e9818721",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model_results = pd.read_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200d0b6",
   "metadata": {},
   "source": [
    "I will display the results for all classifiers in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b479ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the classifiers as the index for our model results df\n",
    "initial_f1s = pd.DataFrame(initial_model_results, columns=[\"Neg F1\"]).copy()\n",
    "\n",
    "# then populate the dataframe with F1 scores by reference to the 'initial_model_results' series.\n",
    "# the initial_model_results series stores the actual and predicted y-values in the form [model_object, true_y, predicted_y]\n",
    "for index in initial_f1s.index:\n",
    "    initial_f1s.loc[index, \"Neg F1\"] = f1_score(initial_model_results[index][1].copy(), initial_model_results[index][2].copy(), pos_label=0)\n",
    "    initial_f1s.loc[index, \"Pos F1\"] = f1_score(initial_model_results[index][1].copy(), initial_model_results[index][2].copy(), pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6e4e3d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Pos F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>0.899445</td>\n",
       "      <td>0.638961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>0.669568</td>\n",
       "      <td>0.490168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>0.806461</td>\n",
       "      <td>0.593805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>0.726319</td>\n",
       "      <td>0.498129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>0.462056</td>\n",
       "      <td>0.452227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>0.00202</td>\n",
       "      <td>0.405178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>0.893707</td>\n",
       "      <td>0.574127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>0.580838</td>\n",
       "      <td>0.451220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>0.793005</td>\n",
       "      <td>0.601985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>0.132278</td>\n",
       "      <td>0.409509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>0.782924</td>\n",
       "      <td>0.547148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>0.405584</td>\n",
       "      <td>0.437018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>0.867858</td>\n",
       "      <td>0.221066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>0.867858</td>\n",
       "      <td>0.221066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>0.950561</td>\n",
       "      <td>0.859216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>0.95255</td>\n",
       "      <td>0.862601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>0.953737</td>\n",
       "      <td>0.866959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>0.947714</td>\n",
       "      <td>0.845867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Neg F1    Pos F1\n",
       "Contact                            0.899445  0.638961\n",
       "Contact_E_Mail_Address             0.669568  0.490168\n",
       "Contact_Phone_Number               0.806461  0.593805\n",
       "Identifier_Cookie_or_similar_Tech  0.726319  0.498129\n",
       "Identifier_Device_ID               0.462056  0.452227\n",
       "Identifier_IMEI                     0.00202  0.405178\n",
       "Identifier_MAC                     0.893707  0.574127\n",
       "Identifier_Mobile_Carrier          0.580838  0.451220\n",
       "Location                           0.793005  0.601985\n",
       "Location_Cell_Tower                0.132278  0.409509\n",
       "Location_GPS                       0.782924  0.547148\n",
       "Location_WiFi                      0.405584  0.437018\n",
       "SSO                                0.867858  0.221066\n",
       "Facebook_SSO                       0.867858  0.221066\n",
       "1st_party                          0.950561  0.859216\n",
       "3rd_party                           0.95255  0.862601\n",
       "PERFORMED                          0.953737  0.866959\n",
       "NOT_PERFORMED                      0.947714  0.845867"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10084084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative F1 mean: 0.7052489911821233\n",
      "Positive F1 mean: 0.5542360631222724\n"
     ]
    }
   ],
   "source": [
    "print(f\"Negative F1 mean: {initial_f1s['Neg F1'].mean()}\")\n",
    "print(f\"Positive F1 mean: {initial_f1s['Pos F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ebc42",
   "metadata": {},
   "source": [
    "Many things to discuss based on the above:\n",
    "- The two lowest scores\n",
    "- A wide range of scores\n",
    "- Neg F1 tends to be higher than Pos F1\n",
    "- Compare with results from the paper\n",
    "- Equal results for the two SSO classifiers\n",
    "\n",
    "\n",
    "# Discussion of low F1 scores\n",
    "There is one zero for negative F1 `Identifier_IMEI` Let's start by investigating `Identifier_IMEI`. \n",
    "\n",
    "A segment is annotated with 'Identifier_IMEI' if the segment describes how the company uses IMEI data to identify a customer. IMEI (International Mobile Equipment Identity) is a unique identification number all phone devices have, and can be used to track the history of the handset (including checking whether the phone has ever been reported as stolen).  First let's look at the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6cb8ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>4</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                   2                1972\n",
       "True Positive                   4                 673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Identifier_IMEI'][1].copy(), initial_model_results['Identifier_IMEI'][2].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306af2e",
   "metadata": {},
   "source": [
    "The classifier almost never predicted that this label is not present.  This is likely because in the subset of the data that the classifier was trained on, all of the observations were positive cases. This will be the result of the sentence filtering – the classifier was only trained using text segments that mentioned phrases related to IMEI, all of which happened to be annotated as indicating that the practice was described.\n",
    "\n",
    "This could mean that a rule-based classifier would be suitable for classifying the IMEI practice.  For a model-based classifier, I would need to use a wider range of features for sentence filtering, or would not conduct sentence filtering.\n",
    "\n",
    "We can see the features used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e434a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['imei', 'international mobile equipment', 'equipment id'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_annotation_features[clean_annotation_features['annotation']=='Identifier_IMEI']['features'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021f753",
   "metadata": {},
   "source": [
    "As expected there are very few related phrases, all of which are somewhat technical.\n",
    "\n",
    "Another implication could be that because IMEI is so specific, companies will not mention anything related to it in their privacy policies unless they collect this data.\n",
    "\n",
    "**Location Cell Tower**\n",
    "\n",
    "I think a similar problem has occurred here. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1efa8074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>142</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>31</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                 142                1832\n",
       "True Positive                  31                 646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Location_Cell_Tower'][1].copy(), initial_model_results['Location_Cell_Tower'][2].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae6c5d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['base station', 'cell tower', 'cell id', 'network-based', 'network based', 'cellular network location', 'approximate location', 'approximate geo', 'approximate device location', 'approximate device geo', 'approximate a location', 'coarse location', 'coarse geo', 'coarse device location', 'coarse device geo', 'rough location', 'rough geo', 'rough device location', 'rough device geo', 'imprecise location', 'imprecise geo', 'imprecise device location', 'imprecise device geo', 'located with less precision', 'non-precise location', 'non-precise geo', 'non-precise device location', 'non-precise device geo', 'non-specific location', 'non-specific geo', 'non-specific device location', 'not-specific device geo', 'not-specific location', 'not-specific geo', 'not-specific device location', 'not-specific device geo', 'general location', 'general geo', 'general device location', 'general device geo', 'generally where you are located', 'based on proximity'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_annotation_features[clean_annotation_features['annotation']=='Location_Cell_Tower']['features'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dff03",
   "metadata": {},
   "source": [
    "It looks like there is a similar problem with most of the training data probably having been positive cases, but the cause is less clear because there are a large number of different related crafted features.  By recollection from manually inspecting the frequencies of different crafted features, most of those related to location had surprisingly low frequency in the text because policies all tended to use some exact phrasing for location. As we can tell, the above crafted features are all around imprecise location.\n",
    "\n",
    "**Training this many classifiers**\n",
    "\n",
    "Given the large number of different classifiers being trained, there is a chance that at least one of them is poorly represented in the training set by chance of the random split, or other random factors in the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f89c99",
   "metadata": {},
   "source": [
    "## A wide range of scores\n",
    "\n",
    "With training this many classifiers a wide range of scores is expected, but the targets with the most data (the parties and modalities) have the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb61c7e",
   "metadata": {},
   "source": [
    "## Neg F1 tends to be higher than Pos F1\n",
    "\n",
    "This is partially likely due to imbalance in the data, which sentence filtering was supposed to help with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b1bef",
   "metadata": {},
   "source": [
    "## Compare with results from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8c304",
   "metadata": {},
   "source": [
    "My results are not particularly similar to those listed in the table on page 4.  Most notably the baseline models for Story et al. hardly ended up with any classifiers with low scores, so it's possible that the different ways that they manipulated the data had a significant effect.  The main differences were where in the process they applied tf-idf and how they did sentence filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244616ed",
   "metadata": {},
   "source": [
    "## Matching scores for SSO and FacebookSSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5f051",
   "metadata": {},
   "source": [
    "The SSO (\"Single Sign On\") annotation is applied to a segment if it discusses what the company does with the data used to facilitate a single sign on service, such as when you sign into an app through your google or facebook account.  This is always passed to the appropriate third party to enact the service. The scores for both \"SSO\" and \"Facebook SSO\" are the same because the crafted features and most of the annotations are the same.  Again let's start by looking at the confusion matrix for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df2456bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1967</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>592</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1967                   7\n",
       "True Positive                 592                  85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['SSO'][1].copy(), initial_model_results['SSO'][2].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d143e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1967</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>592</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1967                   7\n",
       "True Positive                 592                  85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Facebook_SSO'][1].copy(), initial_model_results['Facebook_SSO'][2].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d016e6",
   "metadata": {},
   "source": [
    "The proportions of both these annotations must be the same in the validation set. I think this could be due to an issue with my data manipulation as I expect there to be some difference.\n",
    "\n",
    "This time the model very rarely predicted that a segment contained a practice relating to SSO, most likely because there is such great class imbalance in the training data (not the validation data) even after applying sentence filtering. This has caused our model to be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c1a21008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7922\n",
       "1     146\n",
       "Name: SSO, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_pipelining_train['SSO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8076c",
   "metadata": {},
   "source": [
    "Yes, the proportions are hugely different in the training data and very imbalanced. For this classifier it definitely looks like we got unlucky with the train/validate split (assuming it was random)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d978f00",
   "metadata": {},
   "source": [
    "# Final model performance on test set\n",
    "\n",
    "Now I will train all classifiers using the Train and Validate data sets combined and test the score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "81016d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a4e82",
   "metadata": {},
   "source": [
    "I am yet to refactor the code to smoothly adapt my functions to work on the test set so I will have to re-assign the targets of the input variables to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "baf170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[(df_for_pipelining['policy_type'] == 'TRAINING') | (df_for_pipelining['policy_type'] == 'VALIDATION')].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "df_for_pipelining_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)\n",
    "\n",
    "df_for_pipelining_val = df_for_pipelining_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298d22",
   "metadata": {},
   "source": [
    "Running modelling pipeline for all classifiers\n",
    "\n",
    "**! This could take some time to run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67f30b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Shape of Contact train df after sentence filtering is: (480, 511)\n",
      "df_for_pipelining_train_SF: (480, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (480, 16471)\n",
      "classifier_y_train: (480,)\n",
      "classifier_X_val: (4824, 16471)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact was 4.07699\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Shape of Contact_E_Mail_Address train df after sentence filtering is: (731, 511)\n",
      "df_for_pipelining_train_SF: (731, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (731, 20188)\n",
      "classifier_y_train: (731,)\n",
      "classifier_X_val: (4824, 20188)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact_E_Mail_Address was 1.73435\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Shape of Contact_Phone_Number train df after sentence filtering is: (714, 511)\n",
      "df_for_pipelining_train_SF: (714, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (714, 22018)\n",
      "classifier_y_train: (714,)\n",
      "classifier_X_val: (4824, 22018)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact_Phone_Number was 1.9674\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Shape of Identifier_Cookie_or_similar_Tech train df after sentence filtering is: (921, 511)\n",
      "df_for_pipelining_train_SF: (921, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (921, 23175)\n",
      "classifier_y_train: (921,)\n",
      "classifier_X_val: (4824, 23175)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 2.82952\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Shape of Identifier_Device_ID train df after sentence filtering is: (356, 511)\n",
      "df_for_pipelining_train_SF: (356, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (356, 10816)\n",
      "classifier_y_train: (356,)\n",
      "classifier_X_val: (4824, 10816)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Device_ID was 1.06498\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Shape of Identifier_IMEI train df after sentence filtering is: (59, 511)\n",
      "df_for_pipelining_train_SF: (59, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (59, 2401)\n",
      "classifier_y_train: (59,)\n",
      "classifier_X_val: (4824, 2401)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_IMEI was 0.43859\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Shape of Identifier_MAC train df after sentence filtering is: (169, 511)\n",
      "df_for_pipelining_train_SF: (169, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (169, 9085)\n",
      "classifier_y_train: (169,)\n",
      "classifier_X_val: (4824, 9085)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_MAC was 0.67715\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Shape of Identifier_Mobile_Carrier train df after sentence filtering is: (102, 511)\n",
      "df_for_pipelining_train_SF: (102, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (102, 5617)\n",
      "classifier_y_train: (102,)\n",
      "classifier_X_val: (4824, 5617)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Mobile_Carrier was 0.47136\n",
      "\n",
      "Running for classifier: Location\n",
      "Shape of Location train df after sentence filtering is: (920, 511)\n",
      "df_for_pipelining_train_SF: (920, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (920, 25911)\n",
      "classifier_y_train: (920,)\n",
      "classifier_X_val: (4824, 25911)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location was 2.4156\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Shape of Location_Cell_Tower train df after sentence filtering is: (105, 511)\n",
      "df_for_pipelining_train_SF: (105, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (105, 3973)\n",
      "classifier_y_train: (105,)\n",
      "classifier_X_val: (4824, 3973)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_Cell_Tower was 0.51688\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Shape of Location_GPS train df after sentence filtering is: (184, 511)\n",
      "df_for_pipelining_train_SF: (184, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (184, 7304)\n",
      "classifier_y_train: (184,)\n",
      "classifier_X_val: (4824, 7304)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_GPS was 0.61649\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Shape of Location_WiFi train df after sentence filtering is: (137, 511)\n",
      "df_for_pipelining_train_SF: (137, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (137, 6154)\n",
      "classifier_y_train: (137,)\n",
      "classifier_X_val: (4824, 6154)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_WiFi was 0.59517\n",
      "\n",
      "Running for classifier: SSO\n",
      "Shape of SSO train df after sentence filtering is: (31, 511)\n",
      "df_for_pipelining_train_SF: (31, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (31, 1671)\n",
      "classifier_y_train: (31,)\n",
      "classifier_X_val: (4824, 1671)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for SSO was 0.42088\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Shape of Facebook_SSO train df after sentence filtering is: (31, 511)\n",
      "df_for_pipelining_train_SF: (31, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (31, 1671)\n",
      "classifier_y_train: (31,)\n",
      "classifier_X_val: (4824, 1671)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Facebook_SSO was 0.36426\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Shape of 1st_party train df after sentence filtering is: (9730, 511)\n",
      "df_for_pipelining_train_SF: (9730, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (9730, 135540)\n",
      "classifier_y_train: (9730,)\n",
      "classifier_X_val: (4824, 135540)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for 1st_party was 63.04887\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Shape of 3rd_party train df after sentence filtering is: (5550, 511)\n",
      "df_for_pipelining_train_SF: (5550, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (5550, 94818)\n",
      "classifier_y_train: (5550,)\n",
      "classifier_X_val: (4824, 94818)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for 3rd_party was 29.76391\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Shape of PERFORMED train df after sentence filtering is: (9037, 511)\n",
      "df_for_pipelining_train_SF: (9037, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (9037, 129163)\n",
      "classifier_y_train: (9037,)\n",
      "classifier_X_val: (4824, 129163)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for PERFORMED was 57.33238\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Shape of NOT_PERFORMED train df after sentence filtering is: (4563, 511)\n",
      "df_for_pipelining_train_SF: (4563, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (4563, 86310)\n",
      "classifier_y_train: (4563,)\n",
      "classifier_X_val: (4824, 86310)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for NOT_PERFORMED was 19.6191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    full_modelling_pipeline(each_classifier, model_results_series=final_results, \n",
    "                            sentence_filtering=True, inspect_flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3437a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_results = pd.read_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc68380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Pos F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>0.91178</td>\n",
       "      <td>0.633547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>0.386906</td>\n",
       "      <td>0.320734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>0.81761</td>\n",
       "      <td>0.518854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>0.728218</td>\n",
       "      <td>0.427560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>0.566071</td>\n",
       "      <td>0.371892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.313135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>0.915175</td>\n",
       "      <td>0.556488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>0.472029</td>\n",
       "      <td>0.353999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>0.741646</td>\n",
       "      <td>0.475503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>0.535186</td>\n",
       "      <td>0.372625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>0.786771</td>\n",
       "      <td>0.466231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>0.442146</td>\n",
       "      <td>0.342367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>0.908371</td>\n",
       "      <td>0.256846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>0.908371</td>\n",
       "      <td>0.256846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>0.962497</td>\n",
       "      <td>0.843179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>0.95811</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>0.96184</td>\n",
       "      <td>0.840751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>0.957721</td>\n",
       "      <td>0.818032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Neg F1    Pos F1\n",
       "Contact                             0.91178  0.633547\n",
       "Contact_E_Mail_Address             0.386906  0.320734\n",
       "Contact_Phone_Number                0.81761  0.518854\n",
       "Identifier_Cookie_or_similar_Tech  0.728218  0.427560\n",
       "Identifier_Device_ID               0.566071  0.371892\n",
       "Identifier_IMEI                    0.004063  0.313135\n",
       "Identifier_MAC                     0.915175  0.556488\n",
       "Identifier_Mobile_Carrier          0.472029  0.353999\n",
       "Location                           0.741646  0.475503\n",
       "Location_Cell_Tower                0.535186  0.372625\n",
       "Location_GPS                       0.786771  0.466231\n",
       "Location_WiFi                      0.442146  0.342367\n",
       "SSO                                0.908371  0.256846\n",
       "Facebook_SSO                       0.908371  0.256846\n",
       "1st_party                          0.962497  0.843179\n",
       "3rd_party                           0.95811  0.819582\n",
       "PERFORMED                           0.96184  0.840751\n",
       "NOT_PERFORMED                      0.957721  0.818032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative F1 mean: 0.7202505215060437\n",
      "Positive F1 mean: 0.4993427790530155\n"
     ]
    }
   ],
   "source": [
    "# take the classifiers as the index for our model results df\n",
    "final_f1s = pd.DataFrame(final_model_results, columns=[\"Neg F1\"]).copy()\n",
    "\n",
    "# then populate the dataframe with F1 scores by reference to the 'initial_model_results' series.\n",
    "# the initial_model_results series stores the actual and predicted y-values in the form [model_object, true_y, predicted_y]\n",
    "for index in final_f1s.index:\n",
    "    final_f1s.loc[index, \"Neg F1\"] = f1_score(final_model_results[index][1].copy(), final_model_results[index][2].copy(), pos_label=0)\n",
    "    final_f1s.loc[index, \"Pos F1\"] = f1_score(final_model_results[index][1].copy(), final_model_results[index][2].copy(), pos_label=1)\n",
    "display(final_f1s)\n",
    "print(f\"Negative F1 mean: {final_f1s['Neg F1'].mean()}\")\n",
    "print(f\"Positive F1 mean: {final_f1s['Pos F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa691d98",
   "metadata": {},
   "source": [
    "Interestingly the score for Contact_E_Mail_Address has gone down significantly. It would be very interesting to explore why this might be.\n",
    "\n",
    "The scores for SSO have only increased by a small amount despite the class imbalance issue potentially being solved by using the train and validate data.\n",
    "\n",
    "The mean F1 scores have remained largely the same and are still much lower for Positive F1.  I feel that this is still due to such imbalance across the dataset but would like to explore more.\n",
    "\n",
    "One conclusion is that by increasing the amount of data being used, the model performance has not really increased, meaning that the limiting factors are probably in the model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf36bf",
   "metadata": {},
   "source": [
    "# Further work\n",
    "\n",
    "A next step is to implement an ability of the modelling pipeline to remove sentence filtering from the flow.  It is causing some issues in my model scores and Story et al. found that some classifiers' performance improves without it.\n",
    "\n",
    "I would then work on other preprocessing steps:\n",
    "- I have not applied scaling to the data. Every column is between 0 and 1 anyway so the effect would be small but it could have big effect for some classifiers.\n",
    "- tf-idf: \n",
    "    - I would explore training classifiers without bigrams as Story et al. also explored this and found it to be helpful for a small number of classifiers \n",
    "    - Fixing data leak: currently, the tf-idf transformer is fitted and applied to the entire train set before cross-validation. The effect is that some of the \n",
    "\n",
    "This leads on to a more general issue that I have created a series of functions but none of my own classes, so I would refactor into a more object-oriented framework.\n",
    "\n",
    "Other types of models:\n",
    "- Naive bayes classifiers and ensemble models such as random forest have also shown to perform well in the NLP domain and I would wish to see the performance of these models too.\n",
    "- Neural Network architecture is also likely to get very strong performance, although may require more investment and training resources.\n",
    "\n",
    "Deeper exploration into the effect of different models in this domain\n",
    "\n",
    "- It would be insightful to explore why different preprocessing steps or why different models have different effects on different classifiers when trained on this dataset\n",
    "\n",
    "**Thanks for reading!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
