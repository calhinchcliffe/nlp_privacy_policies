{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05253c1",
   "metadata": {},
   "source": [
    "<h1> Preprocessing<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "In this notebook, I follow the text preprocessing steps described by Story et al. (Story et al. (2019) \"Natural Language Processing for Mobile App Privacy Compliance\", available from https://usableprivacy.org/publications)\n",
    "\n",
    "Many of the steps described by them are somewhat vague so this is not a true replication.\n",
    "\n",
    "The purpose of these steps are to improve the performance of the models, to more accurately populate the crafted features data, and there are additional advantages to other tasks that could be done with the data (that I have not taken advantage of, such as more accurate EDA).\n",
    "\n",
    "The steps described by Story et al. are:\n",
    "\n",
    "- Normalize whitespace\n",
    "- Normalize punctuation\n",
    "- Remove non-ASCII characters\n",
    "- Make all text lowercase\n",
    "\n",
    "Following that, I then load the 'crafted features' provided by Story et al. and I find some issues in the clenliness so I conduct the same steps to these too.\n",
    "\n",
    "Finally I append the crafted features to the dataframe.  The data will then be ready for modelling.\n",
    "\n",
    "While previously I looked at classifiers at the sentence level, going forwards I will investigate Story et al.'s work more closely and so will process the text at the segment level.\n",
    "\n",
    "**Crafted features**\n",
    "\n",
    "To help to create accurate classifiers, columns will be added to the dataframe that contain key phrases that may be found in segment that has been annotated with a specific annotation. For example, the phrases 'phone book', 'phonebook' or 'address book' could be found in segments that have been annotated with the Contact_Address_Book annotation and adding these phrases as columns could help a classifier to correctly identify Contact_Address_Book.\n",
    "\n",
    "Story et al. created these features based on their expertise and findings across the train and validation policies and have made them available along with the data.\n",
    "\n",
    "**A note on my code**\n",
    "\n",
    "Although these functions are only being applied to this dataset once, I still write them out as functions as they can in principle be modified for other projects and it is easier to verify that each function works in function format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094785f7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Confirm-all-text-data-is-string-format\" data-toc-modified-id=\"Confirm-all-text-data-is-string-format-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Confirm all text data is string format</a></span></li><li><span><a href=\"#Setting-up-normalization-functions\" data-toc-modified-id=\"Setting-up-normalization-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setting up normalization functions</a></span></li><li><span><a href=\"#Normalize-Whitespace\" data-toc-modified-id=\"Normalize-Whitespace-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Normalize Whitespace</a></span></li><li><span><a href=\"#Normalize-punctuation\" data-toc-modified-id=\"Normalize-punctuation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Normalize punctuation</a></span></li><li><span><a href=\"#Remove-non-ASCII-characters\" data-toc-modified-id=\"Remove-non-ASCII-characters-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Remove non-ASCII characters</a></span></li><li><span><a href=\"#Make-all-policy-text-lowercase\" data-toc-modified-id=\"Make-all-policy-text-lowercase-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Make all policy text lowercase</a></span></li><li><span><a href=\"#Same-pre-processing-steps-for-Crafted-Features\" data-toc-modified-id=\"Same-pre-processing-steps-for-Crafted-Features-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Same pre-processing steps for Crafted Features</a></span></li><li><span><a href=\"#Append-crafted-features-to-dataframe\" data-toc-modified-id=\"Append-crafted-features-to-dataframe-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Append crafted features to dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-crafted-features\" data-toc-modified-id=\"Load-crafted-features-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Load crafted features</a></span></li><li><span><a href=\"#Add-crafted-features-columns-to-df\" data-toc-modified-id=\"Add-crafted-features-columns-to-df-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Add crafted features columns to df</a></span></li></ul></li><li><span><a href=\"#Saving-the-dataframe-to-be-used-for-modelling\" data-toc-modified-id=\"Saving-the-dataframe-to-be-used-for-modelling-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Saving the dataframe to be used for modelling</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588ec3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3f4a4",
   "metadata": {},
   "source": [
    "# Confirm all text data is string format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746402d",
   "metadata": {},
   "source": [
    "I realised that it is possible that a segment is not correctly stored in string format, so firstly I wish to confirm that every segment in my dataframe is stored in string format.\n",
    "\n",
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_segment_annots_df = pd.read_pickle('objects/segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6cc49",
   "metadata": {},
   "source": [
    "Writing and verifying a function to confirm the datatype of every cell in a pandas dataframe column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b93e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def column_all_dtype(dataframe_column, dtype):\n",
    "    \"\"\"\n",
    "    Inputs: a specific dtype\n",
    "    Example inputs for dtype: str, \"<class 'numpy.int64'>\", \"<class 'numpy.bool_'>\", \"<class 'list'>\"\n",
    "    Outputs: True or False appropriately depending on whether any item in the dataframe column is not the dtype passed.\n",
    "    \"\"\"\n",
    "    \n",
    "    for _index in range(len(dataframe_column)):\n",
    "        if str(type(dataframe_column[_index])) != str(dtype):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# validation on columns of known type:\n",
    "print(column_all_dtype(clean_segment_annots_df['policy_segment_id'], \"<class 'numpy.int64'>\") ) # should return True\n",
    "print(column_all_dtype(clean_segment_annots_df['policy_segment_id'], str) ) # should return False\n",
    "print(column_all_dtype(clean_segment_annots_df['contains_synthetic'], \"<class 'numpy.bool_'>\") ) # should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461083f",
   "metadata": {},
   "source": [
    "Confirming all my text data is in string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2e7298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_all_dtype(clean_segment_annots_df['segment_text'], str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ab0fe",
   "metadata": {},
   "source": [
    "This can also be verified by the fact that every cell has the same type (the number of unique types is 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ad46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_segment_annots_df['segment_text'].apply(type).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09defb0f",
   "metadata": {},
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa05b74",
   "metadata": {},
   "source": [
    "# Setting up normalization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7f0c6",
   "metadata": {},
   "source": [
    "For some of the below functions, to see that they worked, I want to check the total length of all strings in the DataFrame before and after running the function.  I can use a decorator to append these lines to the following functions where appropriate.\n",
    "\n",
    "I want to check the length of all the text before and after to see any difference made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b3dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total length\n",
    "def total_length(dataframe):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with a column named \"segment_text\" \n",
    "    Returns the sum of the string length of each cell in the column\n",
    "    \"\"\"\n",
    "    total_length_all_segments = dataframe[\"segment_text\"].str.len().sum()\n",
    "    return f\"Total length of all segments is {total_length_all_segments}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f47fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5917918"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_segment_annots_df[\"segment_text\"].str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c11d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(func, dataframe):\n",
    "    \"\"\"\n",
    "    Prints the length of the dataframe before and after running the function.\n",
    "    Inputs: \n",
    "    - Any function that can take the dataframe as an input\n",
    "    - A pandas dataframe\n",
    "    Actions:\n",
    "    Passing the dataframe as an argument for each function: \n",
    "    Prints the output of the total_length function, \n",
    "    then calls the function passed as argument to this check_length function,\n",
    "    then prints the total_length function again\n",
    "    Outputs: none\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Before running function: {total_length(dataframe)}\")\n",
    "    \n",
    "    func(dataframe)\n",
    "    \n",
    "    print(f\"After running function: {total_length(dataframe)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582f563",
   "metadata": {},
   "source": [
    "# Normalize Whitespace\n",
    "\n",
    "Using `new_string = \" \".join(old_string.split())`.  The `.split()` function considers a range of forms of whitespace.\n",
    "\n",
    "I want to check the length of all the text before and after to see any difference made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c38cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(dataframe):\n",
    "    \"\"\"\n",
    "    Removed whitespace from all cells in the \"segment_text\" column.\n",
    "    For verification, call this function within the 'check_length' function.\n",
    "    Input: Dataframe with a column called \"segment_text\" that contains strings for the whitespace to be removed.\n",
    "    Returns: nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(lambda x: \" \".join(x.split()))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237aefe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917918\n",
      "After running function: Total length of all segments is 5917918\n"
     ]
    }
   ],
   "source": [
    "check_length(normalize_whitespace, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b13ac",
   "metadata": {},
   "source": [
    "Suspiciously nothing changed, so I will verify my function before concluding that the privacy policy segments have no whitespace in them.\n",
    "\n",
    "Verifying the function by testing it on some whitespace.\n",
    "- create  a dataframe where I have intentionally added whitespace \n",
    "- calling the function on this dataframe\n",
    "- confirming the whitespace is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65180163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917320\n",
      "After running function: Total length of all segments is 5917279\n"
     ]
    }
   ],
   "source": [
    "text_with_space = 'PRIVACY                                          This.'\n",
    "verify_whitespace_df = clean_segment_annots_df.copy()\n",
    "verify_whitespace_df.loc[0, 'segment_text'] = text_with_space\n",
    "check_length(normalize_whitespace,verify_whitespace_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76088696",
   "metadata": {},
   "source": [
    "I have verified that my function works and so can conclude that the privacy policy segments have no whitespace in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66297e7a",
   "metadata": {},
   "source": [
    "# Normalize punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd5e24",
   "metadata": {},
   "source": [
    "I took the below function from this towardsdatascience article [here](https://towardsdatascience.com/text-normalization-7ecc8e084e31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69da5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simplify_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function simplifies doubled or more complex punctuation. The exception is '...'.\n",
    "    \"\"\"\n",
    "    \n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n",
    "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ef8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_punctuation(dataframe):\n",
    "    \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(_simplify_punctuation)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d209643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917918\n",
      "After running function: Total length of all segments is 5917879\n"
     ]
    }
   ],
   "source": [
    "check_length(remove_duplicate_punctuation, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614dd1da",
   "metadata": {},
   "source": [
    "Only a small number of characters were removed as expected.\n",
    "\n",
    "Further punctuation normalization such as converting other characters to their english standardized versions (e.g. the opening speachmark “ to \", or elipses … to ...) would be ideal, but the ommission of this should not affect the sentence filtering much, and won't have any effect on the tf-idf matrix, because it ignores punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a20351",
   "metadata": {},
   "source": [
    "# Remove non-ASCII characters\n",
    "\n",
    "This can be done by checking whether each character has a unicode index below 128, as ASCII characters are coded above 128.  Checking the unicode 'code point' is done with `ord(char)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a7aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "a_b^0\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(string):\n",
    "    \"\"\"\n",
    "    I found this function on this website: https://bobbyhadz.com/blog/python-remove-non-ascii-characters-from-string\n",
    "    \"\"\"\n",
    "    return ''.join(char for char in string if ord(char) < 128)\n",
    "\n",
    "# demonstrate function:\n",
    "print(remove_non_ascii('a€bñcá')) # >> 'abc'\n",
    "print(remove_non_ascii('a_b^0')) # >> a_b^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f16ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonASCII_chars(dataframe):\n",
    "        \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(remove_non_ascii)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd8b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917879\n",
      "After running function: Total length of all segments is 5901658\n"
     ]
    }
   ],
   "source": [
    "check_length(remove_nonASCII_chars, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dbcec",
   "metadata": {},
   "source": [
    "Thousands of characters were removed, representing nearly .3% of all characters.  I hope that this makes at least a small difference for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c635d39",
   "metadata": {},
   "source": [
    "# Make all policy text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f53e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ifiufiwunfiijnf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      segment_text\n",
       "0  ifiufiwunfiijnf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_lowercase(dataframe):\n",
    "\n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].str.lower()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# verify\n",
    "sample_df = pd.DataFrame([\"ifiUFIWUNFIijnf\"], columns=[\"segment_text\"])\n",
    "convert_to_lowercase(sample_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ae5487b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    privacy policy this privacy policy (hereafter ...\n",
       "1    1. about our products 1.1 our products offer a...\n",
       "2    2. the information we collect the information ...\n",
       "Name: segment_text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_lowercase(clean_segment_annots_df)\n",
    "clean_segment_annots_df['segment_text'].head(3) # verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5d06c",
   "metadata": {},
   "source": [
    "It can be seen that the text has been changed to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1490337",
   "metadata": {},
   "source": [
    "**Save the above cleaned dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_segment_annots_df.to_pickle('objects/clean_segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807aeda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4297a",
   "metadata": {},
   "source": [
    "# Same pre-processing steps for Crafted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068c7cd",
   "metadata": {},
   "source": [
    "I will need to populate the segment dataframe with crafted features, but if the segments and the crafted features are in different formats, it will be more difficult to do so.  So I will check and standardize the format of the crafted features too.\n",
    "\n",
    "I save the crafted features and the annotations that they refer to in the variable `annotation features`.\n",
    "\n",
    "Load the list of all the crafted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8500382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_features = pd.read_pickle('objects/annotation_features.pkl')\n",
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "len(list_all_crafted_features) # verify – should be 579 crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffbf32",
   "metadata": {},
   "source": [
    "These crafted features are not quite in the same format as the main dataframe so I cannot apply my functions to them. I won't check for doubled punctuation. To capture non-ASCII characters and uppercase letters I only need to check which non-lowercase letters there are.\n",
    "\n",
    "Checking for any characters that are not lowercase english alphabet characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79ee3520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', ',', '-', '\\xa0', '/', 'S', 'N', 'U', 'T', 'P', 'A', '(', ')', 'I', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "def non_asciis():\n",
    "    list_of_chars = []\n",
    "    for ft in list_all_crafted_features:\n",
    "        for char in ft:\n",
    "            if char.islower() == False: #aka if it's not lowercase\n",
    "                if char not in list_of_chars:\n",
    "                    list_of_chars.append(char)\n",
    "    return list_of_chars\n",
    "print(non_asciis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959fefc",
   "metadata": {},
   "source": [
    "By inspecting this list I can see that the only characters that I don't expect are the uppercase letters and \"\\xa0\", which represents a type of whitespace.  There are no non-ASCII characters so I don't need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb541a76",
   "metadata": {},
   "source": [
    "I also noticed while manually browsing the features that Bluetooth was not listed because it had been incorrectly entered as 'bluethooth', so I will correct that now too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bb04d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "\"bluethooth\" in list_all_crafted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754490ba",
   "metadata": {},
   "source": [
    "Correcting typo and normalizing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6587fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _row in range(len(annotation_features)):\n",
    "    crafted_feature_list = annotation_features.at[_row, 'features']\n",
    "\n",
    "    # correct \"Bluetooth\"\n",
    "    new_crafted_feature_list = [\"bluetooth\" if feature==\"bluethooth\" else feature for feature in crafted_feature_list]\n",
    "    \n",
    "    # change to lowercase\n",
    "    new_crafted_feature_list = [feature.lower() for feature in new_crafted_feature_list]\n",
    "    \n",
    "    # normalize whitespace\n",
    "    new_crafted_feature_list = [\" \".join(feature.split()) for feature in new_crafted_feature_list]\n",
    "\n",
    "    \n",
    "    annotation_features.at[_row, 'features'] = new_crafted_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f24c3",
   "metadata": {},
   "source": [
    "Checking again that the bluetooth typo was corrected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9afbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "\"bluethooth\" in list_all_crafted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ac399",
   "metadata": {},
   "source": [
    "Checking again for non-lowercase characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae166c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_all_crafted_features)) # verify – should be 579 crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04a4d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', ',', '-', '/', '(', ')', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(non_asciis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154dda9",
   "metadata": {},
   "source": [
    "All problematic characters are now removed so this list of features can be used for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12c2b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "annotation_features.to_pickle('objects/clean_annotation_features.pkl')\n",
    "confirm_save_0 = pd.read_pickle('objects/clean_annotation_features.pkl')\n",
    "print(annotation_features.shape == confirm_save_0.shape)\n",
    "print(confirm_save_0.equals(annotation_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6284f80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20e7c1",
   "metadata": {},
   "source": [
    "# Append crafted features to dataframe\n",
    "\n",
    "## Load crafted features\n",
    "\n",
    "The next steps are to:\n",
    "- 1. Append each feature as a column to the dataframe\n",
    "- 2. Populate the columns\n",
    "\n",
    "Then I can move to modelling.\n",
    "\n",
    "I already have a function to help with 1 called `add_empty_annotation_columns`.  I just need to put the new features into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b24ec8",
   "metadata": {},
   "source": [
    "First though, I want to check whether any of the features are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a54cd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_annotation_features = pd.read_pickle('objects/clean_annotation_features.pkl')\n",
    "clean_segment_annots_df = pd.read_pickle('objects/clean_segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4541c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_crafted_features = [feature for row in clean_annotation_features['features'] for feature in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b0ae5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = []\n",
    "duplicate_features = []\n",
    "for feature in list_all_crafted_features:\n",
    "    if feature in all_features:\n",
    "        duplicate_features.append(feature)\n",
    "    all_features.append(feature)\n",
    "len(duplicate_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4ede5",
   "metadata": {},
   "source": [
    "Oddly, a lot of the features are exactly the same. I will remove these duplicates after adding them to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3b184",
   "metadata": {},
   "source": [
    "## Add crafted features columns to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8680a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n",
      "(15543, 41)\n",
      "The shape of the returned dataframe is (15543, 620)\n"
     ]
    }
   ],
   "source": [
    "print(len(list_all_crafted_features))\n",
    "print(clean_segment_annots_df.shape)\n",
    "crafted_features_df = priv_pol_funcs.add_empty_annotation_columns(clean_segment_annots_df, list_all_crafted_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d26787",
   "metadata": {},
   "source": [
    "Verify that the features have been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8edacbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <th>contact info</th>\n",
       "      <th>contact details</th>\n",
       "      <th>contact data</th>\n",
       "      <th>e.g., your name</th>\n",
       "      <th>contact you</th>\n",
       "      <th>your contact</th>\n",
       "      <th>identify, contact</th>\n",
       "      <th>identifying information</th>\n",
       "      <th>your name, address, and e-mail address</th>\n",
       "      <th>...</th>\n",
       "      <th>never be acquired</th>\n",
       "      <th>never be viewed</th>\n",
       "      <th>never be located</th>\n",
       "      <th>never be asked</th>\n",
       "      <th>never be utilized</th>\n",
       "      <th>never be requested</th>\n",
       "      <th>never be transmitted</th>\n",
       "      <th>never be communicated</th>\n",
       "      <th>nor do we collect</th>\n",
       "      <th>does not tell us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 580 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOT_PERFORMED  contact info  contact details  contact data  \\\n",
       "0              0             0                0             0   \n",
       "1              0             0                0             0   \n",
       "\n",
       "   e.g., your name  contact you  your contact  identify, contact  \\\n",
       "0                0            0             0                  0   \n",
       "1                0            0             0                  0   \n",
       "\n",
       "   identifying information  your name, address, and e-mail address  ...  \\\n",
       "0                        0                                       0  ...   \n",
       "1                        0                                       0  ...   \n",
       "\n",
       "   never be acquired  never be viewed  never be located  never be asked  \\\n",
       "0                  0                0                 0               0   \n",
       "1                  0                0                 0               0   \n",
       "\n",
       "   never be utilized  never be requested  never be transmitted  \\\n",
       "0                  0                   0                     0   \n",
       "1                  0                   0                     0   \n",
       "\n",
       "   never be communicated  nor do we collect  does not tell us  \n",
       "0                      0                  0                 0  \n",
       "1                      0                  0                 0  \n",
       "\n",
       "[2 rows x 580 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df.iloc[:,40:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ff022",
   "metadata": {},
   "source": [
    "We can see that the crafted features are all columns from column 41 to the end.  Now let's remove the duplicate feature columns before populating them all. I expect 103 columns to be removed to bring the `crafted_features_df` down to 517 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d48c302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15543, 517)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df = crafted_features_df.loc[:,~crafted_features_df.columns.duplicated()] # remove columns with duplicate names\n",
    "crafted_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00ab88",
   "metadata": {},
   "source": [
    "Perfect, the right number of columns have been removed.\n",
    "\n",
    "Now to populate the crafted features columns, I will:\n",
    "\n",
    "- Take the column name for each crafted feature\n",
    "- take the segment text for each row\n",
    "- if column name in segment text: put 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df312cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 203 ms, total: 28.8 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_rows = range(len(crafted_features_df)) # index of rows to loop through\n",
    "\n",
    "for column_number in range(41, 517): # Looping through each column with a feature\n",
    "\n",
    "    column_name = crafted_features_df.columns[column_number] # for that column feature\n",
    "\n",
    "    for row in all_rows: # and for every row\n",
    "        if column_name in crafted_features_df.at[row, \"segment_text\"]: # if the segment has that feature\n",
    "            crafted_features_df.at[row, column_name] = 1 # make the value for that feature on that row equal 1\n",
    "    \n",
    "    print(f\"Processing {column_number}/517\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38fdf3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 features have not been populated\n"
     ]
    }
   ],
   "source": [
    "# looking at some of the results to verify\n",
    "summations = crafted_features_df.iloc[:,41:].sum()\n",
    "print(f\"{(summations==0).sum()} features have not been populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91616fda",
   "metadata": {},
   "source": [
    "This seems like a lot of empty columns, so I manually looked through the results, as well as checking the source text, and found that most of the crafted feature columns that haven't been populated are generally:\n",
    "- unusual ways of typing a phrase (example: 'post code' instead of postcode)\n",
    "- specific phrases for uncommon data practices (example: 'exact device location')\n",
    "- negative phrases (example: never be requested)\n",
    "\n",
    "These features would have been included by the researchers to capture phrases that don't feature in their dataset but could feature when applying their model beyond their training data.\n",
    "\n",
    "Overall this seems roughly correct so I will use it for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16120c",
   "metadata": {},
   "source": [
    "<font size= \"3\"> **Some final tidying:** <font/>\n",
    "\n",
    "Confirming that every number across the target and feature columns equal 0 or 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "994e5f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(crafted_features_df.iloc[:,7:] == 0 # every cell equals 0\n",
    " | (crafted_features_df.iloc[:,7:] == 1) # or every cell equals 1\n",
    ").nunique().nunique() # All columns and rows in the resulting dataframe of booleans only show one result (True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490071cf",
   "metadata": {},
   "source": [
    "Alternate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a698f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df.iloc[:,7:].isin([0, 1]).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37145695",
   "metadata": {},
   "source": [
    "Changing the dtype of those same columns to int8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c1bad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtypes are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int64      482\n",
       "float64     28\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the dtypes are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int8    510\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"The dtypes are:\")\n",
    "display(crafted_features_df.iloc[:,7:].dtypes.value_counts())\n",
    "\n",
    "subset_df = crafted_features_df.iloc[:,7:].copy()\n",
    "subset_df = subset_df.astype('int8')\n",
    "crafted_features_df.iloc[:,7:] = subset_df\n",
    "\n",
    "print(f\"Now the dtypes are:\")\n",
    "display(crafted_features_df.iloc[:,7:].dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e532f1",
   "metadata": {},
   "source": [
    "# Saving the dataframe to be used for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c4a83",
   "metadata": {},
   "source": [
    "As before, to make it faster to load this dataframe in this notebook and others, I will save this dataframe as a pickle file.  This allows the below code to be ran without waiting for the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e9c64479",
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted_features_df.to_pickle('objects/crafted_features_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f18817",
   "metadata": {},
   "source": [
    "Verifying that the file was correctly saved and can be imported properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "39a25967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "confirm_save_5 = pd.read_pickle('objects/crafted_features_df.pkl')\n",
    "print(crafted_features_df.shape == confirm_save_5.shape)\n",
    "print(confirm_save_5.equals(crafted_features_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd4c2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I now have a dataframe with cleaned text, crafted features, and all targets.  Now I will pass these into a modelling pipeline to follow the steps of Story et al."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
