{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05253c1",
   "metadata": {},
   "source": [
    "<h1> Preprocessing<span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "In this notebook, I follow the text preprocessing steps described by Story et al. (Story et al. (2019) \"Natural Language Processing for Mobile App Privacy Compliance\", available from https://usableprivacy.org/publications). While previously I looked at classifiers at the sentence level, going forwards I will replicate Story et al.'s work more closely by processing the text at the segment level.\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "- Four text cleaning steps for the policy text\n",
    "- Loading and cleaning the 'Crafted Features'\n",
    "- Appending the crafted features to the dataframe\n",
    "- Sentence Filtering and its relevance for training different classifiers\n",
    "\n",
    "Story et al's descriptions of these steps are somewhat vague so this is not a true replication.\n",
    "\n",
    "**Four Text Cleaning Steps**\n",
    "\n",
    "The purpose of these steps are to improve the performance of the models, to more accurately populate the crafted features data, and there are additional advantages to other tasks that could be done with the data (that I have not taken advantage of, such as more accurate EDA).\n",
    "\n",
    "The steps described by Story et al. are:\n",
    "\n",
    "- Normalize whitespace\n",
    "- Normalize punctuation\n",
    "- Remove non-ASCII characters\n",
    "- Make all text lowercase\n",
    "\n",
    "Following that, I then load the 'crafted features' provided by Story et al. and I find some issues in the clenliness so I conduct the same steps to these too.\n",
    "\n",
    "Finally I append the crafted features to the dataframe.  The data will then be ready for modelling.\n",
    "\n",
    "**What are 'Crafted features'?**\n",
    "\n",
    "To help to create accurate classifiers, columns will be added to the dataframe that contain key phrases that may be found in segments that has been annotated with a specific annotation. For example, the phrases 'phone book', 'phonebook' or 'address book' could be found in segments that have been annotated with the Contact_Address_Book annotation and adding these phrases as columns could help a classifier to correctly identify Contact_Address_Book.\n",
    "\n",
    "Story et al. created these features based on their expertise and findings across the train and validation policies and have made them available along with the data.\n",
    "\n",
    "**What is 'Sentence Filtering'?**\n",
    "\n",
    "To try to improve classifier performance, Story et al. apply a preprocessing technique that they call Sentence Filtering. This involves filtering the data to only train a classifier on segments that contain a relevant feature for the target. For example a short segment about location data wouldn't be used to train an email classifier because it wouldn't contain keywords such as \"Email\" or \"Contact\".\n",
    "\n",
    "They do not explicitly specify that these features are equivalent to the crafted features but I am inferring that they are.\n",
    "\n",
    "The effect of applying sentence filtering is that instead of a model being trained on all the data and there being class imbalance in favour of the negative class, a much greater proportion of the training data features the target being classified. The effect is similar to downsampling.\n",
    "\n",
    "**A note on my code**\n",
    "\n",
    "Although these functions are only being applied to this dataset once, I still write them out as functions as they can in principle be modified for other projects and it is easier to verify that each function works in function format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094785f7",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-policy-text\" data-toc-modified-id=\"Cleaning-policy-text-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning policy text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confirm-all-text-data-is-string-format\" data-toc-modified-id=\"Confirm-all-text-data-is-string-format-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Confirm all text data is string format</a></span></li><li><span><a href=\"#Setting-up-normalization-functions\" data-toc-modified-id=\"Setting-up-normalization-functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setting up normalization functions</a></span></li><li><span><a href=\"#Normalize-Whitespace\" data-toc-modified-id=\"Normalize-Whitespace-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Normalize Whitespace</a></span></li><li><span><a href=\"#Normalize-punctuation\" data-toc-modified-id=\"Normalize-punctuation-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Normalize punctuation</a></span></li><li><span><a href=\"#Remove-non-ASCII-characters\" data-toc-modified-id=\"Remove-non-ASCII-characters-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Remove non-ASCII characters</a></span></li><li><span><a href=\"#Make-all-policy-text-lowercase\" data-toc-modified-id=\"Make-all-policy-text-lowercase-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Make all policy text lowercase</a></span></li></ul></li><li><span><a href=\"#Same-pre-processing-steps-for-Crafted-Features\" data-toc-modified-id=\"Same-pre-processing-steps-for-Crafted-Features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Same pre-processing steps for Crafted Features</a></span></li><li><span><a href=\"#Append-crafted-features-to-dataframe\" data-toc-modified-id=\"Append-crafted-features-to-dataframe-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Append crafted features to dataframe</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-crafted-features\" data-toc-modified-id=\"Load-crafted-features-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load crafted features</a></span></li><li><span><a href=\"#Add-crafted-features-columns-to-df\" data-toc-modified-id=\"Add-crafted-features-columns-to-df-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Add crafted features columns to df</a></span></li><li><span><a href=\"#Saving-the-dataframe-to-be-used-for-modelling\" data-toc-modified-id=\"Saving-the-dataframe-to-be-used-for-modelling-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Saving the dataframe to be used for modelling</a></span></li></ul></li><li><span><a href=\"#Sentence-Filtering\" data-toc-modified-id=\"Sentence-Filtering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sentence Filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Alternative-filtering-process-for-when-either-+-support-or---support-is-below-100-after-SF'ing\" data-toc-modified-id=\"Alternative-filtering-process-for-when-either-+-support-or---support-is-below-100-after-SF'ing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Alternative filtering process for when either + support or - support is below 100 after SF'ing</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588ec3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddece0",
   "metadata": {},
   "source": [
    "# Cleaning policy text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3f4a4",
   "metadata": {},
   "source": [
    "## Confirm all text data is string format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f746402d",
   "metadata": {},
   "source": [
    "I realised that it is possible that a segment is not correctly stored in string format, so firstly I wish to confirm that every segment in my dataframe is stored in string format.\n",
    "\n",
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc7245da",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_segment_annots_df = pd.read_pickle('objects/segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a6cc49",
   "metadata": {},
   "source": [
    "Writing and verifying a function to confirm the datatype of every cell in a pandas dataframe column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b93e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def column_all_dtype(dataframe_column, dtype):\n",
    "    \"\"\"\n",
    "    Inputs: a specific dtype\n",
    "    Example inputs for dtype: str, \"<class 'numpy.int64'>\", \"<class 'numpy.bool_'>\", \"<class 'list'>\"\n",
    "    Outputs: True or False appropriately depending on whether any item in the dataframe column is not the dtype passed.\n",
    "    \"\"\"\n",
    "    \n",
    "    for _index in range(len(dataframe_column)):\n",
    "        if str(type(dataframe_column[_index])) != str(dtype):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# validation on columns of known type:\n",
    "print(column_all_dtype(clean_segment_annots_df['policy_segment_id'], \"<class 'numpy.int64'>\") ) # should return True\n",
    "print(column_all_dtype(clean_segment_annots_df['policy_segment_id'], str) ) # should return False\n",
    "print(column_all_dtype(clean_segment_annots_df['contains_synthetic'], \"<class 'numpy.bool_'>\") ) # should return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461083f",
   "metadata": {},
   "source": [
    "Confirming all my text data is in string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f2e7298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_all_dtype(clean_segment_annots_df['segment_text'], str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ab0fe",
   "metadata": {},
   "source": [
    "This can also be verified by the fact that every cell has the same type (the number of unique types is 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ad46a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_segment_annots_df['segment_text'].apply(type).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09defb0f",
   "metadata": {},
   "source": [
    "Perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa05b74",
   "metadata": {},
   "source": [
    "## Setting up normalization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7f0c6",
   "metadata": {},
   "source": [
    "For some of the below functions, to see that they worked, I want to check the total length of all strings in the DataFrame before and after running the function.  I can use a decorator to append these lines to the following functions where appropriate.\n",
    "\n",
    "I want to check the length of all the text before and after to see any difference made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b3dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total length\n",
    "def total_length(dataframe):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with a column named \"segment_text\" \n",
    "    Returns the sum of the string length of each cell in the column\n",
    "    \"\"\"\n",
    "    total_length_all_segments = dataframe[\"segment_text\"].str.len().sum()\n",
    "    return f\"Total length of all segments is {total_length_all_segments}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f47fd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5917918"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_segment_annots_df[\"segment_text\"].str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c11d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_length(func, dataframe):\n",
    "    \"\"\"\n",
    "    Prints the length of the dataframe before and after running the function.\n",
    "    Inputs: \n",
    "    - Any function that can take the dataframe as an input\n",
    "    - A pandas dataframe\n",
    "    Actions:\n",
    "    Passing the dataframe as an argument for each function: \n",
    "    Prints the output of the total_length function, \n",
    "    then calls the function passed as argument to this check_length function,\n",
    "    then prints the total_length function again\n",
    "    Outputs: none\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Before running function: {total_length(dataframe)}\")\n",
    "    \n",
    "    func(dataframe)\n",
    "    \n",
    "    print(f\"After running function: {total_length(dataframe)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582f563",
   "metadata": {},
   "source": [
    "## Normalize Whitespace\n",
    "\n",
    "Using `new_string = \" \".join(old_string.split())`.  The `.split()` function considers a range of forms of whitespace.\n",
    "\n",
    "I want to check the length of all the text before and after to see any difference made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c38cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(dataframe):\n",
    "    \"\"\"\n",
    "    Removed whitespace from all cells in the \"segment_text\" column.\n",
    "    For verification, call this function within the 'check_length' function.\n",
    "    Input: Dataframe with a column called \"segment_text\" that contains strings for the whitespace to be removed.\n",
    "    Returns: nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(lambda x: \" \".join(x.split()))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237aefe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917918\n",
      "After running function: Total length of all segments is 5917918\n"
     ]
    }
   ],
   "source": [
    "check_length(normalize_whitespace, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b13ac",
   "metadata": {},
   "source": [
    "Suspiciously nothing changed, so I will verify my function before concluding that the privacy policy segments have no whitespace in them.\n",
    "\n",
    "Verifying the function by testing it on some whitespace.\n",
    "- create  a dataframe where I have intentionally added whitespace \n",
    "- calling the function on this dataframe\n",
    "- confirming the whitespace is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65180163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917320\n",
      "After running function: Total length of all segments is 5917279\n"
     ]
    }
   ],
   "source": [
    "text_with_space = 'PRIVACY                                          This.'\n",
    "verify_whitespace_df = clean_segment_annots_df.copy()\n",
    "verify_whitespace_df.loc[0, 'segment_text'] = text_with_space\n",
    "check_length(normalize_whitespace,verify_whitespace_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76088696",
   "metadata": {},
   "source": [
    "I have verified that my function works and so can conclude that the privacy policy segments have no whitespace in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66297e7a",
   "metadata": {},
   "source": [
    "## Normalize punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cd5e24",
   "metadata": {},
   "source": [
    "I took the below function from this towardsdatascience article [here](https://towardsdatascience.com/text-normalization-7ecc8e084e31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69da5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simplify_punctuation(text):\n",
    "    \"\"\"\n",
    "    This function simplifies doubled or more complex punctuation. The exception is '...'.\n",
    "    \"\"\"\n",
    "    \n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n",
    "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7ef8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_punctuation(dataframe):\n",
    "    \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(_simplify_punctuation)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d209643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917918\n",
      "After running function: Total length of all segments is 5917879\n"
     ]
    }
   ],
   "source": [
    "check_length(remove_duplicate_punctuation, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614dd1da",
   "metadata": {},
   "source": [
    "Only a small number of characters were removed as expected.\n",
    "\n",
    "Further punctuation normalization such as converting other characters to their english standardized versions (e.g. the opening speachmark “ to \", or elipses … to ...) would be ideal, but the ommission of this should not affect the sentence filtering much, and won't have any effect on the tf-idf matrix, because it ignores punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a20351",
   "metadata": {},
   "source": [
    "## Remove non-ASCII characters\n",
    "\n",
    "This can be done by checking whether each character has a unicode index below 128, as ASCII characters are coded above 128.  Checking the unicode 'code point' is done with `ord(char)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4a7aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "a_b^0\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(string):\n",
    "    \"\"\"\n",
    "    I found this function on this website: https://bobbyhadz.com/blog/python-remove-non-ascii-characters-from-string\n",
    "    \"\"\"\n",
    "    return ''.join(char for char in string if ord(char) < 128)\n",
    "\n",
    "# demonstrate function:\n",
    "print(remove_non_ascii('a€bñcá')) # >> 'abc'\n",
    "print(remove_non_ascii('a_b^0')) # >> a_b^0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f16ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonASCII_chars(dataframe):\n",
    "        \n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].map(remove_non_ascii)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbd8b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before running function: Total length of all segments is 5917879\n",
      "After running function: Total length of all segments is 5901658\n"
     ]
    }
   ],
   "source": [
    "check_length(remove_nonASCII_chars, clean_segment_annots_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dbcec",
   "metadata": {},
   "source": [
    "Thousands of characters were removed, representing nearly .3% of all characters.  I hope that this makes at least a small difference for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c635d39",
   "metadata": {},
   "source": [
    "## Make all policy text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f53e223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ifiufiwunfiijnf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      segment_text\n",
       "0  ifiufiwunfiijnf"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_lowercase(dataframe):\n",
    "\n",
    "    dataframe[\"segment_text\"] = dataframe[\"segment_text\"].str.lower()\n",
    "    \n",
    "    return None\n",
    "\n",
    "# verify\n",
    "sample_df = pd.DataFrame([\"ifiUFIWUNFIijnf\"], columns=[\"segment_text\"])\n",
    "convert_to_lowercase(sample_df)\n",
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ae5487b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    privacy policy this privacy policy (hereafter ...\n",
       "1    1. about our products 1.1 our products offer a...\n",
       "2    2. the information we collect the information ...\n",
       "Name: segment_text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_lowercase(clean_segment_annots_df)\n",
    "clean_segment_annots_df['segment_text'].head(3) # verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5d06c",
   "metadata": {},
   "source": [
    "It can be seen that the text has been changed to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1490337",
   "metadata": {},
   "source": [
    "**Save the above cleaned dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22ab7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_segment_annots_df.to_pickle('objects/clean_segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f807aeda",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4297a",
   "metadata": {},
   "source": [
    "# Same pre-processing steps for Crafted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068c7cd",
   "metadata": {},
   "source": [
    "I will need to populate the segment dataframe with crafted features, but if the segments and the crafted features are in different formats, it will be more difficult to do so.  So I will check and standardize the format of the crafted features too.\n",
    "\n",
    "I save the crafted features and the annotations that they refer to in the variable `annotation features`.\n",
    "\n",
    "Load the list of all the crafted features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8500382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_features = pd.read_pickle('objects/annotation_features.pkl')\n",
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "len(list_all_crafted_features) # verify – should be 579 crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffbf32",
   "metadata": {},
   "source": [
    "These crafted features are not quite in the same format as the main dataframe so I cannot apply my functions to them. I won't check for doubled punctuation. To capture non-ASCII characters and uppercase letters I only need to check which non-lowercase letters there are.\n",
    "\n",
    "Checking for any characters that are not lowercase english alphabet characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79ee3520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', ',', '-', '\\xa0', '/', 'S', 'N', 'U', 'T', 'P', 'A', '(', ')', 'I', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "def non_asciis():\n",
    "    list_of_chars = []\n",
    "    for ft in list_all_crafted_features:\n",
    "        for char in ft:\n",
    "            if char.islower() == False: #aka if it's not lowercase\n",
    "                if char not in list_of_chars:\n",
    "                    list_of_chars.append(char)\n",
    "    return list_of_chars\n",
    "print(non_asciis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959fefc",
   "metadata": {},
   "source": [
    "By inspecting this list I can see that the only characters that I don't expect are the uppercase letters and \"\\xa0\", which represents a type of whitespace.  There are no non-ASCII characters so I don't need to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb541a76",
   "metadata": {},
   "source": [
    "I also noticed while manually browsing the features that Bluetooth was not listed because it had been incorrectly entered as 'bluethooth', so I will correct that now too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bb04d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "\"bluethooth\" in list_all_crafted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754490ba",
   "metadata": {},
   "source": [
    "Correcting typo and normalizing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6587fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _row in range(len(annotation_features)):\n",
    "    crafted_feature_list = annotation_features.at[_row, 'features']\n",
    "\n",
    "    # correct \"Bluetooth\"\n",
    "    new_crafted_feature_list = [\"bluetooth\" if feature==\"bluethooth\" else feature for feature in crafted_feature_list]\n",
    "    \n",
    "    # change to lowercase\n",
    "    new_crafted_feature_list = [feature.lower() for feature in new_crafted_feature_list]\n",
    "    \n",
    "    # normalize whitespace\n",
    "    new_crafted_feature_list = [\" \".join(feature.split()) for feature in new_crafted_feature_list]\n",
    "\n",
    "    \n",
    "    annotation_features.at[_row, 'features'] = new_crafted_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f24c3",
   "metadata": {},
   "source": [
    "Checking again that the bluetooth typo was corrected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9afbe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all_crafted_features = [feature for row in annotation_features['features'] for feature in row]\n",
    "\"bluethooth\" in list_all_crafted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ac399",
   "metadata": {},
   "source": [
    "Checking again for non-lowercase characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae166c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n"
     ]
    }
   ],
   "source": [
    "print(len(list_all_crafted_features)) # verify – should be 579 crafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04a4d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', ',', '-', '/', '(', ')', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(non_asciis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154dda9",
   "metadata": {},
   "source": [
    "All problematic characters are now removed so this list of features can be used for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12c2b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "annotation_features.to_pickle('objects/clean_annotation_features.pkl')\n",
    "confirm_save_0 = pd.read_pickle('objects/clean_annotation_features.pkl')\n",
    "print(annotation_features.shape == confirm_save_0.shape)\n",
    "print(confirm_save_0.equals(annotation_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6284f80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20e7c1",
   "metadata": {},
   "source": [
    "# Append crafted features to dataframe\n",
    "\n",
    "## Load crafted features\n",
    "\n",
    "The next steps are to:\n",
    "- 1. Append each feature as a column to the dataframe\n",
    "- 2. Populate the columns\n",
    "\n",
    "Then I can move to modelling.\n",
    "\n",
    "I already have a function to help with 1 called `add_empty_annotation_columns`.  I just need to put the new features into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b24ec8",
   "metadata": {},
   "source": [
    "First though, I want to check whether any of the features are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a54cd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_annotation_features = pd.read_pickle('objects/clean_annotation_features.pkl')\n",
    "clean_segment_annots_df = pd.read_pickle('objects/clean_segment_annots_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4541c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_crafted_features = [feature for row in clean_annotation_features['features'] for feature in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b0ae5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = []\n",
    "duplicate_features = []\n",
    "for feature in list_all_crafted_features:\n",
    "    if feature in all_features:\n",
    "        duplicate_features.append(feature)\n",
    "    all_features.append(feature)\n",
    "len(duplicate_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4ede5",
   "metadata": {},
   "source": [
    "Oddly, a lot of the features are exactly the same. I will remove these duplicates after adding them to the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3b184",
   "metadata": {},
   "source": [
    "## Add crafted features columns to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8680a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n",
      "(15543, 41)\n",
      "The shape of the returned dataframe is (15543, 620)\n"
     ]
    }
   ],
   "source": [
    "print(len(list_all_crafted_features))\n",
    "print(clean_segment_annots_df.shape)\n",
    "crafted_features_df = priv_pol_funcs.add_empty_annotation_columns(clean_segment_annots_df, list_all_crafted_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d26787",
   "metadata": {},
   "source": [
    "Verify that the features have been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8edacbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <th>contact info</th>\n",
       "      <th>contact details</th>\n",
       "      <th>contact data</th>\n",
       "      <th>e.g., your name</th>\n",
       "      <th>contact you</th>\n",
       "      <th>your contact</th>\n",
       "      <th>identify, contact</th>\n",
       "      <th>identifying information</th>\n",
       "      <th>your name, address, and e-mail address</th>\n",
       "      <th>...</th>\n",
       "      <th>never be acquired</th>\n",
       "      <th>never be viewed</th>\n",
       "      <th>never be located</th>\n",
       "      <th>never be asked</th>\n",
       "      <th>never be utilized</th>\n",
       "      <th>never be requested</th>\n",
       "      <th>never be transmitted</th>\n",
       "      <th>never be communicated</th>\n",
       "      <th>nor do we collect</th>\n",
       "      <th>does not tell us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 580 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOT_PERFORMED  contact info  contact details  contact data  \\\n",
       "0              0             0                0             0   \n",
       "1              0             0                0             0   \n",
       "\n",
       "   e.g., your name  contact you  your contact  identify, contact  \\\n",
       "0                0            0             0                  0   \n",
       "1                0            0             0                  0   \n",
       "\n",
       "   identifying information  your name, address, and e-mail address  ...  \\\n",
       "0                        0                                       0  ...   \n",
       "1                        0                                       0  ...   \n",
       "\n",
       "   never be acquired  never be viewed  never be located  never be asked  \\\n",
       "0                  0                0                 0               0   \n",
       "1                  0                0                 0               0   \n",
       "\n",
       "   never be utilized  never be requested  never be transmitted  \\\n",
       "0                  0                   0                     0   \n",
       "1                  0                   0                     0   \n",
       "\n",
       "   never be communicated  nor do we collect  does not tell us  \n",
       "0                      0                  0                 0  \n",
       "1                      0                  0                 0  \n",
       "\n",
       "[2 rows x 580 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df.iloc[:,40:].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ff022",
   "metadata": {},
   "source": [
    "We can see that the crafted features are all columns from column 41 to the end.  Now let's remove the duplicate feature columns before populating them all. I expect 103 columns to be removed to bring the `crafted_features_df` down to 517 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d48c302e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15543, 517)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df = crafted_features_df.loc[:,~crafted_features_df.columns.duplicated()] # remove columns with duplicate names\n",
    "crafted_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00ab88",
   "metadata": {},
   "source": [
    "Perfect, the right number of columns have been removed.\n",
    "\n",
    "Now to populate the crafted features columns, I will:\n",
    "\n",
    "- Take the column name for each crafted feature\n",
    "- take the segment text for each row\n",
    "- if column name in segment text: put 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df312cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 203 ms, total: 28.8 s\n",
      "Wall time: 28.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_rows = range(len(crafted_features_df)) # index of rows to loop through\n",
    "\n",
    "for column_number in range(41, 517): # Looping through each column with a feature\n",
    "\n",
    "    column_name = crafted_features_df.columns[column_number] # for that column feature\n",
    "\n",
    "    for row in all_rows: # and for every row\n",
    "        if column_name in crafted_features_df.at[row, \"segment_text\"]: # if the segment has that feature\n",
    "            crafted_features_df.at[row, column_name] = 1 # make the value for that feature on that row equal 1\n",
    "    \n",
    "    print(f\"Processing {column_number}/517\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38fdf3f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 features have not been populated\n"
     ]
    }
   ],
   "source": [
    "# looking at some of the results to verify\n",
    "summations = crafted_features_df.iloc[:,41:].sum()\n",
    "print(f\"{(summations==0).sum()} features have not been populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91616fda",
   "metadata": {},
   "source": [
    "This seems like a lot of empty columns, so I manually looked through the results, as well as checking the source text, and found that most of the crafted feature columns that haven't been populated are generally:\n",
    "- unusual ways of typing a phrase (example: 'post code' instead of postcode)\n",
    "- specific phrases for uncommon data practices (example: 'exact device location')\n",
    "- negative phrases (example: never be requested)\n",
    "\n",
    "These features would have been included by the researchers to capture phrases that don't feature in their dataset but could feature when applying their model beyond their training data.\n",
    "\n",
    "Overall this seems roughly correct so I will use it for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16120c",
   "metadata": {},
   "source": [
    "<font size= \"3\"> **Some final tidying:** <font/>\n",
    "\n",
    "Confirming that every number across the target and feature columns equal 0 or 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "994e5f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(crafted_features_df.iloc[:,7:] == 0 # every cell equals 0\n",
    " | (crafted_features_df.iloc[:,7:] == 1) # or every cell equals 1\n",
    ").nunique().nunique() # All columns and rows in the resulting dataframe of booleans only show one result (True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490071cf",
   "metadata": {},
   "source": [
    "Alternate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a698f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crafted_features_df.iloc[:,7:].isin([0, 1]).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37145695",
   "metadata": {},
   "source": [
    "Changing the dtype of those same columns to int8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c1bad46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dtypes are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int64      482\n",
       "float64     28\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the dtypes are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int8    510\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"The dtypes are:\")\n",
    "display(crafted_features_df.iloc[:,7:].dtypes.value_counts())\n",
    "\n",
    "subset_df = crafted_features_df.iloc[:,7:].copy()\n",
    "subset_df = subset_df.astype('int8')\n",
    "crafted_features_df.iloc[:,7:] = subset_df\n",
    "\n",
    "print(f\"Now the dtypes are:\")\n",
    "display(crafted_features_df.iloc[:,7:].dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e532f1",
   "metadata": {},
   "source": [
    "## Saving the dataframe to be used for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434c4a83",
   "metadata": {},
   "source": [
    "As before, to make it faster to load this dataframe in this notebook and others, I will save this dataframe as a pickle file.  This allows the below code to be ran without waiting for the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e9c64479",
   "metadata": {},
   "outputs": [],
   "source": [
    "crafted_features_df.to_pickle('objects/crafted_features_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f18817",
   "metadata": {},
   "source": [
    "Verifying that the file was correctly saved and can be imported properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "39a25967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "confirm_save_5 = pd.read_pickle('objects/crafted_features_df.pkl')\n",
    "print(crafted_features_df.shape == confirm_save_5.shape)\n",
    "print(confirm_save_5.equals(crafted_features_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd4c2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I now have a dataframe with cleaned text, crafted features, and all targets.  In the next notebook, they will be passed into a modelling pipeline to follow the steps of Story et al.  The modelling pipeline also involves another pre-processing step, sentence filtering, which I will lay out now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ceefe1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de68fe",
   "metadata": {},
   "source": [
    "# Sentence Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fc931",
   "metadata": {},
   "source": [
    "As described by Story et al., Sentence Filtering involves filtering the data to only train a classifier on segments that contain a relevant feature for the target. For example a short segment about location data wouldn't be used to train an email classifier because it wouldn't contain keywords such as \"Email\" or \"Contact\".  Although this means training on less data, it could improve performance by reducing class imbalance and by emphasising the key words as indicators.\n",
    "\n",
    "To implement this, for each classifier, I need to filter the data to only contain segments that feature at least one of the crafted features for that classifier.\n",
    "\n",
    "I will do this as part of my classifier pipeline.. (description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c85a33",
   "metadata": {},
   "source": [
    "First I will load all the data I need (more steps?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e35189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "# Create dataframe to work with – segment text, crafted features and targets (annotations)\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Dropping unnecessary metadata columns\n",
    "for dataframe in [df_for_pipelining_train]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fc7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a table to access the appropriate crafted features corresponding to each target\n",
    "clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55acb3",
   "metadata": {},
   "source": [
    "Load a list of the targets of interest. Story et al. found that some of their annotations were not relevant so only trained classifiers for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2456951",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af1d26",
   "metadata": {},
   "source": [
    "Issue with SF'ing causing too few + or - support with some classifiers. Looking at the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6870dd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Whole dataset support</th>\n",
       "      <th>Positive support</th>\n",
       "      <th>Negative support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>128</td>\n",
       "      <td>115</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>662</td>\n",
       "      <td>496</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>346</td>\n",
       "      <td>269</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>596</td>\n",
       "      <td>558</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>332</td>\n",
       "      <td>251</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>537</td>\n",
       "      <td>462</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>142</td>\n",
       "      <td>106</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>126</td>\n",
       "      <td>77</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>146</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>109</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>1833</td>\n",
       "      <td>1793</td>\n",
       "      <td>5504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>530</td>\n",
       "      <td>505</td>\n",
       "      <td>3658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>1681</td>\n",
       "      <td>1612</td>\n",
       "      <td>5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>551</td>\n",
       "      <td>525</td>\n",
       "      <td>2888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Whole dataset support Positive support  \\\n",
       "Contact                                             128              115   \n",
       "Contact_E_Mail_Address                              662              496   \n",
       "Contact_Phone_Number                                346              269   \n",
       "Identifier_Cookie_or_similar_Tech                   596              558   \n",
       "Identifier_Device_ID                                332              251   \n",
       "Identifier_IMEI                                      50               43   \n",
       "Identifier_MAC                                       85               78   \n",
       "Identifier_Mobile_Carrier                            60               39   \n",
       "Location                                            537              462   \n",
       "Location_Cell_Tower                                  89               62   \n",
       "Location_GPS                                        142              106   \n",
       "Location_WiFi                                       126               77   \n",
       "SSO                                                 146               20   \n",
       "Facebook_SSO                                        109               12   \n",
       "1st_party                                          1833             1793   \n",
       "3rd_party                                           530              505   \n",
       "PERFORMED                                          1681             1612   \n",
       "NOT_PERFORMED                                       551              525   \n",
       "\n",
       "                                  Negative support  \n",
       "Contact                                        251  \n",
       "Contact_E_Mail_Address                          61  \n",
       "Contact_Phone_Number                           218  \n",
       "Identifier_Cookie_or_similar_Tech              121  \n",
       "Identifier_Device_ID                            20  \n",
       "Identifier_IMEI                                  0  \n",
       "Identifier_MAC                                  62  \n",
       "Identifier_Mobile_Carrier                       30  \n",
       "Location                                       216  \n",
       "Location_Cell_Tower                             12  \n",
       "Location_GPS                                    27  \n",
       "Location_WiFi                                   24  \n",
       "SSO                                              3  \n",
       "Facebook_SSO                                    11  \n",
       "1st_party                                     5504  \n",
       "3rd_party                                     3658  \n",
       "PERFORMED                                     5130  \n",
       "NOT_PERFORMED                                 2888  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_filtering(X, y, df_filter = sf_filter):\n",
    "        # filter y\n",
    "    y2 = y.loc[df_filter].copy()\n",
    "    y2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        # filter X\n",
    "    X2 = X.loc[df_filter].copy()\n",
    "    X2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return X2, y2\n",
    "\n",
    "support_after_sf_table = pd.DataFrame({\"Whole dataset support\":[\"tbd\"], \"Positive support\":[\"tbd\"], \"Negative support\":[\"tbd\"]}, index=[\"Contact\"])\n",
    "\n",
    "for classifier in list_of_18_classifiers:\n",
    "    \n",
    "    # Separate into x and y\n",
    "    y = df_for_pipelining_train[classifier]\n",
    "    X = pd.concat([df_for_pipelining_train['segment_text'],df_for_pipelining_train.loc[:,'contact info':]], axis=1).copy()\n",
    "\n",
    "    # Before sentence filtering, recording the frequency across the whole train set\n",
    "    support_after_sf_table.loc[classifier, \"Whole dataset support\"] = y.value_counts()[1]\n",
    "    \n",
    "    # Get correct classifier_features to use for sentence filtering: \n",
    "    # Filtering the table to get the list object from the same row that lists the classifier\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ]     \\\n",
    "                            .reset_index().at[0,'features']\n",
    "    \n",
    "     # Using the classifier_features, create a boolean series to use for sentence filtering:\n",
    "    sf_filter = ((X[classifier_features] > 0)\\\n",
    "                 .sum(axis=1) > 0 )\n",
    "    \n",
    "    # Applying sentence filtering\n",
    "    X, y = sentence_filtering(X, y, sf_filter)\n",
    "    \n",
    "    # Populating the table with the frequency of data for each classifier after sentence filtering\n",
    "    support_after_sf_table.loc[classifier, \"Positive support\"] = y.value_counts().get(1, 0)\n",
    "    support_after_sf_table.loc[classifier, \"Negative support\"] = y.value_counts().get(0, 0)\n",
    "\n",
    "support_after_sf_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011c9b5",
   "metadata": {},
   "source": [
    "It can be seen that with Sentence Filtering, some of the targets have too little positive or negative support. It's likely that those models wouldn't be selected as the best ones by the grid search.\n",
    "\n",
    "I also infer that Story et al. didn't use the same sentence filtering process as I have done, since their table 1 on page 4 [(link to paper)](https://usableprivacy.org/static/files/story_pal_2019.pdf) shows sentence filtering was helpful for training a model to find \"Identifier_IMEI\", but with my process I won't be able to train a model at all (no negative support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1751b",
   "metadata": {},
   "source": [
    "## Alternative filtering process for when either + support or - support is below 100 after SF'ing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba56d8",
   "metadata": {},
   "source": [
    "Using an arbirary cut off of 75. In further work I would like to study this more to find a more rigorously defined cut off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a22c4e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_filtering(X, y, df_filter=sf_filter):\n",
    "    \"\"\"\n",
    "    Filter the X and y data using Sentence Filtering, \n",
    "    or if this leaves too few data, filter using balanced downsize filtering.\n",
    "    \n",
    "    Inputs: \n",
    "        X: X data\n",
    "        y: y data\n",
    "        df_filter:  a filter (boolean series) to use to filter the data. \n",
    "                    Intended to be sf_filter (sentence filtering)\n",
    "                    or balanced_downzise_filter (all the positive cases plus an equally sized random sample of negative cases)\n",
    "    Outputs:\n",
    "        X2: filtered X data\n",
    "        y2: filtered y data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "        # filter y\n",
    "    y2 = y.loc[df_filter].copy()\n",
    "    y2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        # filter X\n",
    "    X2 = X.loc[df_filter].copy()\n",
    "    X2.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "        # check whether this sentence filtering leaves enough data (arbitrary > 75)\n",
    "        # if not, use balanced downsizing instead:\n",
    "    if df_filter.equals(sf_filter) & (\n",
    "        ( y2.value_counts().get(1, 0) < 75 ) or ( y2.value_counts().get(0, 0) < 75 )\n",
    "    ):\n",
    "        X2, y2 = sentence_filtering(X, y, df_filter=balanced_downzise_filter)\n",
    "    \n",
    "    return X2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b822df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Whole dataset support</th>\n",
       "      <th>Positive support</th>\n",
       "      <th>Negative support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>128</td>\n",
       "      <td>115</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>662</td>\n",
       "      <td>662</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>346</td>\n",
       "      <td>269</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>596</td>\n",
       "      <td>558</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>537</td>\n",
       "      <td>462</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>1833</td>\n",
       "      <td>1793</td>\n",
       "      <td>5504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>530</td>\n",
       "      <td>505</td>\n",
       "      <td>3658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>1681</td>\n",
       "      <td>1612</td>\n",
       "      <td>5130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>551</td>\n",
       "      <td>525</td>\n",
       "      <td>2888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Whole dataset support Positive support  \\\n",
       "Contact                                             128              115   \n",
       "Contact_E_Mail_Address                              662              662   \n",
       "Contact_Phone_Number                                346              269   \n",
       "Identifier_Cookie_or_similar_Tech                   596              558   \n",
       "Identifier_Device_ID                                332              332   \n",
       "Identifier_IMEI                                      50               50   \n",
       "Identifier_MAC                                       85               85   \n",
       "Identifier_Mobile_Carrier                            60               60   \n",
       "Location                                            537              462   \n",
       "Location_Cell_Tower                                  89               89   \n",
       "Location_GPS                                        142              142   \n",
       "Location_WiFi                                       126              126   \n",
       "SSO                                                 146              146   \n",
       "Facebook_SSO                                        109              109   \n",
       "1st_party                                          1833             1793   \n",
       "3rd_party                                           530              505   \n",
       "PERFORMED                                          1681             1612   \n",
       "NOT_PERFORMED                                       551              525   \n",
       "\n",
       "                                  Negative support  \n",
       "Contact                                        251  \n",
       "Contact_E_Mail_Address                         662  \n",
       "Contact_Phone_Number                           218  \n",
       "Identifier_Cookie_or_similar_Tech              121  \n",
       "Identifier_Device_ID                           332  \n",
       "Identifier_IMEI                                 50  \n",
       "Identifier_MAC                                  85  \n",
       "Identifier_Mobile_Carrier                       60  \n",
       "Location                                       216  \n",
       "Location_Cell_Tower                             89  \n",
       "Location_GPS                                   142  \n",
       "Location_WiFi                                  126  \n",
       "SSO                                            146  \n",
       "Facebook_SSO                                   109  \n",
       "1st_party                                     5504  \n",
       "3rd_party                                     3658  \n",
       "PERFORMED                                     5130  \n",
       "NOT_PERFORMED                                 2888  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_after_sf_table = pd.DataFrame({\"Whole dataset support\":[\"tbd\"], \"Positive support\":[\"tbd\"], \"Negative support\":[\"tbd\"]}, index=[\"Contact\"])\n",
    "\n",
    "for classifier in list_of_18_classifiers:\n",
    "    \n",
    "    #S eparate into x and y\n",
    "    y = df_for_pipelining_train[classifier]\n",
    "    X = pd.concat([df_for_pipelining_train['segment_text'],df_for_pipelining_train.loc[:,'contact info':]], axis=1).copy()\n",
    "\n",
    "    # Filtering the table to get the list object from the same row that lists the classifier\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ]     \\\n",
    "                            .reset_index().at[0,'features']\n",
    "    \n",
    "     # True/false boolean series for sentence filtering:\n",
    "    sf_filter = ((X[classifier_features] > 0)\\\n",
    "                 .sum(axis=1) > 0 )\n",
    "    \n",
    "    # True/false boolean series for balanced downsizing filter:\n",
    "    positive_rows = (y == 1)\n",
    "    negative_rows = (y == 0)\n",
    "    balanced_downzise_filter = (\n",
    "        positive_rows |\n",
    "        negative_rows.where(negative_rows == True).dropna().sample(n=positive_rows.sum(), replace=False)\n",
    "    )\n",
    "    \n",
    "    # Total positive support in whole dataset (without sentence filtering)\n",
    "    support_after_sf_table.loc[classifier, \"Whole dataset support\"] = y.value_counts()[1]\n",
    "\n",
    "    X, y = sentence_filtering(X, y, sf_filter)\n",
    "    \n",
    "    support_after_sf_table.loc[classifier, \"Positive support\"] = y.value_counts().get(1, 0)\n",
    "    support_after_sf_table.loc[classifier, \"Negative support\"] = y.value_counts().get(0, 0)\n",
    "    \n",
    "support_after_sf_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddb113a",
   "metadata": {},
   "source": [
    "These figures look more reasonable. I will take this amendment to the Sentence Filtering process forwards to my pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509eb8f",
   "metadata": {},
   "source": [
    "</br> \n",
    "\n",
    "---\n",
    "\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fd08a",
   "metadata": {},
   "source": [
    "All the preprocessing is now complete and I can run a grid search. I have done:\n",
    "\n",
    "1, 2, 3, 4 things\n",
    "\n",
    "So now I take forward (these things?). I will use them to create a pipeline that I will pass each classifier into. The pipeline will do a grid search to find a model and save the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
