{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb32d037",
   "metadata": {},
   "source": [
    "<h1> Modelling <span class=\"tocSkip\"></span></h1>\n",
    "\n",
    "Having ran a baseline model, and having conducted further preprocessing, I would now like to see whether I can replicate similar steps to Story et al. and see how different preprocessing steps affect the performance of models.\n",
    "\n",
    "I will begin by replicating their steps, but just training a single classifier.  Then I will evaluate the results. This will be section 1 of this notebook.\n",
    "\n",
    "Then, in section 2, having demonstrated the steps, I will build functions for training models to use to train multiple classifiers.\n",
    "\n",
    "This will give results for all classifiers that I can then discuss.\n",
    "\n",
    "Finally I can run my most promising models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475bb91",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Crafted-Features-and-Sentence-Filtering\" data-toc-modified-id=\"Crafted-Features-and-Sentence-Filtering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Crafted Features and Sentence Filtering</a></span></li><li><span><a href=\"#Section-1\" data-toc-modified-id=\"Section-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Section 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-and-splitting-into-train/validate/test\" data-toc-modified-id=\"Loading-data-and-splitting-into-train/validate/test-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Loading data and splitting into train/validate/test</a></span></li></ul></li><li><span><a href=\"#Step-1:-select-classifier:-&quot;1st_party&quot;\" data-toc-modified-id=\"Step-1:-select-classifier:-&quot;1st_party&quot;-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Step 1: select classifier: \"1st_party\"</a></span></li><li><span><a href=\"#Step-2:-Separate-into-X-and-y\" data-toc-modified-id=\"Step-2:-Separate-into-X-and-y-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Step 2: Separate into X and y</a></span></li><li><span><a href=\"#Step-3:-Set-up-filter-for-sentence-filtering-to-use-in-Pipeline\" data-toc-modified-id=\"Step-3:-Set-up-filter-for-sentence-filtering-to-use-in-Pipeline-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Step 3: Set up filter for sentence filtering to use in Pipeline</a></span></li><li><span><a href=\"#Step-4:-Set-up-for-tf-idf-matrix-to-use-in-Pipeline\" data-toc-modified-id=\"Step-4:-Set-up-for-tf-idf-matrix-to-use-in-Pipeline-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Step 4: Set up for tf-idf matrix to use in Pipeline</a></span></li><li><span><a href=\"#Step-5:-5-fold-CV-Grid-Search\" data-toc-modified-id=\"Step-5:-5-fold-CV-Grid-Search-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Step 5: 5-fold CV Grid Search</a></span></li><li><span><a href=\"#Evaluation-on-Train-dataset\" data-toc-modified-id=\"Evaluation-on-Train-dataset-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation on Train dataset</a></span></li><li><span><a href=\"#Evaluation-on-Validation-dataset\" data-toc-modified-id=\"Evaluation-on-Validation-dataset-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Evaluation on Validation dataset</a></span></li><li><span><a href=\"#Section-2:-modelling-pipeline\" data-toc-modified-id=\"Section-2:-modelling-pipeline-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Section 2: modelling pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements-for-modelling-pipeline\" data-toc-modified-id=\"Requirements-for-modelling-pipeline-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Requirements for modelling pipeline</a></span></li></ul></li><li><span><a href=\"#Old-from-here:\" data-toc-modified-id=\"Old-from-here:-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Old from here:</a></span></li><li><span><a href=\"#Run-all-classifiers-through-the-pipeline\" data-toc-modified-id=\"Run-all-classifiers-through-the-pipeline-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Run all classifiers through the pipeline</a></span></li><li><span><a href=\"#Creating-model-evaluation-table\" data-toc-modified-id=\"Creating-model-evaluation-table-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Creating model evaluation table</a></span></li><li><span><a href=\"#Discussion-of-low-F1-scores\" data-toc-modified-id=\"Discussion-of-low-F1-scores-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Discussion of low F1 scores</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-wide-range-of-scores\" data-toc-modified-id=\"A-wide-range-of-scores-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;</span>A wide range of scores</a></span></li><li><span><a href=\"#Neg-F1-tends-to-be-higher-than-Pos-F1\" data-toc-modified-id=\"Neg-F1-tends-to-be-higher-than-Pos-F1-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;</span>Neg F1 tends to be higher than Pos F1</a></span></li><li><span><a href=\"#Compare-with-results-from-the-paper\" data-toc-modified-id=\"Compare-with-results-from-the-paper-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;</span>Compare with results from the paper</a></span></li><li><span><a href=\"#Matching-scores-for-SSO-and-FacebookSSO\" data-toc-modified-id=\"Matching-scores-for-SSO-and-FacebookSSO-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;</span>Matching scores for SSO and FacebookSSO</a></span></li></ul></li><li><span><a href=\"#Final-model-performance-on-test-set\" data-toc-modified-id=\"Final-model-performance-on-test-set-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Final model performance on test set</a></span></li><li><span><a href=\"#Further-work\" data-toc-modified-id=\"Further-work-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Further work</a></span></li><li><span><a href=\"#OLD:-TF-IDF-matrix-(old-content)\" data-toc-modified-id=\"OLD:-TF-IDF-matrix-(old-content)-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>OLD: TF-IDF matrix (old content)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-y-(old-content)\" data-toc-modified-id=\"Create-y-(old-content)-17.1\"><span class=\"toc-item-num\">17.1&nbsp;&nbsp;</span>Create y (old content)</a></span></li></ul></li><li><span><a href=\"#Step-2:-apply-Sentence-Filtering-(old-content)\" data-toc-modified-id=\"Step-2:-apply-Sentence-Filtering-(old-content)-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Step 2: apply Sentence Filtering (old content)</a></span></li><li><span><a href=\"#Evaluation-on-Validation-dataset-(Old-content)\" data-toc-modified-id=\"Evaluation-on-Validation-dataset-(Old-content)-19\"><span class=\"toc-item-num\">19&nbsp;&nbsp;</span>Evaluation on Validation dataset (Old content)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719e140",
   "metadata": {},
   "source": [
    "The steps in this notebook are:\n",
    "\n",
    "<font size= \"4\"> **Section 1** </font>\n",
    "\n",
    "- Load data and split into Train/Validate/Test\n",
    "\n",
    "    - For demonstrative purposes I will begin by running through the modelling steps for just a single classifier: 1st Party\n",
    "    \n",
    "\n",
    "- Separating into X and y\n",
    "  - X, the input data to the model, is a union of the crafted features columns and a tf-idf matrix\n",
    "\n",
    "\n",
    "- 5-fold cross-validation grid search\n",
    "  - Over Sentence Filtering, whether to include bigrams in the tf-idf matrix and the hyperparameters for SVM model and  as performed by Story et al.\n",
    "  - Additionally I opt to search over a default logistic regression classifier for comparison\n",
    "\n",
    "\n",
    "- Evaluation on train dataset\n",
    "\n",
    "\n",
    "- Evaluation on validation set and review.\n",
    "\n",
    "<font size= \"4\"> **Section 2** </font>\n",
    "\n",
    "Having demonstrated the steps for model construction for one classifier, I move to create a pipeline to train multiple classifiers.\n",
    "\n",
    "The full pipeline consists of a series of functions that allows me to train a classifier for each of the 18 targets of interest. The steps are:\n",
    "\n",
    "- Listing the requirements for the pipeline\n",
    "\n",
    "- Functions for the pipeline\n",
    "  - These correspond to the steps executed above for ‘1st party’\n",
    "\n",
    "- Display of all model scores\n",
    "\n",
    "- Discussion of some zero scores\n",
    "\n",
    "- Further discussion\n",
    "\n",
    "- Final model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a7078",
   "metadata": {},
   "source": [
    "# Crafted Features and Sentence Filtering\n",
    "\n",
    "I have discussed this in previous notebooks but will include it here for completion.\n",
    "\n",
    "To try to improve classifier performance, Story et al. apply a preprocessing technique that they call Sentence Filtering. This involves filtering the data to only train a classifier on segments that contain a relevant feature for the target.  For example a short segment about location data wouldn't be used to train an email classifier because it wouldn't contain keywords such as \"Email\" or \"Contact\".\n",
    "\n",
    "They do not explicitly specify that these features are equivalent to the crafted features but I am inferring that they are.\n",
    "\n",
    "The effect of applying sentence filtering is that instead of a model being trained on all the data and there being class imbalance in favour of the negative class, a much greater proportion of the training data features the target being classified. The effect is similar to downsampling.\n",
    "\n",
    "This will be a moderate approximation for a replication of most of their work. The main missing element will be better text pre-processing to get better results from the crafted features and sentence filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a733d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# modelling\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# modelling pipeline\n",
    "from tempfile import mkdtemp\n",
    "# from sklearn.pipeline import Pipeline # not required since using imbalanced learn\n",
    "from imblearn.pipeline import Pipeline \n",
    "    # Using the pipeline from the imbalanced learn library since it allows for sampling\n",
    "from imblearn import FunctionSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# modelling evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eae328",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "## Loading data and splitting into train/validate/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5d3a",
   "metadata": {},
   "source": [
    "**Creating Train, Validate and Test dataframes to use.** <br/> Story et al. already randomly split them into training, validation and test. They do not explain that this split was stratified by any classifier.  The proportions of the splits that they use seem standard for common data science practice – above a 25% test split.  \n",
    "\n",
    "I am suspicious that there is not enough data in this domain to train some classifiers so I would have preferred this proportion to be lower, closer to 25% or perhaps slightly below.  (But I think this would have been difficult for Story et al. to know in advance and they may have had an idea already since they were building on earlier work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1d24f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "# Create separate dataframes for each group\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)\n",
    "    dataframe.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a7e54",
   "metadata": {},
   "source": [
    "Sanity Check – Seeing the shapes of the dataframe – number of observations and confirming the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b34e9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8068, 511)\n",
      "(2651, 511)\n",
      "(4824, 511)\n"
     ]
    }
   ],
   "source": [
    "print(df_for_pipelining_train.shape)\n",
    "print(df_for_pipelining_val.shape)\n",
    "print(df_for_pipelining_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414ab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation features to use for sentence filtering later\n",
    "clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2db48",
   "metadata": {},
   "source": [
    "# Step 1: select classifier: \"1st_party\"\n",
    "\n",
    "Let's start with 1st Party as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565514c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \"1st_party\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e3f6d",
   "metadata": {},
   "source": [
    "# Step 2: Separate into X and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70967ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be left with the 476 different crafted features (CFs), plus one segment text column.\n",
      "X shape is: (8068, 477)\n"
     ]
    }
   ],
   "source": [
    "# Separate into X and y\n",
    "# the crafted features columns happen to be all those after and including 'contact info', so I \n",
    "# use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "X = pd.concat(\n",
    "    [df_for_pipelining_train['segment_text'], \n",
    "     df_for_pipelining_train.loc[:,'contact info':]],\n",
    "    axis=1\n",
    ").copy()\n",
    "\n",
    "print(\"Should be left with the 476 different crafted features (CFs), plus one segment text column.\")\n",
    "print(f\"X shape is: {X.shape}\")\n",
    "\n",
    "y = df_for_pipelining_train[classifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a935e",
   "metadata": {},
   "source": [
    "Now we have our input data and target ready to pass into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe67851",
   "metadata": {},
   "source": [
    "# Step 3: Set up filter for sentence filtering to use in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e6492",
   "metadata": {},
   "source": [
    "Retrieving the crafted features for 1st Party to use for sentence filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9d7b7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'you', 'us', 'our', 'the app', 'the software']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering the table to get the list object from the same row that lists the classifier\n",
    "classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ]     \\\n",
    "                        .reset_index().at[0,'features']\n",
    "\n",
    "classifier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a750a",
   "metadata": {},
   "source": [
    "Filter for the dataframe to get rows where any of those features is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "256c2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true/false boolean series for sentence filtering:\n",
    "sf_filter = ((X[classifier_features] > 0)\\\n",
    "                 .sum(axis=1) > 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df43b84",
   "metadata": {},
   "source": [
    "Filter for the dataframe to conduct balanced downsize filtering. This is a feature of my sentence_filtering function to filter less stringently if there is not enough data left after using the sf_filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5e418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true/false boolean series for balanced downsizing filter:\n",
    "positive_rows = (y == 1)\n",
    "negative_rows = (y == 0)\n",
    "balanced_downsize_filter = (\n",
    "    positive_rows |\n",
    "    negative_rows.where(negative_rows == True).dropna().sample(n=positive_rows.sum(), replace=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a3e8d",
   "metadata": {},
   "source": [
    "These arguments will be passed into the sentence filtering function during the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfa107c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_kw_args = {'df_filter': sf_filter, 'sf_filter': sf_filter, 'balanced_downsize_filter': balanced_downsize_filter}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c2029",
   "metadata": {},
   "source": [
    "# Step 4: Set up for tf-idf matrix to use in Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98eba40",
   "metadata": {},
   "source": [
    "Using 'column transformer' in the pipeline, and by creating the tf-idf objects in advance, we don't need to manually create the tf-idf matrix, fit it to the data, or transform the data with it, or combine the resulting sparse matrix with the crafted features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11331edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf functions to use\n",
    "tfidf_unigrams = TfidfVectorizer(ngram_range=(1,1), stop_words='english', binary=True)\n",
    "tfidf_withbigrams = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "# Create the column transformations list with columns to apply to (for the column transformer)\n",
    "col_transform_unigrams = [('unigrams_only', tfidf_unigrams, 'segment_text')]\n",
    "col_transform_withbigrams = [('with_bigrams', tfidf_withbigrams, 'segment_text')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927efab",
   "metadata": {},
   "source": [
    "# Step 5: 5-fold CV Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161595e9",
   "metadata": {},
   "source": [
    "There are multiple things going on here that are worth explaining:\n",
    "- 5-fold CV (cross-validation)\n",
    "- Model of choice: SVM\n",
    "- Hyperparameter Grid Search\n",
    "- A note on scoring\n",
    "\n",
    "**5-fold Cross Validation**\n",
    "\n",
    "The technique involves partitioning the dataset into five folds, each of which is used in turn as a test set while the remaining folds are used as training data for a model. This process is repeated a number of times, with each fold being used as a test set once. The average performance across all test sets is then used to judge the model's performance.\n",
    "\n",
    "This means that each different model being tested in the grid search is trained and tested 5 times. This approach can be computationally intensive, but it often results in a more accurate estimate of the model's true performance, because each model is being tested on data that it hasn't seen.\n",
    "\n",
    "**Model of Choice: SVM (Support Vector Machines)**\n",
    "\n",
    "Of all possible machine learning models, Story et al. exclusively mention SVM. I presume that other models were not discussed because:\n",
    "- they were building on earlier work informing them of how to find success with SVM in this domain, or\n",
    "- neglected to explain all the other model searching they conducted\n",
    "\n",
    "SVM is a supervised machine learning algorithm that can be used for both classification and regression tasks. In the n-dimensional space of training datapoints, the SVM algorithm finds the hyperplane that maximizes the margin between the data of the two classes (i.e. the decision boundary). The margin is defined as the distance between the hyperplane and the closest data points.  The closest data points to the margin are used to construct the hyperplane and are thus called the 'support vectors'. This image helps demonstrate the idea. The Kernel is a transformation to help separate the classes.\n",
    "\n",
    "<img src=\"https://www.analytixlabs.co.in/blog/wp-content/uploads/2021/07/Blog-08-1-1536x832.jpg\" alt=\"An example of SVM. Image from analytixlabs\" width=700/>\n",
    "\n",
    "*Image source: analytixlabs*\n",
    "\n",
    "The SVM algorithm has a number of advantages, including its ability to handle non-linear decision boundaries, its robustness to overfitting, and its efficiency in high-dimensional spaces such as with text data.\n",
    "\n",
    "In general the time complexity for support vector machines is between O(m * n^2) and O(m * n^3), where m is the number of features and n is the number of training examples, but it is more complex than this and can be specific to different kernels.  I find greatly increasing training time with more observations. But with this small dataset, given the width of the data, the training times are very manageable.\n",
    "\n",
    "I recommend [Sci-kit Learn's page](https://scikit-learn.org/stable/modules/svm.html) to learn more.\n",
    "\n",
    "**SVM: Which hyperparameters can be tuned?**\n",
    "\n",
    "Story et al. state: \"we use scikit-learn’s SVC implementation (scikit-learn developers 2016b). We train those with a linear kernel (kernel=’linear’), balanced class weights (class weight=’balanced’), and a grid search with five-fold cross-validation over the penalty (C=[0.1, 1, 10]) and gamma (gamma=[0.001, 0.01, 0.1]) parameters.\n",
    "\n",
    "- The Kernel function for transformation, e.g. rbf, polynomial, linear. Having decided that Linear is suitable for this problem, Story et al. only consider a linear kernel, which is known to perform well on text data.\n",
    "- C. Decreasing C corresponds to more regularization. The C parameter adds a penalty for each incorrect classification, so a smaller C value results in a decision boundary with a larger margin, but may also result in a higher number of misclassifications.\n",
    "- Gamma. This is not relevant for linear kernels so I'm unsure why they have included it. (See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) ). It is possible that it was left there as an error after the researchers tried different kernels. The code runs either way.  Although I want to include it in case the researchers had some reason, to save computation time I will not include it in my grid search.\n",
    "\n",
    "**Grid Search**\n",
    "To find the best model for each target, we want to find the combination of pre-processing steps and hyperparameters that gives the best results. So for each classifier, we train and evaluate models for each different combination of pre-processing steps and hyperparameters. Effectively, this 'grid' of steps and hyperparameters is searched over to find the best model.\n",
    "\n",
    "In this case, we are searching over:\n",
    "\n",
    "- Sentence filtering or not\n",
    "- tfidf with only unigrams or unigrams and bigrams\n",
    "- SVC with C=[0.1, 1, 10], OR\n",
    "- if not SVC, logistic regression.\n",
    "\n",
    "**Scoring**\n",
    "\n",
    "I am electing to assess each model's performance by the F1 score (harmonic mean of precision and recall). The F1 metric is an important metric for Story et al.'s work so this will help me compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34da8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "# In the pipeline_sequences list, the syntax requires that each step is named and some value is provided,\n",
    "# but that value will be changed as each different option is looped through in the parameter grid.\n",
    "pipeline_sequences = [\n",
    "    ('sentence_filtering', FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args)),\n",
    "    ('tfidf', ColumnTransformer(col_transform_withbigrams, remainder='passthrough')), \n",
    "    ('model', LogisticRegression(random_state=1, max_iter=1000))\n",
    "]\n",
    "\n",
    "pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'model': [LogisticRegression(random_state=1, max_iter=1000)],\n",
    "        'sentence_filtering': [FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args), \n",
    "                               None],\n",
    "        'tfidf': [ColumnTransformer(col_transform_withbigrams, remainder='passthrough'),\n",
    "                  ColumnTransformer(col_transform_unigrams, remainder='passthrough')]\n",
    "    },\n",
    "    {\n",
    "        'model': [SVC(kernel='linear', class_weight='balanced', random_state=1)],\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'sentence_filtering': [FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args), \n",
    "                               None],\n",
    "        'tfidf': [ColumnTransformer(col_transform_withbigrams, remainder='passthrough'),\n",
    "                  ColumnTransformer(col_transform_unigrams, remainder='passthrough')]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "grid_search_object = GridSearchCV(estimator=pipe, param_grid = param_grid, cv = 5, verbose=1, n_jobs=-1, scoring='f1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1f9b273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.27s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.40s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 309 ms, total: 11.5 s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted_search = grid_search_object.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dc002",
   "metadata": {},
   "source": [
    "# Evaluation on Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bd8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      6235\n",
      "           1       0.93      1.00      0.96      1833\n",
      "\n",
      "    accuracy                           0.98      8068\n",
      "   macro avg       0.97      0.99      0.98      8068\n",
      "weighted avg       0.98      0.98      0.98      8068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_prediction = fitted_search.predict(X)\n",
    "print(classification_report(y, classifier_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0f22",
   "metadata": {},
   "source": [
    "These seem like healthy scores across the board but they are all quite high so there is a risk of overfitting.  I will discuss the results once evaluating the score on the validation set, which allows me to compare to Story et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26df140",
   "metadata": {},
   "source": [
    "# Evaluation on Validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77946625",
   "metadata": {},
   "source": [
    "Loading the X and y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97f4a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval = pd.concat(\n",
    "    [df_for_pipelining_val['segment_text'], \n",
    "     df_for_pipelining_val.loc[:,'contact info':]],\n",
    "    axis=1\n",
    ").copy()\n",
    "\n",
    "yval = df_for_pipelining_val[classifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e3cec",
   "metadata": {},
   "source": [
    "**Storing the model results** \n",
    "\n",
    "I create a series to store the results for each classifier I could make.\n",
    "\n",
    "For each model, I will create a list with 3 values: the fitted search object, the y values and the y predictions. Then each of those lists is stored in a series where the index is the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8578222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper\n",
    "\n",
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037c7dc",
   "metadata": {},
   "source": [
    "Running the model and saving the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "927f92bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model\n",
    "classifier_val_prediction = fitted_search.predict(Xval)\n",
    "\n",
    "# saving the model results for future use\n",
    "model_results[classifier] = [fitted_search, yval, classifier_val_prediction]\n",
    "model_results.to_pickle(\"objects/model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae27cc",
   "metadata": {},
   "source": [
    "Showing confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26c47c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(classifier_y_val, classifier_val_prediction):\n",
    "    \"\"\"\n",
    "    Put the actual y values and the predicted y values into a dataframe with labels and display it\n",
    "    \"\"\"\n",
    "    cf_matrix = confusion_matrix(classifier_y_val, classifier_val_prediction)\n",
    "    cf_df = pd.DataFrame(\n",
    "        cf_matrix, columns=[\"Predicted Negative\", \"Predicted Positive\"], index=[\"True Negative\", \"True Positive\"])\n",
    "    display(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91b32671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1866</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>86</td>\n",
       "      <td>591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1866                 108\n",
       "True Positive                  86                 591"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(yval, classifier_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "357fc73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95      1974\n",
      "           1       0.85      0.87      0.86       677\n",
      "\n",
      "    accuracy                           0.93      2651\n",
      "   macro avg       0.90      0.91      0.90      2651\n",
      "weighted avg       0.93      0.93      0.93      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yval, classifier_val_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772055a5",
   "metadata": {},
   "source": [
    "**Scoring**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab55e38",
   "metadata": {},
   "source": [
    "Only the F1 scores of each classifier is given in the paper (Table 1 pg 4). I assume that this is F1 for class 1, and so we can use that for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf6285",
   "metadata": {},
   "source": [
    "Looks like this scored okay.  Looking at the scores on both the training and validation data, there is somewhat of a gap between the two which implies some overfitting.\n",
    "\n",
    "Reviewing whether to focus on Positive F1 score or the negative F1 score:\n",
    "\n",
    "\"Negative F-1 score\" is important because it relates to when a policy fails to mention an important practice (relevant for detecting violations, important to Story et al.).  We want to be sure that if a policy fails to mention it, the classifier correctly states that it is not mentioned.\n",
    "\n",
    "Relatedly Negative Recall is the proportion of *When it was not in, did it say that it was not in?*<br>\n",
    "Negative Precision is *When it predicted that it wasn't in, how often was that the case?*\n",
    "\n",
    "Positive recall is *When it was in, what was the chance it was identified?*  <br>\n",
    "Positive precision is *When it was predicted to be in, what was the chance that it was in?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1770a1",
   "metadata": {},
   "source": [
    "# Section 2: modelling pipeline\n",
    "\n",
    "Now I can apply the same steps to every classifier to be trained.\n",
    "\n",
    "Firstly, what are the required inputs I need to be ready before training all the classifiers?\n",
    "\n",
    "## Requirements for modelling pipeline\n",
    "\n",
    "1. List of all the different classifiers to train\n",
    "2. Empty table of classifier results to populate\n",
    "3. `df_for_pipelining_train/val/test`. This is the appropriate dataframe with all the X and y data.\n",
    "4. tf-idf column transformers to use in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd1fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating this list here so that the below cells can all be ran without running any above cells\n",
    "\n",
    "# Requirement 1: List of all the different classifiers to train\n",
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper\n",
    "\n",
    "# Requirement 2: Empty table of classifier results to populate\n",
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)\n",
    "\n",
    "full_results_table = pd.DataFrame(model_results, columns=[\"Frequency in train set\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77316778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 3: Loading the data\n",
    "\n",
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "# Create separate dataframes for each group\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)\n",
    "    dataframe.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f606c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirement 4: tf-idf column transformers to use in the pipeline\n",
    "\n",
    "tfidf_unigrams = TfidfVectorizer(ngram_range=(1,1), stop_words='english', binary=True)\n",
    "tfidf_withbigrams = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "# Create the column transformations list with columns to apply to (for the column transformer)\n",
    "col_transform_unigrams = [('unigrams_only', tfidf_unigrams, 'segment_text')]\n",
    "col_transform_withbigrams = [('with_bigrams', tfidf_withbigrams, 'segment_text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec12502",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f4da8",
   "metadata": {},
   "source": [
    "Next, what are the required steps that will happen in training and evaluating each different classifier?\n",
    "\n",
    "The steps are:\n",
    "\n",
    "- Select the classifier\n",
    "- Separate the training data into X and y\n",
    "- Set up filter for sentence filtering to use in Pipeline\n",
    "- CV Grid Search\n",
    "- Separate the validation data into X and y, run the model on them and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b12cfd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_modelling_pipeline(classifier, df_for_pipelining, df_for_evaluation, model_results_series, \n",
    "                            full_results_table, col_transform_unigrams, col_transform_withbigrams):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "        classifier: name (exact string) of the target column to predict\n",
    "        \n",
    "        df_for_pipelining: Dataframe with text segments, classifiers and crafted features\n",
    "        \n",
    "        model_results_series:   The empty series to save the results (classifier-objects and predictions) to. \n",
    "                                A pickle file of the same name will be saved with the results. \n",
    "                                \n",
    "        full_results_table:     An empty dataframe to show the results of all the classifiers \n",
    "        \n",
    "        col_transform_unigrams:    for the column transformer, column transformation with column to apply to\n",
    "        \n",
    "        col_transform_withbigrams: as above\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1 – declare the classifier\n",
    "    \n",
    "    print(f\"Running for classifier: {classifier}\")\n",
    "    start_code_time = time.time()\n",
    "    \n",
    "    # Step 2 – Separate into X and y\n",
    "    \n",
    "    # the crafted features columns happen to be all those after and including 'contact info', so I \n",
    "    # use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "    X = pd.concat(\n",
    "        [df_for_pipelining['segment_text'], \n",
    "         df_for_pipelining.loc[:,'contact info':]],\n",
    "        axis=1\n",
    "    ).copy()\n",
    "\n",
    "    y = df_for_pipelining[classifier]\n",
    "    \n",
    "    # Step 3 – filter for sentence filtering to use in Pipeline\n",
    "    \n",
    "    # filtering the crafted features to get the list of features from the same row that lists the classifier\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ]     \\\n",
    "                            .reset_index().at[0,'features']\n",
    "    \n",
    "    # true/false boolean series for sentence filtering:\n",
    "    sf_filter = ((X[classifier_features] > 0)\\\n",
    "                     .sum(axis=1) > 0 )\n",
    "    \n",
    "    # true/false boolean series for balanced downsizing filter:\n",
    "    positive_rows = (y == 1)\n",
    "    negative_rows = (y == 0)\n",
    "    balanced_downsize_filter = (\n",
    "        positive_rows |\n",
    "        negative_rows.where(negative_rows == True).dropna().sample(n=positive_rows.sum(), replace=False)\n",
    "    )\n",
    "    \n",
    "    print(\"Ready for grid search.\")\n",
    "    \n",
    "    # Step 4 – CV Grid Search\n",
    "    \n",
    "    fitted_search = model_pipeline_step_4(X, y, sf_filter, balanced_downsize_filter, \n",
    "                                          col_transform_unigrams, col_transform_withbigrams)\n",
    "\n",
    "    print(\"Ready for evaluation\")\n",
    "    \n",
    "    # Step 5 – separate test (evaluation) data into X and y, run the model on them and save the results\n",
    "    \n",
    "    X_eval = pd.concat(\n",
    "        [df_for_evaluation['segment_text'], \n",
    "         df_for_evaluation.loc[:,'contact info':]],\n",
    "        axis=1\n",
    "    ).copy()\n",
    "\n",
    "    y_eval = df_for_evaluation[classifier]\n",
    "    \n",
    "    # Running the model\n",
    "    classifier_prediction = fitted_search.predict(X_eval)\n",
    "\n",
    "    # saving the model results for future use\n",
    "    model_results_series[classifier] = [fitted_search, y_eval, classifier_prediction]\n",
    "    model_results_series.to_pickle(\"objects/model_results.pkl\")\n",
    "    \n",
    "    # Populating full results table\n",
    "    print(\"Populating results table\")\n",
    "    \n",
    "    # Total instances in dataset\n",
    "    full_results_table.loc[classifier, \"Frequency in train set\"] = \\\n",
    "        df_for_pipelining[classifier].sum()\n",
    "\n",
    "    # Neg F1\n",
    "    full_results_table.loc[classifier, \"Neg F1\"] = \\\n",
    "        f1_score(model_results_series[classifier][1].copy(), model_results_series[classifier][2].copy(), pos_label=0)\n",
    "\n",
    "    # Pos F1\n",
    "    full_results_table.loc[classifier, \"Pos F1\"] = \\\n",
    "        f1_score(model_results_series[classifier][1].copy(), model_results_series[classifier][2].copy(), pos_label=1)\n",
    "\n",
    "    # Sentence Filtering\n",
    "    full_results_table.loc[classifier, \"Sentence Filtering\"] = \\\n",
    "        str(fitted_search.best_estimator_.steps[0][1])\n",
    "\n",
    "    # Including Bigrams or not\n",
    "    full_results_table.loc[classifier, \"including bigrams or not\"] = \\\n",
    "        fitted_search.best_estimator_.named_steps[\"tfidf\"].get_params()[\"transformers\"][0][0]\n",
    "\n",
    "    # Model type\n",
    "    full_results_table.loc[classifier, \"SVM or Logistic Regression\"] = \\\n",
    "        fitted_search.best_estimator_.named_steps[\"model\"]\n",
    "    \n",
    "    # Tests\n",
    "    if type(model_results_series[classifier]) == int:\n",
    "        print(\"Model results not saved.\")\n",
    "        raise NotSavedError(\"Check model results\")\n",
    "    \n",
    "    print(f\"The runtime for {classifier} was {round(time.time() - start_code_time, 5)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b05a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_4(X, y, sf_filter, balanced_downsize_filter, \n",
    "                          col_transform_unigrams, col_transform_withbigrams):\n",
    "\n",
    "    cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "    sf_kw_args = {'df_filter': sf_filter, 'sf_filter': sf_filter, 'balanced_downsize_filter': balanced_downsize_filter}\n",
    "    \n",
    "    # In the pipeline_sequences list, the syntax requires that each step is named and some value is provided,\n",
    "    # but that value will be changed as each different option is looped through in the parameter grid.\n",
    "    pipeline_sequences = [\n",
    "        ('sentence_filtering', FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args)),\n",
    "        ('tfidf', ColumnTransformer(col_transform_withbigrams, remainder='passthrough')), \n",
    "        ('model', LogisticRegression(random_state=1, max_iter=1000))\n",
    "    ]\n",
    "\n",
    "    pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "    param_grid = [\n",
    "        {\n",
    "            'model': [LogisticRegression(random_state=1, max_iter=1000)],\n",
    "            'sentence_filtering': [FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args), \n",
    "                                   None],\n",
    "            'tfidf': [ColumnTransformer(col_transform_withbigrams, remainder='passthrough'),\n",
    "                      ColumnTransformer(col_transform_unigrams, remainder='passthrough')]\n",
    "        },\n",
    "        {\n",
    "            'model': [SVC(kernel='linear', class_weight='balanced', random_state=1)],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'sentence_filtering': [FunctionSampler(func=priv_pol_funcs.sentence_filtering, validate=False, kw_args=sf_kw_args), \n",
    "                                   None],\n",
    "            'tfidf': [ColumnTransformer(col_transform_withbigrams, remainder='passthrough'),\n",
    "                      ColumnTransformer(col_transform_unigrams, remainder='passthrough')]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create grid search object\n",
    "    grid_search_object = GridSearchCV(estimator=pipe, param_grid = param_grid, cv = 5, verbose=1, n_jobs=-1, scoring='f1')\n",
    "    \n",
    "    fitted_search = grid_search_object.fit(X, y)\n",
    "    \n",
    "    return fitted_search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100305c8",
   "metadata": {},
   "source": [
    "Pass every different classifier into the modelling pipeline:\n",
    "\n",
    "**! This takes a few minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76899f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_results = model_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94970d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.14s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Contact was 27.97552\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.37s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Contact_E_Mail_Address was 35.71502\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.42s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Contact_Phone_Number was 36.9169\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 30.80656\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.26s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Identifier_Device_ID was 27.60328\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.27s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Identifier_IMEI was 14.27251\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Identifier_MAC was 13.14747\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.14s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.72s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Identifier_Mobile_Carrier was 18.03328\n",
      "\n",
      "Running for classifier: Location\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.23s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.23s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Location was 26.43787\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.60s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Location_Cell_Tower was 18.77298\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.14s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Location_GPS was 23.18793\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.33s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.35s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.83s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Location_WiFi was 24.30736\n",
      "\n",
      "Running for classifier: SSO\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.14s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for SSO was 22.79482\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.27s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for Facebook_SSO was 14.17362\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.11s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.13s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for 1st_party was 96.22741\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.88s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.88s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.89s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.90s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.94s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.24s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.39s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for 3rd_party was 54.97772\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.08s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.05s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.16s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.24s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for PERFORMED was 94.30847\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Ready for grid search.\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.70s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.75s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 0.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.20s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "/Users/chinchcliffe/opt/anaconda3/envs/priv_pol_nlp/lib/python3.10/site-packages/imblearn/pipeline.py:240: UserWarning: Persisting input arguments took 1.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for evaluation\n",
      "Populating results table\n",
      "The runtime for NOT_PERFORMED was 36.58304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    \n",
    "    full_modelling_pipeline(classifier=each_classifier, df_for_pipelining=df_for_pipelining_train, \n",
    "    df_for_evaluation=df_for_pipelining_val, model_results_series=initial_results, \n",
    "    full_results_table = full_results_table, \n",
    "    col_transform_unigrams=col_transform_unigrams, col_transform_withbigrams=col_transform_withbigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cac6ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency in train set</th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Pos F1</th>\n",
       "      <th>Sentence Filtering</th>\n",
       "      <th>including bigrams or not</th>\n",
       "      <th>SVM or Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>128</td>\n",
       "      <td>0.992875</td>\n",
       "      <td>0.660550</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=0.1, class_weight='balanced', kernel='li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>662</td>\n",
       "      <td>0.986949</td>\n",
       "      <td>0.839196</td>\n",
       "      <td>None</td>\n",
       "      <td>unigrams_only</td>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>346</td>\n",
       "      <td>0.991876</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>596</td>\n",
       "      <td>0.990924</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>332</td>\n",
       "      <td>0.994713</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=10, class_weight='balanced', kernel='lin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>50</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=0.1, class_weight='balanced', kernel='li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>85</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>None</td>\n",
       "      <td>unigrams_only</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>60</td>\n",
       "      <td>0.996014</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>537</td>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.825737</td>\n",
       "      <td>None</td>\n",
       "      <td>unigrams_only</td>\n",
       "      <td>LogisticRegression(max_iter=1000, random_state=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>89</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>142</td>\n",
       "      <td>0.996728</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>126</td>\n",
       "      <td>0.996352</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>146</td>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>109</td>\n",
       "      <td>0.997318</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>None</td>\n",
       "      <td>unigrams_only</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>1833</td>\n",
       "      <td>0.950586</td>\n",
       "      <td>0.859012</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>530</td>\n",
       "      <td>0.972808</td>\n",
       "      <td>0.641711</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>1681</td>\n",
       "      <td>0.945706</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>None</td>\n",
       "      <td>with_bigrams</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>551</td>\n",
       "      <td>0.981420</td>\n",
       "      <td>0.803493</td>\n",
       "      <td>None</td>\n",
       "      <td>unigrams_only</td>\n",
       "      <td>SVC(C=1, class_weight='balanced', kernel='line...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Frequency in train set    Neg F1    Pos F1  \\\n",
       "Contact                                              128  0.992875  0.660550   \n",
       "Contact_E_Mail_Address                               662  0.986949  0.839196   \n",
       "Contact_Phone_Number                                 346  0.991876  0.839216   \n",
       "Identifier_Cookie_or_similar_Tech                    596  0.990924  0.903084   \n",
       "Identifier_Device_ID                                 332  0.994713  0.861538   \n",
       "Identifier_IMEI                                       50  0.999620  0.937500   \n",
       "Identifier_MAC                                        85  0.998669  0.837209   \n",
       "Identifier_Mobile_Carrier                             60  0.996014  0.363636   \n",
       "Location                                             537  0.986813  0.825737   \n",
       "Location_Cell_Tower                                   89  0.995615  0.596491   \n",
       "Location_GPS                                         142  0.996728  0.841121   \n",
       "Location_WiFi                                        126  0.996352  0.795699   \n",
       "SSO                                                  146  0.995004  0.734694   \n",
       "Facebook_SSO                                         109  0.997318  0.829268   \n",
       "1st_party                                           1833  0.950586  0.859012   \n",
       "3rd_party                                            530  0.972808  0.641711   \n",
       "PERFORMED                                           1681  0.945706  0.824000   \n",
       "NOT_PERFORMED                                        551  0.981420  0.803493   \n",
       "\n",
       "                                  Sentence Filtering including bigrams or not  \\\n",
       "Contact                                         None             with_bigrams   \n",
       "Contact_E_Mail_Address                          None            unigrams_only   \n",
       "Contact_Phone_Number                            None             with_bigrams   \n",
       "Identifier_Cookie_or_similar_Tech               None             with_bigrams   \n",
       "Identifier_Device_ID                            None             with_bigrams   \n",
       "Identifier_IMEI                                 None             with_bigrams   \n",
       "Identifier_MAC                                  None            unigrams_only   \n",
       "Identifier_Mobile_Carrier                       None             with_bigrams   \n",
       "Location                                        None            unigrams_only   \n",
       "Location_Cell_Tower                             None             with_bigrams   \n",
       "Location_GPS                                    None             with_bigrams   \n",
       "Location_WiFi                                   None             with_bigrams   \n",
       "SSO                                             None             with_bigrams   \n",
       "Facebook_SSO                                    None            unigrams_only   \n",
       "1st_party                                       None             with_bigrams   \n",
       "3rd_party                                       None             with_bigrams   \n",
       "PERFORMED                                       None             with_bigrams   \n",
       "NOT_PERFORMED                                   None            unigrams_only   \n",
       "\n",
       "                                                          SVM or Logistic Regression  \n",
       "Contact                            SVC(C=0.1, class_weight='balanced', kernel='li...  \n",
       "Contact_E_Mail_Address             LogisticRegression(max_iter=1000, random_state=1)  \n",
       "Contact_Phone_Number               SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Identifier_Cookie_or_similar_Tech  SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Identifier_Device_ID               SVC(C=10, class_weight='balanced', kernel='lin...  \n",
       "Identifier_IMEI                    SVC(C=0.1, class_weight='balanced', kernel='li...  \n",
       "Identifier_MAC                     SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Identifier_Mobile_Carrier          SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Location                           LogisticRegression(max_iter=1000, random_state=1)  \n",
       "Location_Cell_Tower                SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Location_GPS                       SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Location_WiFi                      SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "SSO                                SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "Facebook_SSO                       SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "1st_party                          SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "3rd_party                          SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "PERFORMED                          SVC(C=1, class_weight='balanced', kernel='line...  \n",
       "NOT_PERFORMED                      SVC(C=1, class_weight='balanced', kernel='line...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aa1004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative F1 mean: 0.9872217712600695\n",
      "Positive F1 mean: 0.7773976302357587\n"
     ]
    }
   ],
   "source": [
    "print(f\"Negative F1 mean: {full_results_table['Neg F1'].mean()}\")\n",
    "print(f\"Positive F1 mean: {full_results_table['Pos F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74ae2d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=&#x27;/var/folders/d2/s5sb3p416xbgzjf589bgpp_w0000gn/T/tmpj2c7b1ym&#x27;,\n",
       "         steps=[(&#x27;sentence_filtering&#x27;, None),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;with_bigrams&#x27;,\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  stop_words=&#x27;english&#x27;),\n",
       "                                                  &#x27;segment_text&#x27;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 SVC(C=10, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;,\n",
       "                     random_state=1))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-91\" type=\"checkbox\" ><label for=\"sk-estimator-id-91\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=&#x27;/var/folders/d2/s5sb3p416xbgzjf589bgpp_w0000gn/T/tmpj2c7b1ym&#x27;,\n",
       "         steps=[(&#x27;sentence_filtering&#x27;, None),\n",
       "                (&#x27;tfidf&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;with_bigrams&#x27;,\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  stop_words=&#x27;english&#x27;),\n",
       "                                                  &#x27;segment_text&#x27;)])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 SVC(C=10, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;,\n",
       "                     random_state=1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-92\" type=\"checkbox\" ><label for=\"sk-estimator-id-92\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">None</label><div class=\"sk-toggleable__content\"><pre>None</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\" ><label for=\"sk-estimator-id-93\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tfidf: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;with_bigrams&#x27;,\n",
       "                                 TfidfVectorizer(binary=True,\n",
       "                                                 ngram_range=(1, 2),\n",
       "                                                 stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;segment_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">with_bigrams</label><div class=\"sk-toggleable__content\"><pre>segment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(binary=True, ngram_range=(1, 2), stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\" ><label for=\"sk-estimator-id-96\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;contact info&#x27;, &#x27;contact details&#x27;, &#x27;contact data&#x27;, &#x27;e.g., your name&#x27;, &#x27;contact you&#x27;, &#x27;your contact&#x27;, &#x27;identify, contact&#x27;, &#x27;identifying information&#x27;, &#x27;your name, address, and e-mail address&#x27;, &#x27;including e-mail&#x27;, &#x27;phone book&#x27;, &#x27;phonebook&#x27;, &#x27;contact information in your device&#x27;, &#x27;address book&#x27;, &#x27;contacts&#x27;, &#x27;contact names&#x27;, &#x27;contact list&#x27;, &#x27;contacts list&#x27;, &#x27;phone contacts&#x27;, &#x27;contact entries&#x27;, &#x27;import contacts&#x27;, &#x27;friend list&#x27;, &#x27;friends list&#x27;, &#x27;city&#x27;, &#x27;hometown&#x27;, &#x27;e-mail address&#x27;, &#x27;email address&#x27;, &#x27;e-mail and mailing address&#x27;, &#x27;email and mailing address&#x27;, &#x27;e-mail or mailing address&#x27;, &#x27;email or mailing address&#x27;, &#x27;password&#x27;, &#x27;authentication process&#x27;, &#x27;credential&#x27;, &#x27;authentication token&#x27;, &#x27;phone&#x27;, &#x27;number call&#x27;, &#x27;mailing address&#x27;, &#x27;street address&#x27;, &#x27;address,&#x27;, &#x27;address&#x27;, &#x27;postal address&#x27;, &#x27;billing address&#x27;, &#x27;shipping address&#x27;, &#x27;home or work address&#x27;, &#x27;other address&#x27;, &#x27;physical address&#x27;, &#x27;your address&#x27;, &#x27;home address&#x27;, &#x27;residential address&#x27;, &#x27;area code&#x27;, &#x27;zip code&#x27;, &#x27;zip-code&#x27;, &#x27;postal code&#x27;, &#x27;zip/postal code&#x27;, &#x27;postcode&#x27;, &#x27;post code&#x27;, &#x27;demographic&#x27;, &#x27;age&#x27;, &#x27;birth&#x27;, &#x27;gender&#x27;, &#x27;male&#x27;, &#x27;sex&#x27;, &#x27;login credentials from one of your accounts&#x27;, &#x27;application authentication options&#x27;, &#x27;receives your information from an sns&#x27;, &#x27;accessed on a third party platform or social network&#x27;, &#x27;register using your user credentials to certain social media sites&#x27;, &#x27;allow us to access and/or collect certain information from your third party platform profile/account&#x27;, &#x27;logging in to the application using third party social network&#x27;, &#x27;accessing the services through a social network&#x27;, &#x27;when you choose to connect with those services&#x27;, &#x27;if you choose to register your app account via such social media providers&#x27;, &#x27;third party platform&#x27;, &#x27;identifier&#x27;, &#x27;unique id&#x27;, &#x27;adid&#x27;, &#x27;ad id&#x27;, &#x27;advertising id&#x27;, &#x27;idfa&#x27;, &#x27;identifier for advertis&#x27;, &#x27;identifiers for advertis&#x27;, &#x27;advertisement id&#x27;, &#x27;gaid&#x27;, &#x27;id for advertis&#x27;, &#x27;ids for advertis&#x27;, &#x27;aaid&#x27;, &#x27;cookie&#x27;, &#x27;web beacon&#x27;, &#x27;flash cookie&#x27;, &#x27;local shared object&#x27;, &#x27;pixel tag&#x27;, &#x27;local storage&#x27;, &#x27;tracking technolog&#x27;, &#x27;tracing technolog&#x27;, &#x27;tracking pixel&#x27;, &#x27;similar technolog&#x27;, &#x27;invisible image&#x27;, &#x27;device id&#x27;, &#x27;android id&#x27;, &#x27;udid&#x27;, &#x27;iphone id&#x27;, &#x27;identifier associated with your device&#x27;, &#x27;identifiers associated with your device&#x27;, &#x27;id associated with your device&#x27;, &#x27;ids associated with your device&#x27;, &#x27;mobile id&#x27;, &#x27;device and application id&#x27;, &#x27;identifier for your device&#x27;, &#x27;identifiers for your device&#x27;, &#x27;id for your device&#x27;, &#x27;ids for your device&#x27;, &#x27;id of your device&#x27;, &#x27;ids of your device&#x27;, &#x27;identifier of your device&#x27;, &#x27;identifiers of your device&#x27;, &#x27;id of your mobile device&#x27;, &#x27;ids of your mobile device&#x27;, &#x27;identifier of your mobile device&#x27;, &#x27;identifiers of your mobile device&#x27;, &#x27;id for vendors&#x27;, &#x27;ids for vendors&#x27;, &#x27;identifier for vendors&#x27;, &#x27;identifiers for vendors&#x27;, &#x27;idvf&#x27;, &#x27;odin&#x27;, &#x27;device model number&#x27;, &#x27;imei&#x27;, &#x27;international mobile equipment&#x27;, &#x27;equipment id&#x27;, &#x27;imsi&#x27;, &#x27;international mobile subscriber&#x27;, &#x27;internet protocol&#x27;, &#x27;ip address&#x27;, &#x27;(ip) address&#x27;, &#x27;number that is automatically assigned to your computer when you use the internet&#x27;, &#x27;mac&#x27;, &#x27;media access control&#x27;, &#x27;machine id&#x27;, &#x27;hardware id&#x27;, &#x27;hardware or device id&#x27;, &#x27;hardware-based id&#x27;, &#x27;hardware based id&#x27;, &#x27;mobile network&#x27;, &#x27;mobile carrier&#x27;, &#x27;mobile operator&#x27;, &#x27;device carrier&#x27;, &#x27;carrier provider&#x27;, &#x27;wireless carrier/operator&#x27;, &#x27;wireless carrier&#x27;, &#x27;phone carrier&#x27;, &#x27;mobile service provider&#x27;, &#x27;mobile or internet carrier&#x27;, &#x27;carrier detail&#x27;, &#x27;serial number&#x27;, &#x27;sim&#x27;, &#x27;subscriber identification module&#x27;, &#x27;iccid&#x27;, &#x27;serial no&#x27;, &#x27;wifi name&#x27;, &#x27;wi-fi name&#x27;, &#x27;names of wifi&#x27;, &#x27;names of wi-fi&#x27;, &#x27;name of wifi&#x27;, &#x27;name of wi-fi&#x27;, &#x27;names of connected wifi&#x27;, &#x27;names of connected wi-fi&#x27;, &#x27;name of connected wifi&#x27;, &#x27;name of connected wi-fi&#x27;, &#x27;names of your wifi&#x27;, &#x27;names of your wi-fi&#x27;, &#x27;name of your wifi&#x27;, &#x27;name of your wi-fi&#x27;, &#x27;wifi networks used&#x27;, &#x27;wi-fi networks used&#x27;, &#x27;wifi network used&#x27;, &#x27;wi-fi network used&#x27;, &#x27;bssid&#x27;, &#x27;ssid&#x27;, &#x27;service set id&#x27;, &#x27;locat&#x27;, &#x27;geo&#x27;, &#x27;devices beacon&#x27;, &#x27;device beacon&#x27;, &#x27;bluetooth&#x27;, &#x27;precise location&#x27;, &#x27;precise geo&#x27;, &#x27;precise device location&#x27;, &#x27;precise device geo&#x27;, &#x27;specific location&#x27;, &#x27;specific geo&#x27;, &#x27;specific device location&#x27;, &#x27;specific device geo&#x27;, &#x27;exact location&#x27;, &#x27;exact geo&#x27;, &#x27;exact device location&#x27;, &#x27;exact device geo&#x27;, &#x27;detailed location&#x27;, &#x27;detailed geo&#x27;, &#x27;detailed device location&#x27;, &#x27;detailed device geo&#x27;, &#x27;accurate location&#x27;, &#x27;accurate geo&#x27;, &#x27;accurate device location&#x27;, &#x27;accurate device geo&#x27;, &#x27;precise information about the location&#x27;, &#x27;precise information about the geo&#x27;, &#x27;precise information about the device location&#x27;, &#x27;precise information about the device geo&#x27;, &#x27;base station&#x27;, &#x27;cell tower&#x27;, &#x27;cell id&#x27;, &#x27;network-based&#x27;, &#x27;network based&#x27;, &#x27;cellular network location&#x27;, &#x27;approximate location&#x27;, &#x27;approximate geo&#x27;, &#x27;approximate device location&#x27;, &#x27;approximate device geo&#x27;, &#x27;approximate a location&#x27;, &#x27;coarse location&#x27;, &#x27;coarse geo&#x27;, &#x27;coarse device location&#x27;, &#x27;coarse device geo&#x27;, &#x27;rough location&#x27;, &#x27;rough geo&#x27;, &#x27;rough device location&#x27;, &#x27;rough device geo&#x27;, &#x27;imprecise location&#x27;, &#x27;imprecise geo&#x27;, &#x27;imprecise device location&#x27;, &#x27;imprecise device geo&#x27;, &#x27;located with less precision&#x27;, &#x27;non-precise location&#x27;, &#x27;non-precise geo&#x27;, &#x27;non-precise device location&#x27;, &#x27;non-precise device geo&#x27;, &#x27;non-specific location&#x27;, &#x27;non-specific geo&#x27;, &#x27;non-specific device location&#x27;, &#x27;not-specific device geo&#x27;, &#x27;not-specific location&#x27;, &#x27;not-specific geo&#x27;, &#x27;not-specific device location&#x27;, &#x27;general location&#x27;, &#x27;general geo&#x27;, &#x27;general device location&#x27;, &#x27;general device geo&#x27;, &#x27;generally where you are located&#x27;, &#x27;based on proximity&#x27;, &#x27;gps&#x27;, &#x27;global position&#x27;, &#x27;latitud&#x27;, &#x27;longitud&#x27;, &#x27;wifi signal&#x27;, &#x27;wifi access point&#x27;, &#x27;wifi location&#x27;, &#x27;wifi geo&#x27;, &#x27;location based on gps/wifi/communications&#x27;, &#x27;wi-fi signal&#x27;, &#x27;wi-fi access point&#x27;, &#x27;wi-fi location&#x27;, &#x27;wi-fi geo&#x27;, &#x27;location based on gps/wi-fi/communications&#x27;, &#x27;we&#x27;, &#x27;you&#x27;, &#x27;us&#x27;, &#x27;our&#x27;, &#x27;the app&#x27;, &#x27;the software&#x27;, &#x27;partner&#x27;, &#x27;third part&#x27;, &#x27;third-part&#x27;, &#x27;service provider&#x27;, &#x27;contractor&#x27;, &#x27;vendor&#x27;, &#x27;provider&#x27;, &#x27;supplier&#x27;, &#x27;analytics compan&#x27;, &#x27;advertising compan&#x27;, &#x27;advertiser&#x27;, &#x27;brand owner&#x27;, &#x27;sponsor&#x27;, &#x27;advertising network&#x27;, &#x27;trusted affiliate&#x27;, &#x27;ad serv&#x27;, &#x27;facebook&#x27;, &#x27;ad network&#x27;, &#x27;social&#x27;, &#x27;google&#x27;, &#x27;our behalf&#x27;, &#x27;ad compan&#x27;, &#x27;flurry&#x27;, &#x27;these providers&#x27;, &#x27;these entities&#x27;, &#x27;ad server&#x27;, &#x27;crosswise&#x27;, &#x27;tapad&#x27;, &#x27;agent&#x27;, &#x27;share&#x27;, &#x27;shari&#x27;, &#x27;collect&#x27;, &#x27;consent&#x27;, &#x27;permission&#x27;, &#x27;opt&#x27;, &#x27;access&#x27;, &#x27;gather&#x27;, &#x27;obtain&#x27;, &#x27;require&#x27;, &#x27;use&#x27;, &#x27;receive&#x27;, &#x27;store&#x27;, &#x27;save&#x27;, &#x27;disclose&#x27;, &#x27;automatically&#x27;, &#x27;give&#x27;, &#x27;sell&#x27;, &#x27;allow&#x27;, &#x27;process&#x27;, &#x27;track&#x27;, &#x27;possible&#x27;, &#x27;offer&#x27;, &#x27;employ&#x27;, &#x27;read&#x27;, &#x27;have&#x27;, &#x27;record&#x27;, &#x27;acquire&#x27;, &#x27;get&#x27;, &#x27;view&#x27;, &#x27;locate&#x27;, &#x27;ask&#x27;, &#x27;utilize&#x27;, &#x27;tell&#x27;, &#x27;request&#x27;, &#x27;be transmitted&#x27;, &#x27;be communicated&#x27;, &#x27;not collect&#x27;, &#x27;no longer collect&#x27;, &#x27;not access&#x27;, &#x27;not gather&#x27;, &#x27;not obtain&#x27;, &#x27;not require&#x27;, &#x27;not use&#x27;, &#x27;not receive&#x27;, &#x27;not share&#x27;, &#x27;not store&#x27;, &#x27;not save&#x27;, &#x27;not disclose&#x27;, &#x27;not automatically&#x27;, &#x27;not give&#x27;, &#x27;not sell&#x27;, &#x27;not allow&#x27;, &#x27;not process&#x27;, &#x27;not track&#x27;, &#x27;not possible&#x27;, &#x27;not offer&#x27;, &#x27;not employ&#x27;, &#x27;not read&#x27;, &#x27;not have&#x27;, &#x27;not record&#x27;, &#x27;not acquire&#x27;, &#x27;not get&#x27;, &#x27;not view&#x27;, &#x27;not locate&#x27;, &#x27;not ask&#x27;, &#x27;not utilize&#x27;, &#x27;not tell&#x27;, &#x27;not request&#x27;, &#x27;not communicate&#x27;, &#x27;not transmit&#x27;, &#x27;not be transmitted&#x27;, &#x27;not be communicated&#x27;, &#x27;not provide&#x27;, &#x27;not have to&#x27;, &quot;n&#x27;t&quot;, &#x27;not&#x27;, &#x27;never&#x27;, &#x27;no longer&#x27;, &#x27;will not&#x27;, &quot;n&#x27;t collect&quot;, &quot;n&#x27;t access&quot;, &quot;n&#x27;t gather&quot;, &quot;n&#x27;t obtain&quot;, &quot;n&#x27;t require&quot;, &quot;n&#x27;t use&quot;, &quot;n&#x27;t receive&quot;, &quot;n&#x27;t share&quot;, &quot;n&#x27;t store&quot;, &quot;n&#x27;t save&quot;, &quot;n&#x27;t disclose&quot;, &quot;n&#x27;t automatically&quot;, &quot;n&#x27;t give&quot;, &quot;n&#x27;t sell&quot;, &quot;n&#x27;t allow&quot;, &quot;n&#x27;t process&quot;, &quot;n&#x27;t track&quot;, &quot;n&#x27;t possible&quot;, &quot;n&#x27;t offer&quot;, &quot;n&#x27;t employ&quot;, &quot;n&#x27;t read&quot;, &quot;n&#x27;t have&quot;, &quot;n&#x27;t record&quot;, &quot;n&#x27;t acquire&quot;, &quot;n&#x27;t get&quot;, &quot;n&#x27;t view&quot;, &quot;n&#x27;t locate&quot;, &quot;n&#x27;t ask&quot;, &quot;n&#x27;t utilize&quot;, &quot;n&#x27;t tell&quot;, &quot;n&#x27;t request&quot;, &quot;n&#x27;t be transmitted&quot;, &quot;n&#x27;t be communicated&quot;, &#x27;never collect&#x27;, &#x27;never access&#x27;, &#x27;never gather&#x27;, &#x27;never obtain&#x27;, &#x27;never require&#x27;, &#x27;never use&#x27;, &#x27;never receive&#x27;, &#x27;never share&#x27;, &#x27;never store&#x27;, &#x27;never save&#x27;, &#x27;never disclose&#x27;, &#x27;never automatically&#x27;, &#x27;never give&#x27;, &#x27;never sell&#x27;, &#x27;never allow&#x27;, &#x27;never process&#x27;, &#x27;never track&#x27;, &#x27;never possible&#x27;, &#x27;never offer&#x27;, &#x27;never employ&#x27;, &#x27;never read&#x27;, &#x27;never have&#x27;, &#x27;never record&#x27;, &#x27;never acquire&#x27;, &#x27;never get&#x27;, &#x27;never view&#x27;, &#x27;never locate&#x27;, &#x27;never ask&#x27;, &#x27;never utilize&#x27;, &#x27;never tell&#x27;, &#x27;never request&#x27;, &#x27;never transmit&#x27;, &#x27;never communicate&#x27;, &#x27;never be collected&#x27;, &#x27;never be accessed&#x27;, &#x27;never be gathered&#x27;, &#x27;never be obtained&#x27;, &#x27;never be required&#x27;, &#x27;never be used&#x27;, &#x27;never be received&#x27;, &#x27;never be shared&#x27;, &#x27;never be stored&#x27;, &#x27;never be saved&#x27;, &#x27;never be disclosed&#x27;, &#x27;never be automatically&#x27;, &#x27;never be given&#x27;, &#x27;never be sold&#x27;, &#x27;never be allowed&#x27;, &#x27;never be processed&#x27;, &#x27;never be tracked&#x27;, &#x27;never be offered&#x27;, &#x27;never be employed&#x27;, &#x27;never be read&#x27;, &#x27;never be recorded&#x27;, &#x27;never be acquired&#x27;, &#x27;never be viewed&#x27;, &#x27;never be located&#x27;, &#x27;never be asked&#x27;, &#x27;never be utilized&#x27;, &#x27;never be requested&#x27;, &#x27;never be transmitted&#x27;, &#x27;never be communicated&#x27;, &#x27;nor do we collect&#x27;, &#x27;does not tell us&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\" ><label for=\"sk-estimator-id-97\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\" ><label for=\"sk-estimator-id-98\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, class_weight=&#x27;balanced&#x27;, kernel=&#x27;linear&#x27;, random_state=1)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(memory='/var/folders/d2/s5sb3p416xbgzjf589bgpp_w0000gn/T/tmpj2c7b1ym',\n",
       "         steps=[('sentence_filtering', None),\n",
       "                ('tfidf',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('with_bigrams',\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               2),\n",
       "                                                                  stop_words='english'),\n",
       "                                                  'segment_text')])),\n",
       "                ('model',\n",
       "                 SVC(C=10, class_weight='balanced', kernel='linear',\n",
       "                     random_state=1))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_results['Identifier_Device_ID'][0].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc23fc",
   "metadata": {},
   "source": [
    "The results for any classifier can then be retrieved from the results series. The series stores the model and the actual and predicted y-values in the form `[model_object, true_y, predicted_y]`\n",
    "\n",
    "Here is how a classification report could be produced for one of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c91e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2429\n",
      "           1       0.95      0.75      0.84       222\n",
      "\n",
      "    accuracy                           0.98      2651\n",
      "   macro avg       0.96      0.87      0.91      2651\n",
      "weighted avg       0.98      0.98      0.97      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(initial_results['Contact_E_Mail_Address'][1] , \n",
    "                            initial_results['Contact_E_Mail_Address'][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1872f19",
   "metadata": {},
   "source": [
    "# Old from here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76c1f4",
   "metadata": {},
   "source": [
    "Another next step is to move the pipeline functions I have created to a separate file to tidy up the notebook. For now, please accept the following sequence of custom functions. They mimic the steps already performed above for the 1st Party classifier.\n",
    "\n",
    "The first function calls the different sub-functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcb51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_modelling_pipeline(classifier, model_results_series, sentence_filtering=True, inspect_flow=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    \n",
    "        classifier: name (exact string) of the target column to predict\n",
    "        \n",
    "        model_results_series:   the empty series to save the results to. \n",
    "                                A pickle file of the same name will be saved with the results. \n",
    "        \n",
    "        sentence_filtering:     sentence_filtering takes a boolean, default True. \n",
    "                                If False, the flow will ommit the sentence filtering step.\n",
    "        \n",
    "        inspect_flow: Passing inspect_flow=True will print out the shape of dataframes moving through the flow \n",
    "    \"\"\"\n",
    "#     if sentence_filtering == False:\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    # Step 1 – declare the classifier\n",
    "    \n",
    "    print(f\"Running for classifier: {classifier}\")\n",
    "    start_code_time = time.time()\n",
    "    \n",
    "    # Step 2 – execute sentence filtering on the data to use\n",
    "    \n",
    "    clean_annotation_features = pd.read_pickle(\"objects/clean_annotation_features.pkl\")\n",
    "    \n",
    "    if sentence_filtering == True:\n",
    "        df_for_pipelining_train_SF = model_pipeline_step_2(classifier, clean_annotation_features)\n",
    "    elif sentence_filtering == False:\n",
    "        df_for_pipelining_train_SF = df_for_pipelining_train.copy()\n",
    "        df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    else:\n",
    "        raise Sentence_Filtering_Not_Boolean_Error\n",
    "    \n",
    "    if inspect_flow == True: \n",
    "        print(f\"Shape of {classifier} train df after sentence filtering is: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    # Step 3 – create the X and y data\n",
    "    \n",
    "    classifier_X_train, tfidfTransformer = model_pipeline_step_3_1(df_for_pipelining_train_SF)\n",
    "    \n",
    "    classifier_y_train = model_pipeline_step_3_2(classifier, df_for_pipelining_train_SF)\n",
    "    \n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_train (made of CFs plus tf-idf matrix): {classifier_X_train.shape}\")\n",
    "        print(f\"classifier_y_train: {classifier_y_train.shape}\")\n",
    "        print(\"classifier_y_train value counts:\")\n",
    "        print(classifier_y_train.value_counts())\n",
    "    \n",
    "    # Step 4 – run CV Grid Search\n",
    "    \n",
    "    fitted_search = model_pipeline_step_4(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    # Step 5 – run the best model on the validation set and save the results\n",
    "    \n",
    "    classifier_X_val, classifier_y_val = model_pipeline_step_5_1(classifier, df_for_pipelining_val, tfidfTransformer)\n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_val: {classifier_X_val.shape}\")\n",
    "        print(f\"classifier_y_val: {classifier_y_val.shape}\")\n",
    "    \n",
    "    model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val, model_results_series)\n",
    "    \n",
    "    # Tests\n",
    "    if type(model_results_series[classifier]) == int:\n",
    "        print(\"Model results not saved.\")\n",
    "        raise NotSavedError(\"Check model results\")\n",
    "    \n",
    "    print(f\"The runtime for {classifier} was {round(time.time() - start_code_time, 5)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6da5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_2(classifier, clean_annotation_features):\n",
    "    \"\"\"\n",
    "    Step 2 – Get crafted features for classifier to use for sentence filtering.\n",
    "    \n",
    "    Inputs: classifier name, annotation features\n",
    "    \n",
    "    Outputs: The train dataframe (containing the segment text, classifiers and features)\n",
    "        with rows filtered out that don't contain segments featuring a related feature (\"sentence filtering applied\").\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtering the annotations & features table to get the list object from the same row that lists the classifier:\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ] \\\n",
    "                            .reset_index().at[0,'features']\n",
    "    \n",
    "    # Filter the DF for rows where any of those features is 1:\n",
    "    df_for_pipelining_train_SF = df_for_pipelining_train[\n",
    "        ( (df_for_pipelining_train[classifier_features] > 0)\n",
    "         .sum(axis=1) > 0 )]\n",
    "    df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return df_for_pipelining_train_SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d36c429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_1(df_for_pipelining_train_SF):\n",
    "    \"\"\"\n",
    "    Step 3.1 – get the training data X values\n",
    "    \n",
    "    Inputs:  The train dataframe with sentence filtering applied\n",
    "    \n",
    "    Outputs: \n",
    "        - The training data X values to be passed into the models (crafted features & tf-idf matrix)\n",
    "        - The tf-idf transformer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate into X\n",
    "    \n",
    "    tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "    train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])\n",
    "    \n",
    "    # Extract crafted features columns:\n",
    "    \n",
    "    classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy() \n",
    "        # Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "    \n",
    "    if classifier_X_train_cfs.shape[1] != 476:\n",
    "        print(f\"Should be left with the 476 crafted features (CF). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "        raise Step_3_CF_error(\"Crafted features not being applied correctly\")\n",
    "\n",
    "    # Convert to sparse\n",
    "    classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "    # Combine CF columns with TF-IDF to create X\n",
    "    classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])\n",
    "    \n",
    "    return classifier_X_train, tfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83018c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_2(classifier, df_for_pipelining_train_SF):\n",
    "    \"\"\"\n",
    "    Step 3.2 – Get the training data y values\n",
    "    \n",
    "    Inputs:\n",
    "        - Classifier name\n",
    "        - The train dataframe with sentence filtering applied\n",
    "    \n",
    "    Outpus:\n",
    "        The dataframe filtered to only the column with the classifier (the y values to be passed into the models).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate into y\n",
    "    \n",
    "    classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()\n",
    "    \n",
    "    if classifier_y_train.max() != 1:\n",
    "        print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\")\n",
    "        raise Step_3_y_error(\"train target colum not binary\")\n",
    "    \n",
    "    return classifier_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a200e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_4(classifier_X_train, classifier_y_train):\n",
    "    \"\"\"\n",
    "    Step 4 – Run a CV Grid Search\n",
    "    \n",
    "    Inputs: \n",
    "        - The X data to be passed into the models\n",
    "        - The y data to be passed into the models\n",
    "    \n",
    "    Outputs: A fitted search object corresponding to the best model for the classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Grid Search\n",
    "    \n",
    "    cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "    pipeline_sequences = [\n",
    "            ('SVC', SVC(kernel='linear', class_weight='balanced', random_state=1)) ]\n",
    "    pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "    svc_params = {'SVC__C': [0.1, 1, 10]}\n",
    "\n",
    "    # Create grid search object\n",
    "    grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=0, n_jobs=-1, scoring='f1')\n",
    "    \n",
    "    fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    return fitted_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b2b8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_1(classifier, df_for_pipelining_val, tfidfTransformer):\n",
    "    \"\"\"\n",
    "    Step 5.1 – Prepare the validation data\n",
    "    \n",
    "    Inputs:\n",
    "        - The classifier name\n",
    "        - The validation dataframe\n",
    "        - The tf-idf transformer that was fit to the training data\n",
    "    \n",
    "    Ouptuts:\n",
    "        - The X data (from the validation dataset) to run the model against\n",
    "        - The y data (from the validation dataset) to run the model against\n",
    "    \"\"\"\n",
    "    \n",
    "    # create validate X and y\n",
    "\n",
    "    val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "    \n",
    "    # Extract CF columns:\n",
    "    classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "    \n",
    "    # convert to sparse:\n",
    "    classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X:\n",
    "    classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "    classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "\n",
    "    # Ensure Y_val only has binary values\n",
    "    if classifier_y_val.isin([0, 1]).all().all() != True:\n",
    "        raise Step_5_val_error(\"Validation target column not binary\")\n",
    "    \n",
    "    return classifier_X_val, classifier_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b187f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val, model_results_series):\n",
    "    \"\"\"\n",
    "    Step 5.2 – Put the model through the validation data to get results for this classifier.\n",
    "    \n",
    "    Inputs:\n",
    "        - Classifier name\n",
    "        - The fitted search object corresponding to the best model for the classifier\n",
    "        - The X data (from the validation dataset) to run the model against\n",
    "        - The y data (from the validation dataset) to run the model against\n",
    "        - A series indexed by classifier name to store the model results to.\n",
    "    \n",
    "    Outputs: None. \n",
    "    \n",
    "    Actions: Saves the results to the model_results_series. It saves a list with 3 values:\n",
    "        - the fitted search object\n",
    "        - the y values in the validation dataset\n",
    "        - the y values predicted by the model    \n",
    "    \"\"\"\n",
    "    \n",
    "    # scoring\n",
    "    classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "    # save the model to a series\n",
    "    model_results_series[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "    \n",
    "    # save the series to disk to prevent data loss in case of crash\n",
    "    model_results_series.to_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4388b0",
   "metadata": {},
   "source": [
    "# Run all classifiers through the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df049c4",
   "metadata": {},
   "source": [
    "Creating a series to store the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ba8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_results_sf = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9ce82",
   "metadata": {},
   "source": [
    "Pass every different classifier into the modelling pipeline:\n",
    "\n",
    "**! This takes a few minutes to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4e25149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Shape of Contact train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7940\n",
      "1     128\n",
      "Name: Contact, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact was 9.63536\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Shape of Contact_E_Mail_Address train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7406\n",
      "1     662\n",
      "Name: Contact_E_Mail_Address, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_E_Mail_Address was 25.06858\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Shape of Contact_Phone_Number train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7722\n",
      "1     346\n",
      "Name: Contact_Phone_Number, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_Phone_Number was 18.32602\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Shape of Identifier_Cookie_or_similar_Tech train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7472\n",
      "1     596\n",
      "Name: Identifier_Cookie_or_similar_Tech, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 14.05302\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Shape of Identifier_Device_ID train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7736\n",
      "1     332\n",
      "Name: Identifier_Device_ID, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Device_ID was 14.68286\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Shape of Identifier_IMEI train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    8018\n",
      "1      50\n",
      "Name: Identifier_IMEI, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_IMEI was 4.26225\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Shape of Identifier_MAC train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7983\n",
      "1      85\n",
      "Name: Identifier_MAC, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_MAC was 5.2072\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Shape of Identifier_Mobile_Carrier train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    8008\n",
      "1      60\n",
      "Name: Identifier_Mobile_Carrier, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Mobile_Carrier was 7.05231\n",
      "\n",
      "Running for classifier: Location\n",
      "Shape of Location train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7531\n",
      "1     537\n",
      "Name: Location, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location was 16.42117\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Shape of Location_Cell_Tower train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7979\n",
      "1      89\n",
      "Name: Location_Cell_Tower, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_Cell_Tower was 7.57836\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Shape of Location_GPS train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7926\n",
      "1     142\n",
      "Name: Location_GPS, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_GPS was 10.35614\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Shape of Location_WiFi train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7942\n",
      "1     126\n",
      "Name: Location_WiFi, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_WiFi was 10.26218\n",
      "\n",
      "Running for classifier: SSO\n",
      "Shape of SSO train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7922\n",
      "1     146\n",
      "Name: SSO, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for SSO was 11.33459\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Shape of Facebook_SSO train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7959\n",
      "1     109\n",
      "Name: Facebook_SSO, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Facebook_SSO was 5.6685\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Shape of 1st_party train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    6235\n",
      "1    1833\n",
      "Name: 1st_party, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 1st_party was 33.84027\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Shape of 3rd_party train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7538\n",
      "1     530\n",
      "Name: 3rd_party, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 3rd_party was 22.37898\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Shape of PERFORMED train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    6387\n",
      "1    1681\n",
      "Name: PERFORMED, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for PERFORMED was 38.30317\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Shape of NOT_PERFORMED train df after sentence filtering is: (8068, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (8068, 113262)\n",
      "classifier_y_train: (8068,)\n",
      "classifier_y_train value counts:\n",
      "0    7517\n",
      "1     551\n",
      "Name: NOT_PERFORMED, dtype: int64\n",
      "classifier_X_val: (2651, 113262)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for NOT_PERFORMED was 16.90295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    \n",
    "    full_modelling_pipeline(each_classifier, model_results_series=initial_results_sf, \n",
    "                            sentence_filtering=False, inspect_flow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f475edc9",
   "metadata": {},
   "source": [
    "The results for any classifier can then be retrieved from the results series. The series stores the actual and predicted y-values in the form `[model_object, true_y, predicted_y]`\n",
    "\n",
    "Here is an example classification report for one of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d712ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.55      0.67      1974\n",
      "           1       0.36      0.75      0.49       677\n",
      "\n",
      "    accuracy                           0.60      2651\n",
      "   macro avg       0.61      0.65      0.58      2651\n",
      "weighted avg       0.74      0.60      0.62      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(initial_results_sf['Contact_E_Mail_Address'][1] , initial_results_sf['Contact_E_Mail_Address'][2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd79937",
   "metadata": {},
   "source": [
    "# Creating model evaluation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9818721",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model_results = pd.read_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4200d0b6",
   "metadata": {},
   "source": [
    "I will display the results for all classifiers in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b479ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the classifiers as the index for our model results df\n",
    "initial_f1s = pd.DataFrame(initial_model_results, columns=[\"Neg F1\"]).copy()\n",
    "\n",
    "# then populate the dataframe with F1 scores by reference to the 'initial_model_results' series.\n",
    "# the initial_model_results series stores the actual and predicted y-values in the form [model_object, true_y, predicted_y]\n",
    "for index in initial_f1s.index:\n",
    "    initial_f1s.loc[index, \"Neg F1\"] = \\\n",
    "        f1_score(initial_model_results[index][1].copy(), initial_model_results[index][2].copy(), pos_label=0)\n",
    "    initial_f1s.loc[index, \"Pos F1\"] = \\\n",
    "        f1_score(initial_model_results[index][1].copy(), initial_model_results[index][2].copy(), pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e4e3d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Pos F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>0.992875</td>\n",
       "      <td>0.660550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>0.986464</td>\n",
       "      <td>0.845070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>0.991876</td>\n",
       "      <td>0.839216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>0.990924</td>\n",
       "      <td>0.903084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>0.993328</td>\n",
       "      <td>0.834951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>0.99962</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>0.998289</td>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>0.996014</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>0.98595</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>0.995615</td>\n",
       "      <td>0.596491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>0.996728</td>\n",
       "      <td>0.841121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>0.996352</td>\n",
       "      <td>0.795699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>0.995004</td>\n",
       "      <td>0.734694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>0.997703</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>0.950586</td>\n",
       "      <td>0.859012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>0.972808</td>\n",
       "      <td>0.641711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>0.945706</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>0.982334</td>\n",
       "      <td>0.801843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Neg F1    Pos F1\n",
       "Contact                            0.992875  0.660550\n",
       "Contact_E_Mail_Address             0.986464  0.845070\n",
       "Contact_Phone_Number               0.991876  0.839216\n",
       "Identifier_Cookie_or_similar_Tech  0.990924  0.903084\n",
       "Identifier_Device_ID               0.993328  0.834951\n",
       "Identifier_IMEI                     0.99962  0.937500\n",
       "Identifier_MAC                     0.998289  0.790698\n",
       "Identifier_Mobile_Carrier          0.996014  0.363636\n",
       "Location                            0.98595  0.823529\n",
       "Location_Cell_Tower                0.995615  0.596491\n",
       "Location_GPS                       0.996728  0.841121\n",
       "Location_WiFi                      0.996352  0.795699\n",
       "SSO                                0.995004  0.734694\n",
       "Facebook_SSO                       0.997703  0.846154\n",
       "1st_party                          0.950586  0.859012\n",
       "3rd_party                          0.972808  0.641711\n",
       "PERFORMED                          0.945706  0.824000\n",
       "NOT_PERFORMED                      0.982334  0.801843"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10084084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative F1 mean: 0.98712093292967\n",
      "Positive F1 mean: 0.7743867067641877\n"
     ]
    }
   ],
   "source": [
    "print(f\"Negative F1 mean: {initial_f1s['Neg F1'].mean()}\")\n",
    "print(f\"Positive F1 mean: {initial_f1s['Pos F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ebc42",
   "metadata": {},
   "source": [
    "Many things to discuss based on the above:\n",
    "- The two lowest scores\n",
    "- A wide range of scores\n",
    "- Neg F1 tends to be higher than Pos F1\n",
    "- Compare with results from the paper\n",
    "- Equal results for the two SSO classifiers\n",
    "\n",
    "\n",
    "# Discussion of low F1 scores\n",
    "There is one zero for negative F1 `Identifier_IMEI` Let's start by investigating `Identifier_IMEI`. \n",
    "\n",
    "A segment is annotated with 'Identifier_IMEI' if the segment describes how the company uses IMEI data to identify a customer. IMEI (International Mobile Equipment Identity) is a unique identification number all phone devices have, and can be used to track the history of the handset (including checking whether the phone has ever been reported as stolen).  First let's look at the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cb8ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>4</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                   2                1972\n",
       "True Positive                   4                 673"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Identifier_IMEI'][1].copy(), initial_model_results['Identifier_IMEI'][2].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306af2e",
   "metadata": {},
   "source": [
    "The classifier almost never predicted that this label is not present.  This is likely because in the subset of the data that the classifier was trained on, all of the observations were positive cases. This will be the result of the sentence filtering – the classifier was only trained using text segments that mentioned phrases related to IMEI, all of which happened to be annotated as indicating that the practice was described.\n",
    "\n",
    "This could mean that a rule-based classifier would be suitable for classifying the IMEI practice.  For a model-based classifier, I would need to use a wider range of features for sentence filtering, or would not conduct sentence filtering.\n",
    "\n",
    "We can see the features used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e434a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['imei', 'international mobile equipment', 'equipment id'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_annotation_features[clean_annotation_features['annotation']=='Identifier_IMEI']['features'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d021f753",
   "metadata": {},
   "source": [
    "As expected there are very few related phrases, all of which are somewhat technical.\n",
    "\n",
    "Another implication could be that because IMEI is so specific, companies will not mention anything related to it in their privacy policies unless they collect this data.\n",
    "\n",
    "**Location Cell Tower**\n",
    "\n",
    "I think a similar problem has occurred here. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1efa8074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>142</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>31</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                 142                1832\n",
       "True Positive                  31                 646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Location_Cell_Tower'][1].copy(), initial_model_results['Location_Cell_Tower'][2].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae6c5d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['base station', 'cell tower', 'cell id', 'network-based', 'network based', 'cellular network location', 'approximate location', 'approximate geo', 'approximate device location', 'approximate device geo', 'approximate a location', 'coarse location', 'coarse geo', 'coarse device location', 'coarse device geo', 'rough location', 'rough geo', 'rough device location', 'rough device geo', 'imprecise location', 'imprecise geo', 'imprecise device location', 'imprecise device geo', 'located with less precision', 'non-precise location', 'non-precise geo', 'non-precise device location', 'non-precise device geo', 'non-specific location', 'non-specific geo', 'non-specific device location', 'not-specific device geo', 'not-specific location', 'not-specific geo', 'not-specific device location', 'not-specific device geo', 'general location', 'general geo', 'general device location', 'general device geo', 'generally where you are located', 'based on proximity'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_annotation_features[clean_annotation_features['annotation']=='Location_Cell_Tower']['features'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319dff03",
   "metadata": {},
   "source": [
    "It looks like there is a similar problem with most of the training data probably having been positive cases, but the cause is less clear because there are a large number of different related crafted features.  By recollection from manually inspecting the frequencies of different crafted features, most of those related to location had surprisingly low frequency in the text because policies all tended to use some exact phrasing for location. As we can tell, the above crafted features are all around imprecise location.\n",
    "\n",
    "**Training this many classifiers**\n",
    "\n",
    "Given the large number of different classifiers being trained, there is a chance that at least one of them is poorly represented in the training set by chance of the random split, or other random factors in the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f89c99",
   "metadata": {},
   "source": [
    "## A wide range of scores\n",
    "\n",
    "With training this many classifiers a wide range of scores is expected, but the targets with the most data (the parties and modalities) have the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb61c7e",
   "metadata": {},
   "source": [
    "## Neg F1 tends to be higher than Pos F1\n",
    "\n",
    "This is partially likely due to imbalance in the data, which sentence filtering was supposed to help with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b1bef",
   "metadata": {},
   "source": [
    "## Compare with results from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8c304",
   "metadata": {},
   "source": [
    "My results are not particularly similar to those listed in the table on page 4.  Most notably the baseline models for Story et al. hardly ended up with any classifiers with low scores, so it's possible that the different ways that they manipulated the data had a significant effect.  The main differences were where in the process they applied tf-idf and how they did sentence filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244616ed",
   "metadata": {},
   "source": [
    "## Matching scores for SSO and FacebookSSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5f051",
   "metadata": {},
   "source": [
    "The SSO (\"Single Sign On\") annotation is applied to a segment if it discusses what the company does with the data used to facilitate a single sign on service, such as when you sign into an app through your google or facebook account.  This is always passed to the appropriate third party to enact the service. The scores for both \"SSO\" and \"Facebook SSO\" are the same because the crafted features and most of the annotations are the same.  Again let's start by looking at the confusion matrix for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df2456bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1967</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>592</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1967                   7\n",
       "True Positive                 592                  85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['SSO'][1].copy(), initial_model_results['SSO'][2].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d143e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1967</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>592</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1967                   7\n",
       "True Positive                 592                  85"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(initial_model_results['Facebook_SSO'][1].copy(), initial_model_results['Facebook_SSO'][2].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d016e6",
   "metadata": {},
   "source": [
    "The proportions of both these annotations must be the same in the validation set. I think this could be due to an issue with my data manipulation as I expect there to be some difference.\n",
    "\n",
    "This time the model very rarely predicted that a segment contained a practice relating to SSO, most likely because there is such great class imbalance in the training data (not the validation data) even after applying sentence filtering. This has caused our model to be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1a21008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7922\n",
       "1     146\n",
       "Name: SSO, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_pipelining_train['SSO'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8076c",
   "metadata": {},
   "source": [
    "Yes, the proportions are hugely different in the training data and very imbalanced. For this classifier it definitely looks like we got unlucky with the train/validate split (assuming it was random)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d978f00",
   "metadata": {},
   "source": [
    "# Final model performance on test set\n",
    "\n",
    "Now I will train all classifiers using the Train and Validate data sets combined and test the score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81016d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a4e82",
   "metadata": {},
   "source": [
    "I am yet to refactor the code to smoothly adapt my functions to work on the test set so I will have to re-assign the targets of the input variables to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "baf170b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"objects/crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[(df_for_pipelining['policy_type'] == 'TRAINING') | (df_for_pipelining['policy_type'] == 'VALIDATION')].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "df_for_pipelining_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other unneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)\n",
    "\n",
    "df_for_pipelining_val = df_for_pipelining_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298d22",
   "metadata": {},
   "source": [
    "Running modelling pipeline for all classifiers\n",
    "\n",
    "**! This could take some time to run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67f30b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Shape of Contact train df after sentence filtering is: (480, 511)\n",
      "df_for_pipelining_train_SF: (480, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (480, 16471)\n",
      "classifier_y_train: (480,)\n",
      "classifier_X_val: (4824, 16471)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact was 1.25053\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Shape of Contact_E_Mail_Address train df after sentence filtering is: (731, 511)\n",
      "df_for_pipelining_train_SF: (731, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (731, 20188)\n",
      "classifier_y_train: (731,)\n",
      "classifier_X_val: (4824, 20188)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact_E_Mail_Address was 1.54034\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Shape of Contact_Phone_Number train df after sentence filtering is: (714, 511)\n",
      "df_for_pipelining_train_SF: (714, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (714, 22018)\n",
      "classifier_y_train: (714,)\n",
      "classifier_X_val: (4824, 22018)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Contact_Phone_Number was 1.82173\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Shape of Identifier_Cookie_or_similar_Tech train df after sentence filtering is: (921, 511)\n",
      "df_for_pipelining_train_SF: (921, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (921, 23175)\n",
      "classifier_y_train: (921,)\n",
      "classifier_X_val: (4824, 23175)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 2.84204\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Shape of Identifier_Device_ID train df after sentence filtering is: (356, 511)\n",
      "df_for_pipelining_train_SF: (356, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (356, 10816)\n",
      "classifier_y_train: (356,)\n",
      "classifier_X_val: (4824, 10816)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Device_ID was 0.94658\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Shape of Identifier_IMEI train df after sentence filtering is: (59, 511)\n",
      "df_for_pipelining_train_SF: (59, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (59, 2401)\n",
      "classifier_y_train: (59,)\n",
      "classifier_X_val: (4824, 2401)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_IMEI was 0.41278\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Shape of Identifier_MAC train df after sentence filtering is: (169, 511)\n",
      "df_for_pipelining_train_SF: (169, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (169, 9085)\n",
      "classifier_y_train: (169,)\n",
      "classifier_X_val: (4824, 9085)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_MAC was 0.67886\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Shape of Identifier_Mobile_Carrier train df after sentence filtering is: (102, 511)\n",
      "df_for_pipelining_train_SF: (102, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (102, 5617)\n",
      "classifier_y_train: (102,)\n",
      "classifier_X_val: (4824, 5617)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Identifier_Mobile_Carrier was 0.57555\n",
      "\n",
      "Running for classifier: Location\n",
      "Shape of Location train df after sentence filtering is: (920, 511)\n",
      "df_for_pipelining_train_SF: (920, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (920, 25911)\n",
      "classifier_y_train: (920,)\n",
      "classifier_X_val: (4824, 25911)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location was 2.3968\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Shape of Location_Cell_Tower train df after sentence filtering is: (105, 511)\n",
      "df_for_pipelining_train_SF: (105, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (105, 3973)\n",
      "classifier_y_train: (105,)\n",
      "classifier_X_val: (4824, 3973)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_Cell_Tower was 0.44674\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Shape of Location_GPS train df after sentence filtering is: (184, 511)\n",
      "df_for_pipelining_train_SF: (184, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (184, 7304)\n",
      "classifier_y_train: (184,)\n",
      "classifier_X_val: (4824, 7304)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_GPS was 0.65056\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Shape of Location_WiFi train df after sentence filtering is: (137, 511)\n",
      "df_for_pipelining_train_SF: (137, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (137, 6154)\n",
      "classifier_y_train: (137,)\n",
      "classifier_X_val: (4824, 6154)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Location_WiFi was 0.57327\n",
      "\n",
      "Running for classifier: SSO\n",
      "Shape of SSO train df after sentence filtering is: (31, 511)\n",
      "df_for_pipelining_train_SF: (31, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (31, 1671)\n",
      "classifier_y_train: (31,)\n",
      "classifier_X_val: (4824, 1671)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for SSO was 0.38418\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Shape of Facebook_SSO train df after sentence filtering is: (31, 511)\n",
      "df_for_pipelining_train_SF: (31, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (31, 1671)\n",
      "classifier_y_train: (31,)\n",
      "classifier_X_val: (4824, 1671)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for Facebook_SSO was 0.38512\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Shape of 1st_party train df after sentence filtering is: (9730, 511)\n",
      "df_for_pipelining_train_SF: (9730, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (9730, 135540)\n",
      "classifier_y_train: (9730,)\n",
      "classifier_X_val: (4824, 135540)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for 1st_party was 61.20125\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Shape of 3rd_party train df after sentence filtering is: (5550, 511)\n",
      "df_for_pipelining_train_SF: (5550, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (5550, 94818)\n",
      "classifier_y_train: (5550,)\n",
      "classifier_X_val: (4824, 94818)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for 3rd_party was 27.92781\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Shape of PERFORMED train df after sentence filtering is: (9037, 511)\n",
      "df_for_pipelining_train_SF: (9037, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (9037, 129163)\n",
      "classifier_y_train: (9037,)\n",
      "classifier_X_val: (4824, 129163)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for PERFORMED was 54.41476\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Shape of NOT_PERFORMED train df after sentence filtering is: (4563, 511)\n",
      "df_for_pipelining_train_SF: (4563, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (4563, 86310)\n",
      "classifier_y_train: (4563,)\n",
      "classifier_X_val: (4824, 86310)\n",
      "classifier_y_val: (4824,)\n",
      "The runtime for NOT_PERFORMED was 18.45924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    full_modelling_pipeline(each_classifier, model_results_series=final_results, \n",
    "                            sentence_filtering=True, inspect_flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3437a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_results = pd.read_pickle(\"objects/most_recent_model_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc68380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Pos F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contact</th>\n",
       "      <td>0.91178</td>\n",
       "      <td>0.633547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_E_Mail_Address</th>\n",
       "      <td>0.386906</td>\n",
       "      <td>0.320734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contact_Phone_Number</th>\n",
       "      <td>0.81761</td>\n",
       "      <td>0.518854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Cookie_or_similar_Tech</th>\n",
       "      <td>0.728218</td>\n",
       "      <td>0.427560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Device_ID</th>\n",
       "      <td>0.566071</td>\n",
       "      <td>0.371892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_IMEI</th>\n",
       "      <td>0.004063</td>\n",
       "      <td>0.313135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_MAC</th>\n",
       "      <td>0.915175</td>\n",
       "      <td>0.556488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identifier_Mobile_Carrier</th>\n",
       "      <td>0.472029</td>\n",
       "      <td>0.353999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>0.741646</td>\n",
       "      <td>0.475503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_Cell_Tower</th>\n",
       "      <td>0.535186</td>\n",
       "      <td>0.372625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_GPS</th>\n",
       "      <td>0.786771</td>\n",
       "      <td>0.466231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location_WiFi</th>\n",
       "      <td>0.442146</td>\n",
       "      <td>0.342367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSO</th>\n",
       "      <td>0.908371</td>\n",
       "      <td>0.256846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Facebook_SSO</th>\n",
       "      <td>0.908371</td>\n",
       "      <td>0.256846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1st_party</th>\n",
       "      <td>0.962497</td>\n",
       "      <td>0.843179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3rd_party</th>\n",
       "      <td>0.95811</td>\n",
       "      <td>0.819582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERFORMED</th>\n",
       "      <td>0.96184</td>\n",
       "      <td>0.840751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOT_PERFORMED</th>\n",
       "      <td>0.957721</td>\n",
       "      <td>0.818032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Neg F1    Pos F1\n",
       "Contact                             0.91178  0.633547\n",
       "Contact_E_Mail_Address             0.386906  0.320734\n",
       "Contact_Phone_Number                0.81761  0.518854\n",
       "Identifier_Cookie_or_similar_Tech  0.728218  0.427560\n",
       "Identifier_Device_ID               0.566071  0.371892\n",
       "Identifier_IMEI                    0.004063  0.313135\n",
       "Identifier_MAC                     0.915175  0.556488\n",
       "Identifier_Mobile_Carrier          0.472029  0.353999\n",
       "Location                           0.741646  0.475503\n",
       "Location_Cell_Tower                0.535186  0.372625\n",
       "Location_GPS                       0.786771  0.466231\n",
       "Location_WiFi                      0.442146  0.342367\n",
       "SSO                                0.908371  0.256846\n",
       "Facebook_SSO                       0.908371  0.256846\n",
       "1st_party                          0.962497  0.843179\n",
       "3rd_party                           0.95811  0.819582\n",
       "PERFORMED                           0.96184  0.840751\n",
       "NOT_PERFORMED                      0.957721  0.818032"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative F1 mean: 0.7202505215060437\n",
      "Positive F1 mean: 0.4993427790530155\n"
     ]
    }
   ],
   "source": [
    "# take the classifiers as the index for our model results df\n",
    "final_f1s = pd.DataFrame(final_model_results, columns=[\"Neg F1\"]).copy()\n",
    "\n",
    "# then populate the dataframe with F1 scores by reference to the 'initial_model_results' series.\n",
    "# the initial_model_results series stores the actual and predicted y-values in the form [model_object, true_y, predicted_y]\n",
    "for index in final_f1s.index:\n",
    "    final_f1s.loc[index, \"Neg F1\"] = f1_score(final_model_results[index][1].copy(), final_model_results[index][2].copy(), pos_label=0)\n",
    "    final_f1s.loc[index, \"Pos F1\"] = f1_score(final_model_results[index][1].copy(), final_model_results[index][2].copy(), pos_label=1)\n",
    "display(final_f1s)\n",
    "print(f\"Negative F1 mean: {final_f1s['Neg F1'].mean()}\")\n",
    "print(f\"Positive F1 mean: {final_f1s['Pos F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa691d98",
   "metadata": {},
   "source": [
    "Interestingly the score for Contact_E_Mail_Address has gone down significantly. It would be very interesting to explore why this might be.\n",
    "\n",
    "The scores for SSO have only increased by a small amount despite the class imbalance issue potentially being solved by using the train and validate data.\n",
    "\n",
    "The mean F1 scores have remained largely the same and are still much lower for Positive F1.  I feel that this is still due to such imbalance across the dataset but would like to explore more.\n",
    "\n",
    "One conclusion is that by increasing the amount of data being used, the model performance has not really increased, meaning that the limiting factors are probably in the model creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf36bf",
   "metadata": {},
   "source": [
    "# Further work\n",
    "\n",
    "A next step is to implement an ability of the modelling pipeline to remove sentence filtering from the flow.  It is causing some issues in my model scores and Story et al. found that some classifiers' performance improves without it.\n",
    "\n",
    "I would then work on other preprocessing steps:\n",
    "- I have not applied scaling to the data. Every column is between 0 and 1 anyway so the effect would be small but it could have big effect for some classifiers.\n",
    "- tf-idf: \n",
    "    - I would explore training classifiers without bigrams as Story et al. also explored this and found it to be helpful for a small number of classifiers \n",
    "    - Fixing data leak: currently, the tf-idf transformer is fitted and applied to the entire train set before cross-validation.\n",
    "\n",
    "This leads on to a more general issue that I have created a series of functions but none of my own classes, so I would refactor into a more object-oriented framework.\n",
    "\n",
    "Other types of models:\n",
    "- Naive bayes classifiers and ensemble models such as random forest have also shown to perform well in the NLP domain and I would wish to see the performance of these models too.\n",
    "- Neural Network architecture is also likely to get very strong performance, although may require more investment and training resources.\n",
    "\n",
    "Deeper exploration into the effect of different models in this domain\n",
    "\n",
    "- It would be insightful to explore why different preprocessing steps or why different models have different effects on different classifiers when trained on this dataset\n",
    "\n",
    "**Thanks for reading!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7038d04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f5a35",
   "metadata": {},
   "source": [
    "# OLD: TF-IDF matrix (old content)\n",
    "The input data to the model, X, requires a union of the Crafted Features columns and the TF-IDF matrix.\n",
    "\n",
    "Create TF-IDF matrix, fit it to the text data, and use it to transform it to create the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate transformer object\n",
    "tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "# fit the transformer and transform the data\n",
    "train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a23f3f",
   "metadata": {},
   "source": [
    "This tf-idf matrix is stored in a sparse matrix format.\n",
    "\n",
    "Extract crafted features columns from X and convert it to sparse format so that it can be combined with TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be left with the 476 different crafted features (CFs). CF shape is: (7297, 476)\n"
     ]
    }
   ],
   "source": [
    "# Extract CF columns:\n",
    "classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy() \n",
    "# the CF columns happen to be all those after and including 'contact info', so I \n",
    "# use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "print(f\"Should be left with the 476 different crafted features (CFs). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "\n",
    "#convert to sparse\n",
    "classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf]) \n",
    "    # hstack combines the matricies horizontally (more columns, same number of rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1026263",
   "metadata": {},
   "source": [
    "## Create y (old content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709d38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbcde5",
   "metadata": {},
   "source": [
    "Now we have our input data and target ready to pass into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb83b",
   "metadata": {},
   "source": [
    "# Step 2: apply Sentence Filtering (old content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc013b2d",
   "metadata": {},
   "source": [
    "Retrieving the crafted features for 1st Party to use for sentence filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d283eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'you', 'us', 'our', 'the app', 'the software']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering the table to get the list object from the same row that lists the classifier\n",
    "classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ]     \\\n",
    "                        .reset_index().at[0,'features']\n",
    "\n",
    "classifier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f69f62",
   "metadata": {},
   "source": [
    "Filter the dataframe for rows where any of those features is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7903b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original dataframe's shape was (8068, 511)\n",
      "The dataframe after sentence filtering is (7297, 511)\n"
     ]
    }
   ],
   "source": [
    "df_for_pipelining_train_SF = df_for_pipelining_train[\n",
    "    ((df_for_pipelining_train[classifier_features] > 0)\n",
    "    .sum(axis=1) > 0 )]\n",
    "df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"The original dataframe's shape was {df_for_pipelining_train.shape}\")\n",
    "print(f\"The dataframe after sentence filtering is {df_for_pipelining_train_SF.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d125a",
   "metadata": {},
   "source": [
    "We have lost a big proportion of data but still have enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b0b32",
   "metadata": {},
   "source": [
    "# Evaluation on Validation dataset (Old content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936e811",
   "metadata": {},
   "source": [
    "I conduct similar preprocessing steps to prepare the validate set for prediction. I can ommit sentence filtering because it is only important for training the model, and the model is already trained.\n",
    "\n",
    "(Load the validation dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d14fe0",
   "metadata": {},
   "source": [
    "Create the tfid matrix by transforming the text data (using the transformer fit on the training data), combining this with the created features, and also extracting the target y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "787f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "\n",
    "# Extract CF columns:\n",
    "classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "# convert to sparse\n",
    "classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "# extracting the target y:\n",
    "classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfbd87f",
   "metadata": {},
   "source": [
    "Running the model.\n",
    "\n",
    "**Storing the model results** \n",
    "\n",
    "I create a series to store the results for each classifier I could make.\n",
    "\n",
    "For each model, I will create a list with 3 values: the fitted search object, the y values and the y predictions. Then each of those lists is stored in a series where the index is the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76cc44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper\n",
    "\n",
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7483e2",
   "metadata": {},
   "source": [
    "Running the model and saving the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddedd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model\n",
    "classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "# saving the model results for future use\n",
    "model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "model_results.to_pickle(\"objects/model_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcf0c7",
   "metadata": {},
   "source": [
    "Showing confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d913c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(classifier_y_val, classifier_val_prediction):\n",
    "    \"\"\"\n",
    "    Put the actual y values and the predicted y values into a dataframe with labels and display it\n",
    "    \"\"\"\n",
    "    cf_matrix = confusion_matrix(classifier_y_val, classifier_val_prediction)\n",
    "    cf_df = pd.DataFrame(\n",
    "        cf_matrix, columns=[\"Predicted Negative\", \"Predicted Positive\"], index=[\"True Negative\", \"True Positive\"])\n",
    "    display(cf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5c7b0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1865</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>85</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive\n",
       "True Negative                1865                 109\n",
       "True Positive                  85                 592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(classifier_y_val, classifier_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1c7fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1974\n",
      "           1       0.84      0.87      0.86       677\n",
      "\n",
      "    accuracy                           0.93      2651\n",
      "   macro avg       0.90      0.91      0.90      2651\n",
      "weighted avg       0.93      0.93      0.93      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(classifier_y_val, classifier_val_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
