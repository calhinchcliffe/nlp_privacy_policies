{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb32d037",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "\n",
    "Modelling pipeline: grid search over all classes.\n",
    "\n",
    "Then to make bigrams and sentence filtering optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "86a733d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import ds_utils_callum\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# modelling\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# modelling pipeline\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# modelling evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422a3a6",
   "metadata": {},
   "source": [
    "Future pipeline:\n",
    "\n",
    "For each classifier -><br>\n",
    "Separate to X and Y<br>\n",
    "TF-IDF here option 1 <br>\n",
    "Step for SF'ing<br>\n",
    "TF-IDF here option 2 <br>\n",
    "Split into folds (5-fold CV)<br>\n",
    "3x3 SVM Hyperparameters<br>\n",
    "Find best neg F1 score\n",
    "\n",
    "Plus anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0943e8a",
   "metadata": {},
   "source": [
    "Pipeline to make now:\n",
    "\n",
    "1. Separate into classifiers. For each classifier:\n",
    "2. Apply SF'd\n",
    "3. Separate into X and Y\n",
    "4. Crate TF-IDF Matrix\n",
    "4. Split each set into 5 folds\n",
    "5. Grid search over SVM Hyperparameters to optimise F1 score\n",
    "\n",
    "Output.\n",
    "\n",
    "This will be a moderate approximation for a replication of most of their work. Main missing element will be better text pre-processing to get better results from the CFs and SF'ing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc034a58",
   "metadata": {},
   "source": [
    "Do it for one classifier, then find how to generalise it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5d3a",
   "metadata": {},
   "source": [
    "Train, Validate and Test dataframes to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c1d24f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other uneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b34e9bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8068, 511)\n",
      "(2651, 511)\n",
      "(4824, 511)\n"
     ]
    }
   ],
   "source": [
    "print(df_for_pipelining_train.shape)\n",
    "print(df_for_pipelining_val.shape)\n",
    "print(df_for_pipelining_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2db48",
   "metadata": {},
   "source": [
    "# Step 1: select classifier\n",
    "\n",
    "Let's start with 1st Party as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "565514c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \"1st_party\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb83b",
   "metadata": {},
   "source": [
    "# Step 2: apply SF'ing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc013b2d",
   "metadata": {},
   "source": [
    "1. Get CFs for 1st Party to use for SF'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d283eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'you', 'us', 'our', 'the app', 'the software']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_annotation_features = pd.read_pickle(\"clean_annotation_features.pkl\")\n",
    "\n",
    "# filtering the table to get the list object from the same row that lists the classifier\n",
    "classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "\n",
    "classifier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6248b",
   "metadata": {},
   "source": [
    "2. Filter the DF for rows where any of those features is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1dce19d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8068, 511)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_pipelining_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7903b0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7297, 511)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "df_for_pipelining_train_SF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e3f6d",
   "metadata": {},
   "source": [
    "# Step 3: Separate into X and Y\n",
    "\n",
    "## Create X\n",
    "X requires a union of the Crafted Features columns and the TF-IDF matrix.\n",
    "\n",
    "Create TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0aa24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a23f3f",
   "metadata": {},
   "source": [
    "Extract CF columns from X_train and convert to sparse so that it can be combined with TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5b2d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be left with the 579 crafted features (CF). CF shape is: (7297, 476)\n"
     ]
    }
   ],
   "source": [
    "# Extract CF columns:\n",
    "classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy()\n",
    "# Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "print(f\"Should be left with the 476 different crafted features (CFs). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "\n",
    "#convert to sparse\n",
    "classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1026263",
   "metadata": {},
   "source": [
    "## Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "709d38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed6cef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value should be one. Highest value is: 1\n"
     ]
    }
   ],
   "source": [
    "# Ensure Y_train only has binary values\n",
    "for i in range(len(classifier_y_train)):\n",
    "    if classifier_y_train[i] > 1:\n",
    "        classifier_y_train[i] = 1\n",
    "print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\") # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927efab",
   "metadata": {},
   "source": [
    "# Step 4: 5-fold CV Grid Search over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "34da8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "pipeline_sequences = [\n",
    "        ('SVC', SVC()) ]\n",
    "pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "svc_params = {'SVC__C': [0.1, 1, 10],\n",
    "             'SVC__gamma': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# Create grid search object\n",
    "grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=1, n_jobs=-1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f1f9b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "CPU times: user 9.14 s, sys: 212 ms, total: 9.35 s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dc002",
   "metadata": {},
   "source": [
    "# Evaluation on Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab55e38",
   "metadata": {},
   "source": [
    "To compare to the per-classifier results given in the paper (Table 1 pg 4), I only need to look at F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2bd8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5504\n",
      "           1       1.00      1.00      1.00      1793\n",
      "\n",
      "    accuracy                           1.00      7297\n",
      "   macro avg       1.00      1.00      1.00      7297\n",
      "weighted avg       1.00      1.00      1.00      7297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_prediction = fitted_search.predict(classifier_X_train)\n",
    "print(classification_report(classifier_y_train, classifier_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0f22",
   "metadata": {},
   "source": [
    "Okay, I need to do my CV grid search on just the Train set, then evaluate performance using the Validate set, since it's massively overfitting on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b0b32",
   "metadata": {},
   "source": [
    "# Evaluation on Validate set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936e811",
   "metadata": {},
   "source": [
    "Pre-processing steps to prepare the validate set for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aa31bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "787f50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value should be one. Highest value is: 1\n"
     ]
    }
   ],
   "source": [
    "val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "# Extract CF columns:\n",
    "classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "#convert to sparse\n",
    "classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "# Ensure Y_val only has binary values\n",
    "for i in range(len(classifier_y_val)):\n",
    "    if classifier_y_val[i] > 1:\n",
    "        classifier_y_val[i] = 1\n",
    "print(f\"Highest value should be one. Highest value is: {classifier_y_val.max()}\") # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772055a5",
   "metadata": {},
   "source": [
    "Scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d1c7fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "model_results.to_pickle(\"model_results.pkl\")\n",
    "\n",
    "# print(classification_report(classifier_y_val, classifier_val_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf6285",
   "metadata": {},
   "source": [
    "Nice! Looks like this scored well.  Let's set up the pipeline for all the other classifiers and score them too. But I'm still not sure whether I want the positive F1 score or the negative F1 score.  I think that \"negative F-1 score\" is important because it relates to when a policy fails to mention an important practice.  We want to be sure that if a policy fails to mention it, the classifier correctly states that it is not mentioned.\n",
    "\n",
    "Okay so Negative Recall is the proportion of When it was not in, did it say that it was not in?<br>\n",
    "Negative Precision then is when it predicted that it wasn't in, how often was that the case?\n",
    "\n",
    "Positive recall is When it was in, what was the chance it was identified?  <br>Positive precision is When it was predicted to be in, what was the chance that it was in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d913c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(classifier_y_val, classifier_val_prediction)\n",
    "cf_df = pd.DataFrame(\n",
    "    cf_matrix, columns=[\"Predicted Negative\", \"Predicted Positive\"], index=[\"True Negative\", \"True Positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd8e0d",
   "metadata": {},
   "source": [
    "I think that I want to store negative precision, negative recall, negative F1 and positive F1.\n",
    "\n",
    "This seems like a lot of things to store for each classifier.\n",
    "\n",
    "I think I can just store the tuple of `(classifier_y_val, classifier_val_prediction)`\n",
    "\n",
    "Then from that I can extract and populate a bigger table if I want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226ef24",
   "metadata": {},
   "source": [
    "Could store as lists... for each model, have a list with 3 values: fitted search, classifier_y_val, classifier_val_prediction.  Then could store each of those lists in a series where the index is the classifier.\n",
    "\n",
    "Could store as dictionaries.  Each key is the classifier and each value is the list.\n",
    "\n",
    "Then could loop through to get matrix (table) of scores.\n",
    "\n",
    "In fact I think it will be helpful to have the order be the same as the order that I pass the classifiers, so I should use a series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1770a1",
   "metadata": {},
   "source": [
    "### Requirements for modelling pipeline:\n",
    "\n",
    "- List of all classifiers\n",
    "- df_for_pipelining_train/val/test\n",
    "- Empty table of classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "77316778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_val.reset_index(inplace=True, drop=True)\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "df_for_pipelining_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other uneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9c742",
   "metadata": {},
   "source": [
    "List of all classifiers can be taken from any of the previous dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1cd1fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5bf524f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fcb51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_modelling_pipeline(classifier, inspect_flow=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Passing inspect_flow=True will print out the shape of dataframes moving through the flow \n",
    "    \"\"\"\n",
    "    \n",
    "    # step 1\n",
    "    print(f\"Running for classifier: {classifier}\")\n",
    "    start_code_time = time.time()\n",
    "    \n",
    "    # step 2:\n",
    "    clean_annotation_features = pd.read_pickle(\"clean_annotation_features.pkl\")\n",
    "    df_for_pipelining_train_SF = model_pipeline_step_2(classifier, clean_annotation_features)\n",
    "    if inspect_flow == True: print(f\"df_for_pipelining_train_SF: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    # step 3:\n",
    "    \n",
    "    classifier_X_train, tfidfTransformer = model_pipeline_step_3_1(df_for_pipelining_train_SF)\n",
    "    \n",
    "    classifier_y_train = model_pipeline_step_3_2(df_for_pipelining_train_SF)\n",
    "    \n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_train (made of CFs plus tf-idf matrix): {classifier_X_train.shape}\")\n",
    "        print(f\"classifier_y_train: {classifier_y_train.shape}\")\n",
    "    \n",
    "    # step 4:\n",
    "    \n",
    "    fitted_search = model_pipeline_step_4(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    # step 5:\n",
    "    \n",
    "    classifier_X_val, classifier_y_val = model_pipeline_step_5_1(df_for_pipelining_val, tfidfTransformer)\n",
    "    if inspect_flow == True: \n",
    "        print(f\"classifier_X_val: {classifier_X_val.shape}\")\n",
    "        print(f\"classifier_y_val: {classifier_y_val.shape}\")\n",
    "    \n",
    "    model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val)\n",
    "    \n",
    "    if type(model_results[classifier]) == int:\n",
    "        print(\"Model results not saved.\")\n",
    "        raise NotSavedError(\"Check model results\")\n",
    "    \n",
    "    print(f\"The runtime for {classifier} was {round(time.time() - start_code_time, 5)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a6da5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_2(classifier, clean_annotation_features):\n",
    "    \n",
    "    # step 2 – Get CFs for classifier to use for SF'ing\n",
    "    \n",
    "    # filtering the table to get the list object from the same row that lists the classifier:\n",
    "    classifier_features = clean_annotation_features[ clean_annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "    \n",
    "    # Filter the DF for rows where any of those features is 1:\n",
    "    df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "    df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    print(f\"Shape of {classifier} train df after sentence filtering is: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    return df_for_pipelining_train_SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d36c429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_1(df_for_pipelining_train_SF):\n",
    "    # separate into X\n",
    "    \n",
    "    tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "    train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])\n",
    "    \n",
    "    # Extract CF columns:\n",
    "    classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy()\n",
    "    # Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "    \n",
    "    if classifier_X_train_cfs.shape[1] != 476:\n",
    "        print(f\"Should be left with the 476 crafted features (CF). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "        raise Step_3_CF_error(\"Crafted features not being applied correctly\")\n",
    "\n",
    "    #convert to sparse\n",
    "    classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X\n",
    "    classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])\n",
    "    return classifier_X_train, tfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "83018c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_2(df_for_pipelining_train_SF):\n",
    "    # separate into y\n",
    "    \n",
    "    classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()\n",
    "    # Ensure Y_train only has binary values\n",
    "    for i in range(len(classifier_y_train)):\n",
    "        if classifier_y_train[i] > 1:\n",
    "            classifier_y_train[i] = 1\n",
    "    \n",
    "    if classifier_y_train.max() != 1:\n",
    "        print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\")\n",
    "        raise Step_3_y_error(\"train target colum not binary\")\n",
    "    \n",
    "    return classifier_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a200e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_4(classifier_X_train, classifier_y_train):\n",
    "    cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "    pipeline_sequences = [\n",
    "            ('SVC', SVC()) ]\n",
    "    pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "    svc_params = {'SVC__C': [0.1, 1, 10],\n",
    "                 'SVC__gamma': [0.001, 0.01, 0.1]}\n",
    "\n",
    "    # Create grid search object\n",
    "    grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=0, n_jobs=-1, scoring='f1')\n",
    "    \n",
    "    fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)\n",
    "    \n",
    "    return fitted_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3b2b8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_1(df_for_pipelining_val, tfidfTransformer):\n",
    "    # create validate X and y\n",
    "\n",
    "    val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "    # Extract CF columns:\n",
    "    classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "    #convert to sparse\n",
    "    classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X\n",
    "    classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "    classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "    # Ensure Y_val only has binary values\n",
    "    for i in range(len(classifier_y_val)):\n",
    "        if classifier_y_val[i] > 1:\n",
    "            classifier_y_val[i] = 1\n",
    "    \n",
    "    if classifier_y_val.max() != 1:\n",
    "        print(f\"Highest value should be one. Highest value is: {classifier_y_val.max()}\")\n",
    "        raise Step_5_val_error(\"Validation target column not binary\")\n",
    "    \n",
    "    return classifier_X_val, classifier_y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b187f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_5_2(classifier, fitted_search, classifier_X_val, classifier_y_val):\n",
    "    \n",
    "    # scoring\n",
    "    classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "    model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "    \n",
    "    model_results.to_pickle(\"model_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d3d7da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_classifiers = ['Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f4e25149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for classifier: Contact\n",
      "Shape of Contact train df after sentence filtering is: (366, 511)\n",
      "df_for_pipelining_train_SF: (366, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (366, 13708)\n",
      "classifier_y_train: (366,)\n",
      "classifier_X_val: (2651, 13708)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact was 4.2068\n",
      "\n",
      "Running for classifier: Contact_E_Mail_Address\n",
      "Shape of Contact_E_Mail_Address train df after sentence filtering is: (557, 511)\n",
      "df_for_pipelining_train_SF: (557, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (557, 16513)\n",
      "classifier_y_train: (557,)\n",
      "classifier_X_val: (2651, 16513)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_E_Mail_Address was 1.19725\n",
      "\n",
      "Running for classifier: Contact_Phone_Number\n",
      "Shape of Contact_Phone_Number train df after sentence filtering is: (487, 511)\n",
      "df_for_pipelining_train_SF: (487, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (487, 16970)\n",
      "classifier_y_train: (487,)\n",
      "classifier_X_val: (2651, 16970)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Contact_Phone_Number was 1.40252\n",
      "\n",
      "Running for classifier: Identifier_Cookie_or_similar_Tech\n",
      "Shape of Identifier_Cookie_or_similar_Tech train df after sentence filtering is: (679, 511)\n",
      "df_for_pipelining_train_SF: (679, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (679, 18904)\n",
      "classifier_y_train: (679,)\n",
      "classifier_X_val: (2651, 18904)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Cookie_or_similar_Tech was 2.23285\n",
      "\n",
      "Running for classifier: Identifier_Device_ID\n",
      "Shape of Identifier_Device_ID train df after sentence filtering is: (271, 511)\n",
      "df_for_pipelining_train_SF: (271, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (271, 9529)\n",
      "classifier_y_train: (271,)\n",
      "classifier_X_val: (2651, 9529)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Device_ID was 0.76113\n",
      "\n",
      "Running for classifier: Identifier_IMEI\n",
      "Shape of Identifier_IMEI train df after sentence filtering is: (43, 511)\n",
      "df_for_pipelining_train_SF: (43, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (43, 1872)\n",
      "classifier_y_train: (43,)\n",
      "classifier_X_val: (2651, 1872)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_IMEI was 0.27646\n",
      "\n",
      "Running for classifier: Identifier_MAC\n",
      "Shape of Identifier_MAC train df after sentence filtering is: (140, 511)\n",
      "df_for_pipelining_train_SF: (140, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (140, 7996)\n",
      "classifier_y_train: (140,)\n",
      "classifier_X_val: (2651, 7996)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_MAC was 0.44949\n",
      "\n",
      "Running for classifier: Identifier_Mobile_Carrier\n",
      "Shape of Identifier_Mobile_Carrier train df after sentence filtering is: (69, 511)\n",
      "df_for_pipelining_train_SF: (69, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (69, 4623)\n",
      "classifier_y_train: (69,)\n",
      "classifier_X_val: (2651, 4623)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Identifier_Mobile_Carrier was 0.30107\n",
      "\n",
      "Running for classifier: Location\n",
      "Shape of Location train df after sentence filtering is: (678, 511)\n",
      "df_for_pipelining_train_SF: (678, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (678, 20937)\n",
      "classifier_y_train: (678,)\n",
      "classifier_X_val: (2651, 20937)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location was 2.31339\n",
      "\n",
      "Running for classifier: Location_Cell_Tower\n",
      "Shape of Location_Cell_Tower train df after sentence filtering is: (74, 511)\n",
      "df_for_pipelining_train_SF: (74, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (74, 3509)\n",
      "classifier_y_train: (74,)\n",
      "classifier_X_val: (2651, 3509)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_Cell_Tower was 0.33707\n",
      "\n",
      "Running for classifier: Location_GPS\n",
      "Shape of Location_GPS train df after sentence filtering is: (133, 511)\n",
      "df_for_pipelining_train_SF: (133, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (133, 6103)\n",
      "classifier_y_train: (133,)\n",
      "classifier_X_val: (2651, 6103)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_GPS was 0.47049\n",
      "\n",
      "Running for classifier: Location_WiFi\n",
      "Shape of Location_WiFi train df after sentence filtering is: (101, 511)\n",
      "df_for_pipelining_train_SF: (101, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (101, 5253)\n",
      "classifier_y_train: (101,)\n",
      "classifier_X_val: (2651, 5253)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Location_WiFi was 0.41844\n",
      "\n",
      "Running for classifier: SSO\n",
      "Shape of SSO train df after sentence filtering is: (23, 511)\n",
      "df_for_pipelining_train_SF: (23, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (23, 1480)\n",
      "classifier_y_train: (23,)\n",
      "classifier_X_val: (2651, 1480)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for SSO was 0.26898\n",
      "\n",
      "Running for classifier: Facebook_SSO\n",
      "Shape of Facebook_SSO train df after sentence filtering is: (23, 511)\n",
      "df_for_pipelining_train_SF: (23, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (23, 1480)\n",
      "classifier_y_train: (23,)\n",
      "classifier_X_val: (2651, 1480)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for Facebook_SSO was 0.24808\n",
      "\n",
      "Running for classifier: 1st_party\n",
      "Shape of 1st_party train df after sentence filtering is: (7297, 511)\n",
      "df_for_pipelining_train_SF: (7297, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (7297, 108769)\n",
      "classifier_y_train: (7297,)\n",
      "classifier_X_val: (2651, 108769)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 1st_party was 78.01881\n",
      "\n",
      "Running for classifier: 3rd_party\n",
      "Shape of 3rd_party train df after sentence filtering is: (4163, 511)\n",
      "df_for_pipelining_train_SF: (4163, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (4163, 76734)\n",
      "classifier_y_train: (4163,)\n",
      "classifier_X_val: (2651, 76734)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for 3rd_party was 37.18333\n",
      "\n",
      "Running for classifier: PERFORMED\n",
      "Shape of PERFORMED train df after sentence filtering is: (6742, 511)\n",
      "df_for_pipelining_train_SF: (6742, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (6742, 103405)\n",
      "classifier_y_train: (6742,)\n",
      "classifier_X_val: (2651, 103405)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for PERFORMED was 68.17199\n",
      "\n",
      "Running for classifier: NOT_PERFORMED\n",
      "Shape of NOT_PERFORMED train df after sentence filtering is: (3413, 511)\n",
      "df_for_pipelining_train_SF: (3413, 511)\n",
      "classifier_X_train (made of CFs plus tf-idf matrix): (3413, 70104)\n",
      "classifier_y_train: (3413,)\n",
      "classifier_X_val: (2651, 70104)\n",
      "classifier_y_val: (2651,)\n",
      "The runtime for NOT_PERFORMED was 24.42364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_classifier in list_of_18_classifiers:\n",
    "    full_modelling_pipeline(each_classifier, inspect_flow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2c40cab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Contact                              [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Contact_E_Mail_Address               [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Contact_Phone_Number                 [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Identifier_Cookie_or_similar_Tech    [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Identifier_Device_ID                 [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Identifier_IMEI                      [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Identifier_MAC                       [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Identifier_Mobile_Carrier            [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Location                             [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Location_Cell_Tower                  [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Location_GPS                         [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Location_WiFi                        [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "SSO                                  [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "Facebook_SSO                         [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "1st_party                            [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "3rd_party                            [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "PERFORMED                            [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "NOT_PERFORMED                        [GridSearchCV(cv=5,\\n             estimator=Pi...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e5a819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2646    0\n",
       "2647    0\n",
       "2648    0\n",
       "2649    0\n",
       "2650    0\n",
       "Name: 1st_party, Length: 2651, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['Identifier_Cookie_or_similar_Tech'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d712ef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.07      0.12      1974\n",
      "           1       0.26      0.97      0.41       677\n",
      "\n",
      "    accuracy                           0.30      2651\n",
      "   macro avg       0.57      0.52      0.27      2651\n",
      "weighted avg       0.72      0.30      0.20      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(model_results['Contact_E_Mail_Address'][1] , model_results['Contact_E_Mail_Address'][2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
