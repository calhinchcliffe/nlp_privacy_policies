{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb32d037",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "\n",
    "Modelling pipeline: grid search over all classes.\n",
    "\n",
    "Then to make bigrams and sentence filtering optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86a733d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import ds_utils_callum\n",
    "import priv_policy_manipulation_functions as priv_pol_funcs\n",
    "\n",
    "# pre-processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# modelling\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# modelling pipeline\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# modelling evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422a3a6",
   "metadata": {},
   "source": [
    "Future pipeline:\n",
    "\n",
    "For each classifier -><br>\n",
    "Separate to X and Y<br>\n",
    "TF-IDF here option 1 <br>\n",
    "Step for SF'ing<br>\n",
    "TF-IDF here option 2 <br>\n",
    "Split into folds (5-fold CV)<br>\n",
    "3x3 SVM Hyperparameters<br>\n",
    "Find best neg F1 score\n",
    "\n",
    "Plus anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0943e8a",
   "metadata": {},
   "source": [
    "Pipeline to make now:\n",
    "\n",
    "1. Separate into classifiers. For each classifier:\n",
    "2. Apply SF'd\n",
    "3. Separate into X and Y\n",
    "4. Crate TF-IDF Matrix\n",
    "4. Split each set into 5 folds\n",
    "5. Grid search over SVM Hyperparameters to optimise F1 score\n",
    "\n",
    "Output.\n",
    "\n",
    "This will be a moderate approximation for a replication of most of their work. Main missing element will be better text pre-processing to get better results from the CFs and SF'ing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc034a58",
   "metadata": {},
   "source": [
    "Do it for one classifier, then find how to generalise it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d5d3a",
   "metadata": {},
   "source": [
    "Train, Validate and Test dataframes to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1d24f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_pipelining = pd.read_pickle(\"crafted_features_df.pkl\")\n",
    "\n",
    "df_for_pipelining_train = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TRAINING' ].copy()\n",
    "df_for_pipelining_val = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'VALIDATION' ].copy()\n",
    "df_for_pipelining_test = df_for_pipelining.loc[df_for_pipelining['policy_type'] == 'TEST' ].copy()\n",
    "\n",
    "# now that I have used the 'policy type' column for referring to train/validate/test, \n",
    "# I can delete that column along with other uneccesary columns.\n",
    "for dataframe in [df_for_pipelining_train, df_for_pipelining_val, df_for_pipelining_test]:\n",
    "    dataframe.drop(columns=['source_policy_number', 'policy_type', 'contains_synthetic',\n",
    "           'policy_segment_id', 'annotations', 'sentences'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2db48",
   "metadata": {},
   "source": [
    "# Step 1: select classifier\n",
    "\n",
    "Let's start with 1st Party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "565514c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \"1st_party\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bbb83b",
   "metadata": {},
   "source": [
    "# Step 2: apply SF'ing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc013b2d",
   "metadata": {},
   "source": [
    "1. Get CFs for 1st Party to use for SF'ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d283eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' we ', ' you ', ' us ', ' our ', 'the app', 'the software']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_features = pd.read_pickle(\"annotation_features.pkl\")\n",
    "classifier_features = annotation_features[ annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "# filtering the table to get the list object from the same row that lists the classifier\n",
    "classifier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6248b",
   "metadata": {},
   "source": [
    "2. Filter the DF for rows where any of those features is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7903b0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5101, 614)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "df_for_pipelining_train_SF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e3f6d",
   "metadata": {},
   "source": [
    "# Step 3: Separate into X and Y\n",
    "\n",
    "## Create X\n",
    "X requires a union of the Crafted Features columns and the TF-IDF matrix.\n",
    "\n",
    "Create TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aa24752",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a23f3f",
   "metadata": {},
   "source": [
    "Extract CF columns from X_train and convert to sparse so that it can be combined with TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b2d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be left with the 579 crafted features (CF). CF shape is: (5101, 579)\n"
     ]
    }
   ],
   "source": [
    "# Extract CF columns:\n",
    "classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy()\n",
    "# Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "print(f\"Should be left with the 579 crafted features (CF). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "\n",
    "#convert to sparse\n",
    "classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1026263",
   "metadata": {},
   "source": [
    "## Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "709d38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_y_train = df_for_pipelining_train_SF.loc[:,classifier].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed6cef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value should be one. Highest value is: 1\n"
     ]
    }
   ],
   "source": [
    "# Ensure Y_train only has binary values\n",
    "for i in range(len(classifier_y_train)):\n",
    "    if classifier_y_train[i] > 1:\n",
    "        classifier_y_train[i] = 1\n",
    "print(f\"Highest value should be one. Highest value is: {classifier_y_train.max()}\") # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927efab",
   "metadata": {},
   "source": [
    "# Step 4: 5-fold CV Grid Search over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34da8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp() # Memory dump to help with processing\n",
    "\n",
    "pipeline_sequences = [\n",
    "        ('SVC', SVC()) ]\n",
    "pipe = Pipeline(pipeline_sequences, memory = cachedir)\n",
    "\n",
    "svc_params = {'SVC__C': [0.1, 1, 10],\n",
    "             'SVC__gamma': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# Create grid search object\n",
    "grid_search_object = GridSearchCV(estimator=pipe, param_grid = svc_params, cv = 5, verbose=1, n_jobs=-1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1f9b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "CPU times: user 6.49 s, sys: 178 ms, total: 6.67 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fitted_search = grid_search_object.fit(classifier_X_train, classifier_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92dc002",
   "metadata": {},
   "source": [
    "# Evaluation on Train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab55e38",
   "metadata": {},
   "source": [
    "To compare to the per-classifier results given in the paper (Table 1 pg 4), I only need to look at F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bd8f77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3745\n",
      "           1       1.00      1.00      1.00      1356\n",
      "\n",
      "    accuracy                           1.00      5101\n",
      "   macro avg       1.00      1.00      1.00      5101\n",
      "weighted avg       1.00      1.00      1.00      5101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_prediction = fitted_search.predict(classifier_X_train)\n",
    "print(classification_report(classifier_y_train, classifier_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577d0f22",
   "metadata": {},
   "source": [
    "Okay, I need to do my CV grid search on just the Train set, then evaluate performance using the Validate set, since it's massively overfitting on the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b0b32",
   "metadata": {},
   "source": [
    "# Evaluation on Validate set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936e811",
   "metadata": {},
   "source": [
    "Pre-processing steps to prepare the validate set for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "787f50a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest value should be one. Highest value is: 1\n"
     ]
    }
   ],
   "source": [
    "df_for_pipelining_val.reset_index(inplace=True)\n",
    "\n",
    "val_tfidf = tfidfTransformer.transform(df_for_pipelining_val['segment_text'])\n",
    "# Extract CF columns:\n",
    "classifier_X_val_cfs = df_for_pipelining_val.loc[:,'contact info':].copy()\n",
    "#convert to sparse\n",
    "classifier_X_val_cfs = csr_matrix(classifier_X_val_cfs)\n",
    "\n",
    "# combine CF columns with TF-IDF to create X\n",
    "classifier_X_val = hstack([classifier_X_val_cfs, val_tfidf ])\n",
    "\n",
    "classifier_y_val = df_for_pipelining_val.loc[:,classifier].copy()\n",
    "# Ensure Y_val only has binary values\n",
    "for i in range(len(classifier_y_val)):\n",
    "    if classifier_y_val[i] > 1:\n",
    "        classifier_y_val[i] = 1\n",
    "print(f\"Highest value should be one. Highest value is: {classifier_y_val.max()}\") # should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772055a5",
   "metadata": {},
   "source": [
    "Scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1c7fd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      1974\n",
      "           1       0.86      0.79      0.82       677\n",
      "\n",
      "    accuracy                           0.91      2651\n",
      "   macro avg       0.90      0.87      0.88      2651\n",
      "weighted avg       0.91      0.91      0.91      2651\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_val_prediction = fitted_search.predict(classifier_X_val)\n",
    "\n",
    "model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "model_results.to_pickle(\"model_results.pkl\")\n",
    "\n",
    "# print(classification_report(classifier_y_val, classifier_val_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf6285",
   "metadata": {},
   "source": [
    "Nice! Looks like this scored well.  Let's set up the pipeline for all the other classifiers and score them too. But I'm still not sure whether I want the positive F1 score or the negative F1 score.  I think that \"negative F-1 score\" is important because it relates to when a policy fails to mention an important practice.  We want to be sure that if a policy fails to mention it, the classifier correctly states that it is not mentioned.\n",
    "\n",
    "Okay so Negative Recall is the proportion of When it was not in, did it say that it was not in?<br>\n",
    "Negative Precision then is when it predicted that it wasn't in, how often was that the case?\n",
    "\n",
    "Positive recall is When it was in, what was the chance it was identified?  <br>Positive precision is When it was predicted to be in, what was the chance that it was in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d913c35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Negative</th>\n",
       "      <th>Predicted Positive</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Negative</th>\n",
       "      <td>1890.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Positive</th>\n",
       "      <td>143.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Predicted Negative  Predicted Positive recall\n",
       "True Negative              1890.0                84.0    NaN\n",
       "True Positive               143.0               534.0    NaN"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(classifier_y_val, classifier_val_prediction)\n",
    "cf_df = pd.DataFrame(\n",
    "    cf_matrix, columns=[\"Predicted Negative\", \"Predicted Positive\"], index=[\"True Negative\", \"True Positive\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd8e0d",
   "metadata": {},
   "source": [
    "I think that I want to store negative precision, negative recall, negative F1 and positive F1.\n",
    "\n",
    "This seems like a lot of things to store for each classifier.\n",
    "\n",
    "I think I can just store the tuple of `(classifier_y_val, classifier_val_prediction)`\n",
    "\n",
    "Then from that I can extract and populate a bigger table if I want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226ef24",
   "metadata": {},
   "source": [
    "Could store as lists... for each model, have a list with 3 values: fitted search, classifier_y_val, classifier_val_prediction.  Then could store each of those lists in a series where the index is the classifier.\n",
    "\n",
    "Could store as dictionaries.  Each key is the classifier and each value is the list.\n",
    "\n",
    "Then could loop through to get matrix (table) of scores.\n",
    "\n",
    "In fact I think it will be helpful to have the order be the same as the order that I pass the classifiers, so I should use a series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9c742",
   "metadata": {},
   "source": [
    "List of all classifiers can be taken from any of the previous dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1cd1fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_18_classifiers = ['Contact', 'Contact_E_Mail_Address', 'Contact_Phone_Number', \n",
    "                       'Identifier_Cookie_or_similar_Tech', 'Identifier_Device_ID', 'Identifier_IMEI',\n",
    "                        'Identifier_MAC', 'Identifier_Mobile_Carrier',\n",
    "                        'Location', 'Location_Cell_Tower', 'Location_GPS', 'Location_WiFi',\n",
    "                        'SSO', 'Facebook_SSO',\n",
    "                        '1st_party', '3rd_party',\n",
    "                        'PERFORMED', 'NOT_PERFORMED'] # cross-checked from table on pg 4 of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d5772200",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classifiers = [\"one\", \"1st_party\", \"three\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5bf524f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_results = pd.Series(range(len(list_of_18_classifiers)),\n",
    "                          index=list_of_18_classifiers, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1f861143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results[classifier] = [fitted_search, classifier_y_val, classifier_val_prediction]\n",
    "model_results.to_pickle(\"model_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_modelling_pipeline(classifier):\n",
    "    # step 1\n",
    "    print(f\"Running for classifier: {classifier}\")\n",
    "    \n",
    "    # step 2 – Get CFs for 1st Party to use for SF'ing\n",
    "    annotation_features = pd.read_pickle(\"annotation_features.pkl\")\n",
    "    classifier_features = annotation_features[ annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "    # filtering the table to get the list object from the same row that lists the classifier\n",
    "    \n",
    "    # Filter the DF for rows where any of those features is 1.\n",
    "    df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "    df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    print()\n",
    "    print(f\"Shape of {classifier} train df after sentence filtering is: {df_for_pipelining_train_SF.shape}\")\n",
    "    \n",
    "    # separate into \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a6da5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_2(classifier):\n",
    "    # step 2 – Get CFs for 1st Party to use for SF'ing\n",
    "    annotation_features = pd.read_pickle(\"annotation_features.pkl\")\n",
    "    classifier_features = annotation_features[ annotation_features['annotation'] == classifier ].reset_index().at[0,'features']\n",
    "    # filtering the table to get the list object from the same row that lists the classifier\n",
    "    \n",
    "    # Filter the DF for rows where any of those features is 1.\n",
    "    df_for_pipelining_train_SF = df_for_pipelining_train[( (df_for_pipelining_train[classifier_features] > 0).sum(axis=1) > 0 )]\n",
    "    df_for_pipelining_train_SF.reset_index(inplace=True, drop=True)\n",
    "    print()\n",
    "    print(f\"Shape of {classifier} train df after sentence filtering is: {df_for_pipelining_train_SF.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_1(????????) check the below:\n",
    "    # separate into X\n",
    "    \n",
    "    tfidfTransformer = TfidfVectorizer(ngram_range=(1,2), stop_words='english', binary=True)\n",
    "\n",
    "    train_tfidf = tfidfTransformer.fit_transform(df_for_pipelining_train_SF['segment_text'])\n",
    "    \n",
    "    # Extract CF columns:\n",
    "    classifier_X_train_cfs = df_for_pipelining_train_SF.loc[:,'contact info':].copy()\n",
    "    # Use every column after and including the first crafted feature, which happens to be 'contact info'\n",
    "    print(f\"Should be left with the 579 crafted features (CF). CF shape is: {classifier_X_train_cfs.shape}\")\n",
    "\n",
    "    #convert to sparse\n",
    "    classifier_X_train_cfs = csr_matrix(classifier_X_train_cfs)\n",
    "\n",
    "    # combine CF columns with TF-IDF to create X\n",
    "    classifier_X_train = hstack([classifier_X_train_cfs, train_tfidf ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83018c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_step_3_2()\n",
    "    # separate into y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "priv_pol_nlp",
   "language": "python",
   "name": "priv_pol_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
